[{"name": "Modelling Microbial Communities with Graph Neural Networks", "key_word": ["graph neural networks", " microbial communities", " microbiology", " genomes"], "sn": 9504, "reviews": [{"mark": [2, 2, 2], "rate": 3, "confidence": 4}, {"mark": [2, 2, 2], "rate": 3, "confidence": 3}, {"mark": [3, 2, 1], "rate": 6, "confidence": 3}, {"mark": [3, 4, 3], "rate": 6, "confidence": 3}], "abstract": "Understanding the interactions and interplay of microorganisms is a great challenge with many applications in medical and environmental settings. In this work, we model bacterial communities directly from their genomes using graph neural networks (GNNs). GNNs leverage the inductive bias induced by the set nature of bacteria, enforcing permutation invariance and granting combinatorial generalization. We propose to learn the dynamics implicitly by directly predicting community relative abundance profiles at steady state, thus escaping the need for growth curves. On two real-world datasets, we show for the first time generalization to unseen bacteria and different community structures. \nTo investigate the prediction results more deeply, we created a simulation for flexible data generation and analyze effects of bacteria interaction strength, community size, and training data amount.\n", "detail": [{"summary": "The paper aims at predicting steady-state composition of microbial communities from the gene content of their genomes using graph neural networks.\n", "strengths": "Understanding how distinct bacteria form communities is an important problem, and the manuscript provides a solid introduction to the topic and, in the experimental section, asks important questions about our ability to understand community formation.\n", "weaknesses": "The proposed approach for using GNNs for bacterial communities is vaguely described and not well justified. The key methods section (2.2) provides a generic description of existing GNN approaches, and is missing key microbiome specific information (in particular, what is the topology of the graph). That information is provided in Supplementary information: the graph is fully connected. This makes statements in the manuscript such as \u201cBy using k graph convolutional layers after one another we can achieve k-hop information propagation\u201d rather misleading. \nOverall, the proposed method - applying GNN model in a straightforward way to a very small, fully-connected graph - is poorly justified and weak on novelty.\n", "questions": "What is the rationale for using a GNN on a very simple graph?\nWhat is the benefit of focusing on predicting steady state, instead of focusing on dynamical changes to the relative abundances (e.g., dysbiosis).\n"}, {"summary": "The paper tested the idea of using MPGNN or GraphSAGE to learn generalizable microbial community steady-state dynamics. The proposed models were tested on  simulated and previous publicly available microbial datasets and compared with the MLP-based implementation to show the effectiveness, with the discussions on the generalizability of GNN-based implementations.\n", "strengths": "The presented comparison results with MLP-based implementation demonstrates the potential of GNN-based implementations to model microbial community dynamics.\n", "weaknesses": "\nThe methodological contribution is limited as the presented work is mostly implementing GNNs for microbial steady state predictions. \n\nThe main core of the paper is based on the assumption that if there is a steady state solution to the dynamics of bacterial species, then that steady state can be predicted using the genome data of the species in the system. This is a reasonable assumption to make. However the fact that this method works only for steady state solutions needs to be emphasized. Indeed in the GLV setting in the famous example of foxes and rabbits, there could be steady state and oscillatory solutions even though the participating genomes are foxes and rabbits in both the cases. It might also be a good idea to highlight why authors expect to find (or not) only steady state solutions in systems involving microorganisms such as bacteria. This will add more strength to the paper.\n\nFor simulations, the GLV equations along with initial conditions and parameters \u03bci,Ki,ai,j drawn from different probability density functions are used to generate data. Did all such simulations lead to steady state solutions? Were any simulations that did not lead to steady state solutions discarded? Do the authors also have any comments on the frequency of steady state solutions when random parameters are used?\n\nGiven the GLV equations, the steady state solutions can be found by solving a system of |S| linear algebraic equations:\n \u2211j=1|S|ai,jnj=Ki.\nThe steady state is entirely determined by the parameters ai,j and Ki. The authors use a vector composed of [\u03bci,Ki,\u03bdis,\u03bdir,random] (where ai,j\u2248\u03bdis.\u03bdjr) to simulate the genome data in their simulation. It would be a good idea to highlight that within the simulated genome vector only the components [Ki,\u03bdis,\u03bdir] determine the steady state solution.\n\nThe parameter ai,j (broken into two vectors \u03bdis,\u03bdjr to simulate the genome) contains information on the pairwise interaction between different species. On the other hand, the information in a genome is completely intrinsic to a particular species. The authors should square these two facts.\n\nA proper simulation would entail simulation of the genome data. The genome data typically do not include information on interaction between species. But for simulations, the interaction matrix was used to derive \u03bd vectors. The claim of interpretability seems to be questionable. \n\nThe authors appear to be confused on equivariance and invariance. The permutation invariance justification for using graph neural networks is confusing. For example, GLV models are widely used to model the dynamics of microorganisms. But the GLV model is not permutation invariant. The authors stated \"\\textit{When shuffling the order of bacteria within the train and test communities, the accuracy of MLPs drops significantly, clearly showing that the dynamics learned by MLPs are not invariant to permutations...}\" It is to be expected that shuffling the data will lead to reduction in performance of MLP based models. But as long as all the training and testing is done with a particular order of species, it should not matter.\n\nThe authors need to provide details on how the node (genome) attributes were obtained, especially \u03bd's, as in real-world data, the ground-truth interaction aij is not available.\n\n\n", "questions": "\nHow the nodes, edges and their associated attributes/features were constructed, especially based on the real-world data? \n\nHow scalable is the GNN-based implementation with respect to the number of microbial species?\n\n\n"}, {"summary": "The paper looks at modeling bacterial communities and their interactions using graph neural networks (GNNs). They rely on two open datasets, total n = 552 samples. The authors have downloaded genomes for the bacteria that was converted to growth encodings. To address the issue with limited data the authors also used a simulator based on the Lotka-Volterra model. They compare three different models, MLP as the standard, GNNs and MPGNN. Using GNN/MPGNN the authors were able to model but the models were sensitive to variations and generalizing to larger systems was poor. Models were better than MLP but only marginally.\n", "strengths": "I found the paper interesting and I think the authors are correct that a better modeling of bacteria would open up a much better understanding of a wide range of fields. Key strengths:\n\nThe authors' comparative approach between models is commendable.\nThe paper addresses a clinically relevant topic, shedding light on bacterial interactions.\nThe authors' transparency regarding the challenges in scaling the mod\n\n", "weaknesses": "While I enjoyed reading something on the outskirts of my experience, although I have grown my own tuberculosis communities in the early days of my research, I struggle with some of the basic premises:\n\nMotivation & Context: The paper's motivation needs clearer alignment with real-world applications. The authors cite that understanding these communities is essential for gut, industry and space but I find the step from this paper to extrapolating to gut seems huge. The largest studied communities are 26 and this needs to be put in context with the other fields, citing Wikipedia \u201c1010 to 1011 cells per gram of intestinal content\u201d seems far off from the estimated single colonies. The types of bacteria should also be matched with the environment that you aim to generalize for.\nSample Size & DNA Inclusion: I'm concerned about the limited independent samples, especially in combination with the attempt to include DNA. Making sense of DNA has proven much more difficult than thought of in the beginning and I\u2019m not convinced that the addition made sense. Adding it to the paper risks of overfitting the data even more. I wonder if the field wouldn\u2019t benefit more from going from 500 samples to 1-2000 more than this paper. My experience with building models on this type of data is that they are frustratingly brittle due to the lack of data.\nClarity & Explanation: Coming from medicin to ML is always a challenge. It would be helpful if the paper could provide clearer explanations for terms and metrics, especially for readers transitioning from medical backgrounds. E.g. keystone bacteria are not explained, good vs acceptable R2 is unclear to the reader (I can\u2019t even find clearly how is this calculated, despite looking in appendix A which I should not have to for the main outcome), I assume that R2 is highly dependent on the underlying complexity, also the datasets have completely different bacteria suggesting that their purpose was different but this is unclear to me despite reading it several times.\nSimulation Impact: The paper should provide a clearer explanation of the effect of simulated colonies on the models' stability.\nRegarding the conclusion I\u2019m a little confused as to why it doesn\u2019t recommend including more data. I believe the authors have devoted significant time to this paper and before we put others down this path, perhaps we should wait for more data or do the authors truly feel that GEMs will be the solution?\n\n", "questions": "See weaknesses. \nMy main question is if it is true that the lack of data was your biggest challenge? And if so I would like to have it clearly stated so that others may look for additional data sources or make their own datasets available before we dive into new models.\n"}, {"summary": "The study focuses on understanding the interactions between microorganisms, which is of significant importance in both medical and environmental contexts. The authors introduce a novel approach by modeling bacterial communities using graph neural networks (GNNs) directly from the genomes of the bacteria. The inherent properties of GNNs, such as permutation invariance, allow them to effectively capture the relationships within the bacterial set, thus offering combinatorial generalization.\n", "strengths": "\nNovel problem setup and the first use of GNN to tackle this problem. \nThe use of GNN matches with the data well since it is modeling a dynamic system. \nVery interesting set of experiments and they are extensive. \nThe presentation is nice and clear.\nNice simulation data construction and results.\n\n", "weaknesses": "\nMethodological novelty is limited since it is basically fitting a GNN on a bacterial community graph. This is not to say the novelty of the paper is limited. Since I do believe it is tackling an interesting new problem with impact. I would suggest the authors consider a journal paper instead.\n\n", "questions": "Where are the circles for fig3A (models not on permuted data)? Why only select some of the combinations and not showing all of them? It would also be great if the authors could compare with standard practice of this task instead of just comparing with GraphSAGE. For example, by fitting the mechanistic model.\nHave the authors experimented with other GNN models? Since graphsage is only one instantiation and there are many recent ones with more expressive powers. \nModeling the dynamics sounds interesting. Could the authors also use GNN in an iterative way to model the dynamics? For example, using ideas from this paper: http://proceedings.mlr.press/v119/sanchez-gonzalez20a.html\n"}]}, {"name": "TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023", "key_word": ["tabular", " tabular data", " architecture", " deep learning", " neural networks"], "sn": 9502, "reviews": [{"mark": [3, 3, 3], "rate": 8, "confidence": 3}, {"mark": [4, 4, 2], "rate": 6, "confidence": 2}, {"mark": [3, 3, 3], "rate": 6, "confidence": 3}, {"mark": [3, 2, 2], "rate": 3, "confidence": 4}], "abstract": "Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers.\nHowever, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for these problems.\nOne of the research directions aimed at improving the position of tabular DL involves designing so-called retrieval-augmented models.\nFor a target object, such models retrieve other objects (e.g. the nearest neighbors) from the available training data and use their features and labels to make a better prediction.\nIn this work, we present TabR -- essentially, a feed-forward network with a novel k-Nearest-Neighbors-like component in the middle. On a set of public benchmarks with datasets up to several million objects, TabR marks a big step forward for tabular DL: it demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed \"GBDT-friendly\" benchmark (see Figure 1).\nAmong the novel findings and technical details powering TabR, the main ones lie in the attention-like mechanism that is responsible for retrieving the nearest neighbors and extracting valuable signal from them.\nIn addition to the much higher performance, TabR is simple and significantly more efficient compared to prior retrieval-based tabular DL models.\n", "detail": [{"summary": "This paper considers the problem of making predictions on tabular data. The authors propose a retrieval-augmented approach where a predictor takes the representation not of the table being predicted but also the representation of the nearest neighbors from a training dataset. The encoding representations and the predictors are training together and use straightforward architecture architectures. The main result is that a combination of the carefully crafted techniques outperforms GBDT on an ensemble of tasks. The training time is higher than GBDT but not unreasonable, and better compared to prior deep learning methods. The prediction times are better\n", "strengths": "\nThe results seem to be a significant advance over prior work in tabular data predictions. In particular, the first deep learning model to outperform GBDT on an ensemble of datasets.\nThe experiments and analysis are quite extensive. Multiple datasets of different kinds of data, analysis of training and prediction times.\nClear articulation of which techniques helped. the techniques are overall not too complex.\n\n", "weaknesses": "A comparison of the inference and query complexity between the methods is lacking.\n", "questions": "\nInference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more? How does the inference complexity depend on the size of the table data?\n\nCould a different selection of datasets prove that the tabR is not superior to GBDT? In other words, are these datasets highly representative?\n\nIs it not surprising that Step-1 (adding context labels) did not help that much? One would guess that this is a big component of signal in retrieval augmentation.\n\nNot a question, but the methodology here reminds one of extreme classification and specifically this paper. https://arxiv.org/abs/2207.04452\n\n\n"}, {"summary": "This work proposes a retrieval-augmented deep learning architecture for tabular regression/classification. The model passes x, the row to be classified/predicted, as well as additional retrieval context rows, through a learned encoder. TabR then retrieves the rows most similar to the encoded form of x, where similarity is defined as the Euclidean distance between the encoded versions of two rows, mapped through a linear layer. The top retrieval candidates and their respective labels are then sent through some more learned transformations before being aggregated and combined with the encoded form of the row to be classified/regressed. This combined embedding goes through more MLP layers to result in the output.\nThe paper goes through variants of the architecture and how each respective change impacts performance. It then compares against other deep learning-based models as well as gradient boosted decision trees. In both default-hyperparameter and tuned-hyperparameter settings, TabR performs well.\n", "strengths": "\nThe extensive amount of open-sourcing and experiment reproducibility is greatly appreciated.\nStrong results relative to both deep learning and boosted tree methods, and TabR-S's relatively strong performance relative to out-of-the-box boosted tree libraries suggests this isn't just excessive parameter tweaking and overfitting via architecture search.\nEasy to read, with key pieces of information generally emphasized appropriately.\n\n", "weaknesses": "\nPaper doesn't go into detail describing differences with prior deep learning-based tabular methods. What might explain the performance differences? Ex. \"prior work, where several layers with multi-head attention between objects and features are often used\" but was this what led to retrieval's low benefit in the past?\nInsufficient discussion of categorical variables. Is accuracy or training time particularly affected by their relative abundance relative to numerical features?\nThe steps of Section 3.2 seem rather arbitrary. Some of the detail could be compressed to make room for more intuition why the final architecture makes more sense (content from A.1.1). Description of architectural changes that didn't work would also be very insightful.\nPaper describes training times in A.4, but I believe a summary of this is important enough to warrant inclusion in the main paper. Something like a mention of the geometric mean (over the datasets) of the ratio between TabR's training time to a gradient boosted methods, described in the conclusion, would be sufficient. While the ratio is likely >1, it is better to acknowledge this weakness than to hide it.\n\n", "questions": "See weaknesses. Also, what is Icand? Is it all rows of the table that labels have been provided for? It's mentioned in page 3 that \"we use the same set of candidates for all input objects\" but what it the set of candidates exactly?\n"}, {"summary": "The paper introduces TabR, a retrieval-augmented tabular deep learning model that outperforms gradient-boosted decision trees (GBDT) on various datasets. TabR incorporates a novel retrieval module that is similar to the attention mechanism, which helps the model achieve the best average performance among tabular deep learning models and is more efficient compared to prior retrieval-based models.\n", "strengths": "\nTabR demonstrates superior performance compared to GBDT and other retrieval-based tabular deep learning models on multiple datasets.\nThe new similarity module in TabR has a reasonable intuitive motivation, allowing it to find and exploit natural hints in the data for better predictions.\n\n", "weaknesses": "\nSome aspects are not clear, see the questions section.\n\n", "questions": "\nWhat's the reason for choosing m to be 96? How does m affect the performance of TabR?\nWhat's the inference efficiency of TabR and how does it compare with other baselines (e.g., GBDT)?\nIs TabR applicable to categorical features? It seems like the paper only considers continuous features.\n\n"}, {"summary": "The authors meticulously designed a supervised deep learning model for tabular data prediction, which operates in a retrieval-like manner. It outperformed tree-based models on middle-scale datasets, as well as other retrieval-based deep learning tabular learning models. To achieve this, they introduced a k-Nearest-Neighbors-like idea in model design.\n", "strengths": "\nAs emphasized by the authors, their method has managed to outperform tree based models like xgboost on middle-scale datasets.\n\nOverall, the presentation is clear, and the experiments are comprehensive. The details are clear and the model is highly reproducible.\n\nThis model is the best-performing retrieval based model.\n\n\n", "weaknesses": "\nThe motivations behind the module designs are not entirely clear. It appears that the authors made meticulous module (equation) optimization based on its performance on some datasets empirically. Then:\n\n(1) Why does employing the L2 distance, instand of the dot product, lead to improved performance (as shown in Eq. 3)? \n(2) Why is the T function required to use LinearWithoutBias? \n(3) We are uncertain about the robustness of the designed modules. If the dataset characteristics are changed, is it likely that the performance rankings will change significantly? The performances only on middle-sized datasets cannot show the robustness.\n...\nI suggest that providing a theoretical analysis or intuitive motivation would enhance the reader's understanding of those details.\n\nSome sota DL approaches are not compared, such as T2G-Former (an improved version of FTT)[1], TabPFN [2], and TANGOS [3]. Especially, TabFPN is relatively similar to TabR. These papers are current SOTA, and may outperforms tree based models.\n\n[1] T2G-Former: Organizing tabular features into relation graphs promotes heterogeneous feature interaction\n[2] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\n[3] TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\n\nThe major comparison lies among middle-scale datasets, accompanied with some results on few other datasets shown in Table 3. In scenarios involving sparse, medium, and dense data-distributed datasets (which typically occur in small, medium-sized, and large-sized datasets, respectively), I suppose that there exists a variance in the nearest neighbor retrieval pattern. Hence, conducting tests solely on medium-sized datasets may not suffice. Furthermore, the issue of inefficiency when dealing with large-scaled datasets appears to have hindered the authors from proving the method's effectiveness in large-scaled datasets.\n\nThe method proposed by the authors appears to have achieved slight performance advantages on certain datasets (although some SOTA are not compared). However, due to the lacks of explanation for the model details that are designed empirically, it seems unnecessary and risky to apply this method in real-world scenarios (for example, it's unclear whether L2 distance may fail when uninformative features are present; or, for instance, when a table has a feature with values [f_1, f_2, f_3, ..., f_n], and we take the logarithm of these values [log f_1, log f_2, log f_3, ..., log f_n] or their reciprocals, the method may perform poorly in such cases).\n\n\n", "questions": "\nIn Section 3.1, you mentioned \"continuous (i.e., continuous) features.\" Could this be a typographical error?\n\nI am curious if the L2 design is sensitive to uninformative features? You can offer some analysis or conduct experiments by adding some gaussian noise columns (uninformative features are commonly seen in tabular datasets) and observe the change of performances. Some transformation like logarithm may impact the results.\n\nSome questions in weakness.\n\n\n"}]}, {"name": "Neural Evolutionary Kernel Method: A Knowledge-Based Learning Architechture for Evolutionary PDEs", "key_word": ["Numerical PDE", " structure preserving neural network", " operator learning", " boundary integral"], "sn": 9498, "reviews": [{"mark": [2, 3, 3], "rate": 6, "confidence": 2}, {"mark": [3, 3, 2], "rate": 5, "confidence": 4}, {"mark": [2, 2, 2], "rate": 3, "confidence": 4}, {"mark": [2, 3, 2], "rate": 3, "confidence": 3}], "abstract": "Numerical solution of partial differential equations (PDEs) plays a vital role in various fields of science and engineering. In recent years, deep neural networks (DNNs) have emerged as a powerful tool for solving PDEs. DNN-based methods exploit the approximation capabilities of neural networks to obtain solutions to PDEs in general domains or high-dimensional spaces. However, many of these methods lack the use of mathematical prior knowledge, and DNN-based methods usually require a large number of sample points and parameters, making them computationally expensive and challenging to train. This paper aims to introduce a novel method named the Neural Evolutionary Kernel Method (NEKM) for solving a class of evolutionary PDEs through DNNs based kernels. By using operator splitting and boundary integral techniques, we propose particular neural network architectures which approximate evolutionary kernels of solutions and preserve structures of time-dependent PDEs. Mathematical prior knowledge are naturally built into these DNNs based kernels through convolutional representation with pre-trained Green functions, leading to serious reduction in the number of parameters in the NEKM and very efficient training processes. Experimental results demonstrate the efficiency and accuracy of the NEKM in solving heat equations and Allen-Cahn equations in complex domains and on manifolds, showcasing its promising potential for applications in data driven scientific computing.\n", "detail": [{"summary": "The paper introduces a novel approach called Neural Evolutionary Kernel Method (NEKM) for solving time-dependent semi-linear Partial Differential Equations (PDEs). The authors leverage a combination of operator splitting, boundary integral techniques, and Deep Neural Networks (DNNs) to construct evolutionary blocks that approximate solution operators. NEKM incorporates mathematical prior knowledge into each block, utilizing convolution operations and nonlinear activations tailored to the specific PDEs under consideration. This approach offers several noteworthy contributions:\n\nEfficiency and Generalizability: The use of boundary integral techniques is a standout feature of NEKM, allowing for a reduced requirement of network parameters and sampling points. This not only improves training efficiency but also relaxes the regularity assumptions on solutions. The capacity to apply NEKM to problems in complex domains and on manifolds showcases its versatility and potential real-world applicability.\n\nCompatibility with Time Discretization Schemes: NEKM can be effectively combined with time discretization schemes that possess structure-preserving properties, such as energy stability. This demonstrates the adaptability of the method to diverse mathematical contexts.\n\nTreatment of Singular Boundary Integrals: The paper introduces a method for computing singular boundary integrals that arise from fundamental solutions. This addition contributes to the overall training efficiency and robustness of NEKM.\n\n\nThe empirical validation of NEKM is conducted through testing on heat equations and Allen-Cahn equations in complex domains and on manifolds. The results demonstrate the method's high accuracy and its capacity to generalize across various domains.\nIn summary, the paper presents an innovative and promising approach, NEKM, which addresses the solution of time-dependent semi-linear PDEs. The combination of mathematical prior knowledge, boundary integral techniques, and DNNs provides a compelling method that improves training efficiency, generalizability, and adaptability to different mathematical scenarios. The successful testing on various equations and domains underscores the method's potential significance in the field of mathematical modeling and scientific computing.\n", "strengths": "The strengths of the paper \"Neural Evolutionary Kernel Method (NEKM) for Solving Time-Dependent Semi-Linear PDEs\" include:\n\nInnovative Approach: The paper introduces a novel approach, NEKM, which combines operator splitting, boundary integral techniques, and Deep Neural Networks (DNNs) to address the solution of time-dependent semi-linear Partial Differential Equations (PDEs). This innovation offers a fresh perspective on tackling complex mathematical problems.\n\nEfficiency Improvement: NEKM leverages boundary integral techniques to reduce the need for extensive network parameters and sampling points. This not only enhances the efficiency of training but also relaxes regularity assumptions on solutions. This efficiency improvement is a significant advantage in solving real-world problems.\n\nGeneralizability: The paper demonstrates that NEKM can be applied to problems in complex domains and on manifolds, showcasing its generalizability across different mathematical contexts. This broad applicability enhances its potential usefulness in a wide range of scientific and engineering applications.\n\nCompatibility with Time Discretization Schemes: NEKM's compatibility with time discretization schemes that possess structure-preserving properties, such as energy stability, is a valuable feature. This adaptability makes it easier to integrate NEKM into existing mathematical frameworks.\n\nTreatment of Singular Boundary Integrals: The paper provides a method for computing singular boundary integrals that arise from fundamental solutions. This contribution adds to the method's efficiency and robustness, making it more practical for real-world applications.\n\nEmpirical Validation: The authors validate the NEKM approach through rigorous testing on heat equations and Allen-Cahn equations in complex domains and on manifolds. The high accuracy demonstrated in these tests underscores the practical utility of NEKM.\n\nMathematical Rigor: NEKM incorporates mathematical prior knowledge into its framework through convolution operations and nonlinear activations. This mathematical rigor ensures that the method is well-founded and theoretically sound.\n\nInterdisciplinary Relevance: The paper's focus on solving complex mathematical problems with machine learning techniques has broad interdisciplinary relevance, as it can find applications in various fields, including physics, engineering, and computational science.\n\n\nOverall, the strengths of the paper lie in its innovative approach, efficiency improvements, generalizability, compatibility with existing mathematical schemes, and the rigorous empirical validation of the proposed method. These qualities make NEKM a promising addition to the field of mathematical modeling and scientific computing.\n", "weaknesses": "While the paper on \"Neural Evolutionary Kernel Method (NEKM) for Solving Time-Dependent Semi-Linear PDEs\" offers several strengths, there are also some potential weaknesses to consider:\n\nComplexity: The proposed NEKM method, while innovative, is complex in its approach, involving the integration of operator splitting, boundary integral techniques, and Deep Neural Networks. This complexity might make it challenging for practitioners who are not well-versed in all of these areas to implement and understand.\n\nComputational Resources: The paper does not extensively discuss the computational resources required for training and applying the NEKM method. Deep learning methods often demand significant computational power, which could be a limitation for some users, particularly those without access to high-performance computing resources.\n\nLimited Real-World Use Cases: While the paper demonstrates NEKM's effectiveness in solving specific mathematical problems, it remains largely theoretical. More real-world use cases and practical applications in various domains would strengthen the paper's relevance and utility.\n\nInterpretability: The paper discusses the use of neural networks, which are often seen as \"black-box\" models. While the paper addresses some interpretability challenges, it might not provide a complete solution to the interpretability issues associated with deep learning approaches.\n\nAlgorithm Complexity: The proposed method involves a combination of different techniques, such as boundary integral representation and neural networks. This may make the implementation and understanding of NEKM challenging for some users, potentially limiting its widespread adoption.\n\nEmpirical Validation Scope: While the paper includes empirical validation on heat and Allen-Cahn equations, the scope of the empirical validation might be limited. A more extensive range of test cases across different scientific and engineering domains would strengthen the method's generalizability.\n\nScalability: The paper does not explicitly address the scalability of the NEKM method. As the complexity of problems increases, it remains to be seen whether NEKM can efficiently scale to handle more complex and larger-scale scenarios.\n\nComparison to Existing Methods: The paper lacks a comprehensive comparison of the NEKM method with existing approaches for solving similar problems. Such comparisons would help to better assess the relative strengths and weaknesses of NEKM.\n\n\nIn conclusion, while the NEKM method offers several promising advantages, such as efficiency improvements and generalizability, it also has some potential limitations, including complexity, computational resource requirements, and the need for more extensive real-world applications and validation. These weaknesses should be considered when evaluating the method's suitability for specific applications.\n", "questions": "\nCan you provide more insight into the computational resources required for training and applying the NEKM method? What kind of hardware and software infrastructure is necessary for its practical implementation?\n\nThe NEKM method is quite complex, involving a combination of operator splitting, boundary integral techniques, and neural networks. How user-friendly and accessible is the implementation for researchers and practitioners who may not be experts in all these areas?\n\nThe paper mentions empirical validation on heat and Allen-Cahn equations. Are there plans to expand the empirical validation to a broader range of mathematical problems or real-world applications to further assess the generalizability of NEKM?\n\nHow does NEKM address the interpretability challenge often associated with deep learning methods? Can you provide more details on how NEKM helps users understand and trust its results, especially in cases where interpretability is critical?\n\nThe paper mentions combining NEKM with time discretization schemes that possess structure-preserving properties. Could you elaborate on specific scenarios or use cases where this combination has proven to be advantageous?\n\nNEKM proposes the treatment of singular boundary integrals arising from fundamental solutions. Can you discuss the impact of this addition on the overall efficiency and robustness of the method in practical applications?\n\nIn the real world, problems often scale in complexity. How does NEKM address the scalability challenge, especially when dealing with larger and more complex scenarios beyond the examples provided in the paper?\n\nThe paper does not include a comprehensive comparison of NEKM with existing methods for solving similar problems. Could you share insights into how NEKM performs in comparison to other approaches, and in what scenarios it may have a comparative advantage?\n\nAre there any specific plans or ongoing research aimed at addressing some of the potential weaknesses or limitations identified in the paper, such as making the method more accessible or broadening the scope of empirical validation?\n\nHow do you envision the practical adoption of NEKM in various scientific and engineering domains? Are there specific industries or areas where NEKM is expected to have a significant impact, and if so, what are the next steps for its real-world application?\n\n\nThese questions aim to seek further clarification and insights from the authors regarding the NEKM method and its potential applications and improvements.\n"}, {"summary": "This paper aims to tackle solving partial differential equations (PDEs) traditionally solved by numerical methods with deep neural networks (DNNs). The authors address the challenges of solving PDEs with DNNs that a majority of these methods do not use any mathematical or physical parameters and require a large amount of parameters to tune. The authors propose the Neural Evolutionary Kernel Method (NEKM) to solve a type of evolutionary PDEs with DNN based kernels. The core idea is to incorporate pre-trained Green's functions. NEKM is an alternating two-step procedure that first analytically or numerically solves a nonlinear ODE to obtain a flow map and then numerically integrate the related linear PDE with a convolutional kernel.\n", "strengths": "\nNice abstract that motivates the need for PDEs in science and engineering problems and use of numerical methods to solve them.\nThe paper and abstract are well-written.\nIncorporating ideas from numerical methods, e.g., Green's function, boundary conditions and energy stability is very nice. In particular, I like to the discussion in subsection 2.2 on energy conservation and would like more details in the Appendix.\nThe generalization and use of the pre-trained Green's function is nice.\nThe computational savings of defining the Green's function on the boundary rather than the interior domain is nice. For other boundary integral representations for conservation laws, see Hansen, et. al, \"Learning physical models that can respect conservation laws\", ICML 2023 (https://arxiv.org/abs/2302.11002).\nNice high dimensional simulations in Figures 6-7.\nGeneralizability to different manifolds and boundary conditions.\n\n", "weaknesses": "\nThe authors should define earlier what they mean by evolutionary PDEs.\nConnection to other kernel operator methods such as the Fourier Neural Operator (FNO) should be considered. It is only briefly discussed in one sentence of related work with a majority on the PINNs literature. In particular, in the related works, the authors discuss in detail how boundary conditions are incorporated into Physics-Informed Neural Networks (PINNs). The related in Neural Operator community should be discussed, such as how to incorporated boundary conditions into Neural Operators in Saad et. al, \"Guiding continuous operator learning through Physics-based boundary constraints\", ICLR 2023.\nThe method only works on semi-linear PDEs. This is actually a very strong assumption and limitation. The authors should discuss the extension to nonlinear PDEs.\nEvaluation: the method is only tested on the simple linear heat/diffusion equation and Allen-Cahn equations. The heat equation is smooth and parabolic and very easy for numerical methods to solve. It would be nice to test hyperbolic problems with shocks, e.g., in the GPME benchmarking framework in Hansen, et. al, \"Learning physical models that can respect conservation laws\", ICML 2023 (https://arxiv.org/abs/2302.11002).\nThe method seems to have strong limitations if the first step requires an analytical or numerical solution to the ODE. \nIn particular, the authors should clarify this in the last paragraph of the introduction. I don't understand where the nonlinear ODE is coming from in step 1 and then how there is \"numerically integration\" for the related linear PDE. Typically, in numerical methods a (non)linear PDE is first discretized in space and then the resulting semi-discrete form of the ODE is discretized in time. The authors should clarify what they mean here.\nI think some of the equation details of BINet in the related work should be moved to an appendix or background section.\nCare should be taken with the discretization because this adds a first order error into the scheme. For example, the first equation should not be discretized with the 1st order accurate Forward Euler without even citing the method. This is an explicit method and there are necessary bounds on \u0394t/\u03c4 to ensure numerical stability.  See Krishnapriyan et. al, \"Learning continuous models for continuous physics\", 2023 (https://arxiv.org/pdf/2202.08494.pdf) on how the time discretization matters in NeuralODE and the 4th order RK4 is advantageous but even that scheme without being careful about the numerics can lead to convergence issues.\nIdeally the method and presentation wouldn't need to be separated into separate cases for linear equations or not.\nIt seems like the method depends too strongly on the BINet method and the authors should better differentiate the novelty between the two.\nThe exposition of the method in Section 2 isn't too clear and some of the details can be moved to an appendix.\nThe unique features of the NEKM subsection seems like it could be incorporated with the contributions subsection in the intro.\nLabel x and y axis in Figure 3.\nAnother major weakness in the evaluation is just comparing to the exact solution and no other baseline methods, especially to related neural operator based methods.\n\nMinor\n\nFirst paragraph of related works can be longer and combined with parts of the longer second paragraph.\nheat equation shouldn't be plural in the last bullet point of the contributions.\ncomma after \"In this section\" at the beginning of Section 2 Method\nI would name Section 2 with the specific method name Neural Evolutionary Kernel Method (NEKM) rather than the generic Method.\nCould use standard notation from numerical methods \u0394t instead \u03c4\nComma missing after Equation 7.\nLarger title lave on Figure 6.\n\n", "questions": "\nDoes the method only work on semi-linear PDEs? If so, this is a bit limiting and the authors should discuss the extension to nonlinear PDEs.\n\n"}, {"summary": "The paper presents the Neural Evolutionary Kernel Method (NEKM) for solving semi-linear time-dependent PDEs. NEKM distinguishes itself by utilizing operator splitting and boundary integration, enabling efficient network architectures. The method is demonstrated to be effective and stable in solving classic PDEs, such as the heat equation and the Allen-Cahn equation.\n", "strengths": "NEKM can be combined with time discretization schemes that preserve energy stability, which is crucial for modeling physical systems.\nThe method incorporates an evolutionary kernel, which inherently preserves the structure of the problem.\nThe method incorporates an evolutionary kernel, which inherently preserves the structure of the problem.\n", "weaknesses": "While NEKM is claimed to work in complex domains, the paper primarily provides examples in small and relatively simple domains. It would be beneficial to demonstrate its performance in more complex and realistic domains, similar to the level in the referenced paper (https://arxiv.org/pdf/2309.00583), including real-world scientific and engineering geometries.\nThe paper lacks references to related work that adopts neural networks only at the spatial level while using time discretizations to evolve spatial fields over time. Including references to papers like \"Evolutional deep neural network (Physical Review E 2021),\" \"Implicit Neural Spatial Representations for Time-dependent PDEs (ICML 2023),\" and \"Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations\" could help provide context and comparisons.\nThe paper does not provide information about the computational cost and scalability of NEKM compared to classical numerical methods, especially for larger 3D problems. It would be valuable to include performance comparisons in terms of computational efficiency.\n", "questions": "My biggest confusion and concern is the relationship between this paper (Lin et al., 2023a) as well as (Lin et al., 2023b). Those paper also use a convolution representation of the solutions using Green's functions. What exactly is the author's contribution except working with time-dependent problems?\nThe paper focuses on semi-linear PDEs, but it would be interesting to know if NEKM can be extended to handle nonlinear PDEs. Clarification on the limitations and potential extensions of the method for nonlinear problems would be beneficial.\n"}, {"summary": "This paper proposes a neural network-based algorithm, namely the Neural Evolutionary Kernal Method (NEKM), for solving evolutionary PDEs. The method involves the operator splitting technique and the idea of boundary integral network. Specifically, the method pre-trains a neural network representation of the Green function and then solves the evolutionary PDE by applying the Green function block and kernel function block alternatingly with an ODE solver. Experiments on the heat equation and Allen-Cahn equations are conducted to demonstrate the performance.\n", "strengths": "\nThe paper is well-written and easy-to-follow.\nThe proposed method is interesting and mathematically grounded.\nExperimental results seem strong.\n\n", "weaknesses": "\nIt seems the method heavily relies on the closed form formula of the fundamental solution G0. The numerical error of the integration involving G0 seems troublesome.\nThe experimental results of Allen-Cahn equation is not compared with the exact one or any other method.\nSome minor issues: Figure 12 is too small.\n\n", "questions": "\nNow that the Green function G is computed by pre-training a neural network, the error of this step may propagate to solving the time evolutionary PDE. Was this problem an issue in the experiments? How accurate should the numerically approximated Green function be so as not to affect the performance?\nAs mentioned in the paper, the possible singularity of G0 may demand special handling. But the form of G is generally unknown. How can the singularity appearing in G be dealt with?\nEnergy stsability is claimed as one of the contributions. Is this only empirically observed or grounded with some particular design?\n\n"}]}, {"name": "PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs", "key_word": ["PINN", " machine learning", " physics-informed machine learning"], "sn": 9493, "reviews": [{"mark": [3, 3, 3], "rate": 5, "confidence": 3}, {"mark": [4, 4, 4], "rate": 6, "confidence": 3}, {"mark": [2, 4, 1], "rate": 1, "confidence": 5}, {"mark": [3, 3, 2], "rate": 6, "confidence": 3}], "abstract": "While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research, particularly in areas such as domain decomposition methods and loss reweighting for handling multi-scale problems and complex geometry. While PINNacle does not guarantee success in all real-world scenarios, it represents a significant contribution to the field by offering a robust, diverse, and comprehensive benchmark suite that will undoubtedly foster further research and development in PINNs.\n", "detail": [{"summary": "This paper provides both a collection of benchmark datasets as well as a standardized suite of PINN-type neural network PDE solution approximators arranged as a python package.\nIt further shows benchmark numbers of the different PINN methods on the benchmark datasets.\n", "strengths": "Providing any meaningful benchmark to the community is a valuable service.\nIn addition to creating the benchmark data sets, the authors have made a big effort in collecting and unifying PINN methods into a unified framework.\nThe paper appendix contains detailed specifications about the particular setup for the data benchmark.\n", "weaknesses": "While providing a benchmark data set to the community is a valuable service, several aspects could be improved.\nMinor: \n-It would be great to have a table or list (in the appendix) detailing a comparison of the provided data sets to those in PDEarena (and PDEbench).\nMajor: \n\nThe relative error values in the results tables are for the most part shockingly bad and simply not useful for many numerical analysis contexts. Given that PINNs seem to be mostly providing different function spaces for PDE solutions, one original base PINN should be included in the benchmark, which is to give each hat function on a finite element mesh one parameter, and hence include finite element methods. Because some of the data sets were created using FEM, the original mesh would yield 0 error, but different meshes may not, and in particular coarser meshes would accumulate error. Analyzing a curve of remeshing from same resolution to coarse would provide a baseline for the performances of the other PINNs.\n\nIn the above sense, it also becomes important to quantify flop counts. It appears that most PINNs need to be fitted for each PDE solution, incurring the typically high flop count of solving an optimization problem (compared to one forward pass), and only some of them can learn solutions conditional on hyperparameters given as input and require only forward passes to solve e.g. from different inital conditions.\nFor all cases, there should be 3 different flop counts provided: 1) The number of flops required to create the training set 2) The number of flops required for any general training of the method  3) the number of flops required to evaluate/fit the method on a particular example. Many PINNs, and the FEM baseline would only have nonzero counts in point 3, and it would be good to compare them.\nHaving flop counts or even wall time counts would allow answering questions like \"at equal error rate, does fitting a PINN or fitting FEM cost more computational power?\" and \"At equal computing power, can FEM beat the error rates of the listed PINNs?\"\n\ncontinuing the discussion about flop counts, methods learning from multiple data sets/examples should be included in order to compare flop counts and provide additional reference error values. In particular for the time propagating PDEs, solutions using U-nets or FNOs from e.g. PDE bench should be included as reference values, in terms of performance, flops required for training, flops required to generate the required training data, and flops required to run a forward pass to obtain a solution. Then one can assess how many PINNs or FEM solutions one can compute for the same budget as a certain number of forward passes of the propagator network. The should be a break-even point at some number of forward passes justifying the training effort.\n\n\nWithout these points, the benchmark is unfortunately sitting just beyond actual widespread utility. I would highly encourage the authors to add these baselines to make the benchmark useful. Despite my positive bias towards benchmarking efforts I cannot recommend acceptance of this paper in its current state.\n", "questions": "Would it be possible to address the major issues listed above among weaknesses?\n"}, {"summary": "This paper provides a comprehensive comparison of PINN training methods, problems, and data. The paper visits common problems with training PINNs, namely the complex geometry, the multi-scale phenomena, nonlinearity of some PDE ofrs, and the high dimensional PDE problems. They also provide various training mechanisms such as domain decomposition and loss reweighting methods.\n", "strengths": "\nThe paper is well-written; it seems obvious this work has gone through a few rounds of polishing and review.\nThe literature review is detailed and comprehensive.\nThe challenging aspects of training PINNs are decomposed and categorized well.\nThe appendix section of the paper is thorough and contains quality information.\nThe suite of experiments is admittedly comprehensive; there are more than 20 PDE forms, 10 methods considered and compartmentalized well in this paper.\nThe scale of the experiments and the analyses of the hyper-parameters is certainly admirable.\n\n", "weaknesses": "\nI'm saying out of respect to the author's work, but this paper may be more suited for a journal format. In particular, the page limit constraint is hitting the work hard in my opinion.\n\n\nBy the time the authors present the data and experiments, there is less than half a page left to interpret the results and provide discussions and conclusions.\n\nMany key discussions, at different points in the main text, were deferred to the appendix. While they do exist in the appendix and carry out important information, they carry more scientific content than the existing paper's text.\nTo be clear, the paper's topic is certainly relevant to ICLR and could benefit the ICLR community. However, the conference format may not be the most suitable to present the work as best as it could have been.\n\n\n\nThe work utilizes 10 different methods for training PINNs, but a brief description of these methods in a single mathematical framework is missing. Adding such a description and correlating the numerical findings to the theoretical properties of each method is probably the most important, yet under-performed, part of the work in my opinion.\n To be clear, I understand the paper's space constraints, but this is very important in my opinion. The least the authors could do is to add such a section, however briefly, to the appendix.\n\n\n", "questions": "See the weaknesses section.\n"}, {"summary": "The paper provides a benchmarking tool called PINNacle which was lacking in the domain of PINNs. The tool provides a diverse set of 20 different PDEs spanning over various application domains. The tool also provides implementations of 10 state-of-the-art techniques in PINNs and shows extensive experiments to show the strengths and weaknesses of each method.\n", "strengths": "\nThe paper is overall well written and easy to follow.\nThe paper provides an extensive comparison of the different SOTA methods for different PDEs.\n\n", "weaknesses": "\nThe paper lacks technical novelty to be considered for the main track. In my opinion, the paper is more suitable for an application/dataset track, for e.g., NeurIPS Dataset/Benchmark Track.\nThe insights provided in the paper are not novel and are also well-known in the PINN literature which the paper cites as well.\n\n", "questions": "\nTable 3 shows that all of the selected SOTA methods fail on the KS Equation. However, some PINN methods can solve KS Equations such as Causal PINNs [1]. \nWhen comparing the effect of the parametric PDEs on different PINN variants (shown in Table 4), using the Average L2RE is not a good choice. It would be more informative to show the mean and the standard deviations for the different parameter choices. The average L2RE can be skewed if one (or few) of the parameter settings fails (i.e., have L2RE of 100%) while others have very low errors (such as ~1e-4).\n\n[1] Wang, S., Sankaran, S., & Perdikaris, P. (2022). Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404.\n"}, {"summary": "None\n", "strengths": "The article introduces \"PINNacle\", a robust benchmark suite tailored for Physics-Informed Neural Networks (PINNs). This suite boasts a rich assortment of over 20 intricate PDE challenges, complemented by a user-centric toolbox that houses over 10 of the latest PINN techniques. These techniques are segmented by the authors into categories: loss reweighting, advanced optimizers, unique loss functions, and groundbreaking architectures. An exhaustive analysis is then executed with this benchmark dataset to scrutinize these variations.\nMany of the challenges pinpointed in the dataset resonate with a multitude of real-world scenarios. Thus, the efficacy of a method in tackling these challenges becomes a credible measure of its real-world utility. To generate the data, the authors employ the FEM solver from COMSOL 6.0 for intricately geometric problems and the spectral method from Chebfun for the more chaotic issues. This dataset encompasses challenges like the heat equation, Poisson equation, Burgers' equation, Navier-Stokes equation, among others.\nThe paper outlines a uniform criteria to gauge the performance of varied PINN techniques across all challenges, promoting a methodical comparison of different tactics. Performance assessment is conducted using various metrics, such as accuracy, convergence rate, and computational prowess. Moreover, the authors shed light on the advantages and limitations of these methods, providing direction for subsequent studies, especially in fields like domain decomposition and loss reweighting.\nIn essence, the article's merits lie in its crafting of a dataset that mirrors significant challenges confronted by PINNs, establishing a uniform assessment criteria for different PINN approaches, and giving valuable insights on the strengths and pitfalls of these methods. This work undeniably propels the growth of PINNs, igniting further creativity and advancements in this burgeoning domain.\n", "weaknesses": "This paper stands out with several merits, accentuating its importance in the realm of Physics-Informed Neural Networks (PINNs).\nTo begin with, it offers an all-encompassing benchmark suite for PINNs, showcasing a varied dataset containing over 20 intricate PDE challenges, supplemented by an accessible toolbox with more than 10 leading PINN techniques. This suite facilitates an organized comparison of multiple approaches and delivers a uniform metric to evaluate the efficacy of various PINN methodologies across tasks.\nNext, the authors embark on an in-depth evaluation using the benchmark dataset to appraise these variations. They measure the performance through multiple indicators such as accuracy, convergence speed, and computational prowess. Their findings elucidate the advantages and pitfalls of these methods, charting a course for prospective studies, especially in areas like domain decomposition and loss reweighting.\nMoreover, the challenges pinpointed in the dataset find parallels in many real-world scenarios. Hence, how a method navigates these challenges becomes a tangible testament to its applicability in practical contexts. This tangible applicability amplifies the relevance of both the benchmark suite and the research's findings to field professionals and researchers.\nIn conclusion, this work marks a significant leap in the trajectory of PINNs, fueling further innovation and exploration in this riveting domain. The paper's offerings, spanning from the benchmark suite to the critical insights, are poised to galvanize more in-depth investigations and advancements in PINNs, ushering in enhanced solutions for real-world quandaries.\n", "questions": "The paper has some areas it could improve on.\nFirst, the authors only discuss PINN methods. They didn't look at other common methods. It would be good to see how PINN methods compare to these.\nSecond, they didn't give much detail on what computer stuff is needed for PINN methods. They did say if the methods work fast or slow. But, it would be helpful to know what computer tools or power is needed. People who want to use these methods would find that information useful.\nLast, the authors worked with a set of 20 PDE problems. But they might have missed some other important problems. In future studies, it would be good to add more problems to their list. This way, we can learn even more.\n"}]}, {"name": "SaNN: Simple Yet Powerful Simplicial-aware Neural Networks", "key_word": ["Graph Neural Networks", " Higher-order Representation Learning", " Simplicial Complexes", " Simplicial Neural Networks", " Weisfeiler-Lehman Isomorphism Test"], "sn": 9491, "reviews": [{"mark": [4, 3, 4], "rate": 8, "confidence": 4}, {"mark": [3, 4, 3], "rate": 8, "confidence": 2}, {"mark": [4, 3, 2], "rate": 5, "confidence": 3}, {"mark": [2, 3, 2], "rate": 3, "confidence": 4}], "abstract": "Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman (SWL) test. We also show that SaNN is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories,  simplicial closures, and classifying graphs.\n", "detail": [{"summary": "The paper describes an efficient, and effective approach for learning representations for simplices in a simplicial complex. The central idea is that of using injective functions for aggregating simplicial features, as it ensures that the embeddings are unique. The simplicial features are aggregated over upper, lower, boundary and co-boundary adjacencies. The paper provides precise definitions and theorems and statements on the properties of the networks. The proofs are summarized in the main body and provided in full detail in the appendices. The method is further experimentally validated and shows that the proposed model (SaNN) is both efficients (significantly faster than any of the other baselines) and effective (performance within the uncertainty intervals on accurcies, or above the baselines).\n", "strengths": "\nI am impressed by the clarity of presentation in the paper. I find talking and reading about simplicial complex often a messy business given all the types of simplices and adjacencies, and the abstract notion in the first place. It is clear that the authors though well about how to present the math. This includes proper use of figures.\nThe goal of the paper itself -efficiency whilst not compromising on expressivity- is relevant and important, and it is great to see the authors succeeding in reaching this goal.\nI appreciate the summary of the proofs after the formal statements.\nNext to a sound theoretical exposition, the experiments are thorough as well and include many ablation studies that are used to distill insightful take home messages.\n\n", "weaknesses": "I only have 1 important concern:\n\nAlthough the main principles are clear, I am still confused about the actual architecture/predictive models. In the end we have equation 8, but it describes a representation for each of the N sets of k-simplices, each consisting of the Nk simplices. It is unclear how to distill a global prediction out of all these representations, as would be needed for e.g. the classification tasks. Details on how the architectural design for each of the benchmarks is missing.\n\n", "questions": "Could you respond to the above concern, and additionally address the following questions/comments?\n\nOn several occasions the notion of \"non-deep baselines\" is used. What is meant by this. Could you clarify what non-deep means here, which methods are these?\n\nIn section 2 when presenting the symbols it is mentioned that k=1,2,\u2026,N+1. Does k always run up all the way to N+1?\n\nIn section 4. The sentence that starts with \"The theorem implies that any arbitrary ...\" is extremely long and hard to comprehend. I suggest to split it 2 or 3 sentence to improve readability.\n\nJust above property 1 it is mentioned \"other commonly used sum, mean, or max read-out functions are not injective\" I am not fully sure I understand it correctly. The paragraph above explains that sum aggregation is the best injective aggregator, in contrast to mean aggregation. I think the statement that I just quoted is about aggregating over the different Y's? Perhaps this can be clarified.\n\nIn the tables: since colors red and blue are used you might as well color the text in the caption as well. I.e. \"The {\\color{red}first} and {\\color{blue}second} best performances ...\"\n\nThe insights section says \"The deep models are observed to perform exceptionally better than logistic regression\", where do I see this? Logistic regression taking what as input? Could this be clarified.\n\n\nThank you for considering my comments and questions.\n"}, {"summary": "The authors propose a class of simple simplicial neural network models, referred to as simplicial-aware neural\nnetworks (SaNNs), which leverage precomputation of simplicial features. The authors theoretically demonstrate that under certain conditions, SaNNs are better discriminators of non-isomorphic graphs than the WL and SWL test. Empirically, SaNNs are shown to perform competitively against other SNNs and GNNs on tasks such as trajectory prediction, simplicial closure prediction, and several graph classification tasks over various datasets.\n", "strengths": "\nThe theoretical results are intriguing. Indeed, a competitor to the WL and SWL tests would be a valuable contribution to the graph ML community. \n\nA wide variety of benchmarks over several tasks and datasets are conducted to demonstrate the efficacy and efficiency of SaNNs. \n\nSaNNs inherit several valuable invariance properties of other SNNs including permutation invariance, orientation invariance, and simplicial-awareness. \n\nCompared to MPSNs, consideration of higher-order simplices does not blow up computation complexity.\n\n\n", "weaknesses": "\nIt is unclear for a research with limited expertise in this rather niche area to conclude the strength of the conditions prescribed in Theorems 4.1 and 4.2. (See questions.) \n\nThere do not appear to be any results describing the pre-computation time which should be included in any run-time comparisons which I imagine should scale near-exponentially with graph size and order of simplices considered. \n\nSaNNs are often not outright the winner in terms of prediction accuracies for the tasks displayed in Tables 1 and 3. For example, in Table 1, the SaNN is outcompeted by Projection and Scone on 3/4 of the datasets and the run-time savings of SaNN are not significant enough to justify usage of the SaNN. In Table 3, SaNN is not the leader in 4/5 of the datasets and it is not even the fastest. On the other hand, the time savings against MPSN are quite significant, but since many practitioners of graph learning expect training to take significant amounts of time, accuracy is the topmost priority, so there wouldn't be a strong enough justification to go with a SaNN.\n\n\n", "questions": "\nIs assuming the learnable transformation functions gk(t)\u22c5) are injective too strong? Although the MLPs will be injective, appealing to the Universal Approximation Theorem to declare that gk can be injectively-approximated is probably not practical. \n\nI may have missed this but are pre-computation times explicitly indicated in the results?\n\n\n"}, {"summary": "The authors present a Simplicial Graph Neural Network, which considers higher-order structures in the input graphs. In comparison to previous work, the features from k-simplices are precomputed without trainable parameters and only then fed into a GNN. This leads to lower runtime during training since features can be reused in each epoch, which is validated by the authors theoretically and empirically.\nThe authors prove that their method is more powerful than the WL test and as powerful as the Simplicial WL (SWL) test, when it comes to distinguishing non-isomorphic subgraphs. Further, they prove permutation equivariance, orientation equivariance, and simplicial-awareness.\nThe method is evaluated on trajectory prediction, simplicial closure prediction, and graph classification, where it is on par/slightly outperforms previous works with better training runtimes.\n", "strengths": "\nThe goal of the work, achieving better scalability of expressive networks by using non-parametric simplicial encoders makes sense.\nThe authors thoroughly analyze their method theoretically and provide proofs for all relevant properties.\nThe presented method seems to find a good trade-off between expressiveness, runtime and empirical quality.\nThere is theoretical value in the non-parametric encoder for simplices that keeps equivariant properties and simplicial-awareness\nThe paper is mostly well written\n\n", "weaknesses": "\nRuntime and asymptotic comparisons in this work are done by excluding the precomputation of features. I think this is misleading, since in practice, the precomputation is certainly part of the computation, especially during inference. Thus, the presented gains seem to be only valid during training, when the features need to be computed only once for many iterations of training. \n\nAt the same time, the method only performs on par with previous work, with small gains on some datasets.\n\nThe method requires many hyper parameter choices like hops, T, k, which seem to have different optimal settings on different datasets. The result quality differs substantially depending on the configuration.\n\nI am skeptical regarding the practical relevance of the presented method due to above reasons.\n\nThe method lacks conceptual novelty. The main idea of precomputing features by non-learnable functions has been seen in other areas, e.g. non-parametric GNNs. The general structure of the work follows a long line of work about GNN expressiveness (higher order and WL-level) without presenting many novel insights.\n\n\n", "questions": "\nI wonder how the method compares to previous methods in inference runtime, when feature precomputation needs to be included.\n\n"}, {"summary": "This paper considers the design of neural networks for simplicial complexes, which are more general combinatorial structures than graphs, but less general than hypergraphs. The authors propose to use multihop aggregation schemes to build an architecture that is more expressive than the simplicial Weisfeiler-Lehman isomorphism test, while satisfying useful invariance, equivariance, and expressivity properties. They also demonstrate the efficientcy of their proposed method, and its performance for a few different tasks in simplicial data processing.\n", "strengths": "\nFor the most part, this paper is well-written, and is easy to digest for someone who is familiar with graph neural networks. I don't think the intended audience of this paper includes someone not familiar with GNNs, but this is fine in my opinion.\n\nThe proposed method is demonstrated to be quite efficient in comparison to existing ones, with similar performance as well.\n\n\n", "weaknesses": "\nCertain definitions regarding the types of operators and features are not laid out clearly enough, which leads to ambiguity in the paper on a technical level. As noted in the list of questions and suggestions, the claimed properties of the proposed models are not clearly true, possibly due to this misunderstanding.\n\n", "questions": "My most important concern is summarized in point 1 -- in particular, the ambiguities around orientation equivariance and the use of oriented operators built from the incidence matrices are what cause me to suggest this paper be rejected. If the authors are to focus on either of the two points in order to change my mind on this paper, it should be the first one.\n\nThere are some details missing regarding the type of data being handled. In particular, the incidence matrices are not defined in a way sufficient for the discussion following in the paper. Normally, the incidence matrices have values of +-1 depending on a chosen reference orientation (usually given by some ordering of the nodes). Coupled to this, the signs of the features on the simplices are determined relative to the same reference orientation -- this gives meaning to the notion of orientation equivariance. Without discussing these things, orientation equivariance is not a meaningful concept within the context of the paper.\n\na. This calls into question the validity of the example in Section 4.1. You say that all simplices are given a feature value given by some scalar a -- yet, the matrices acting on these feature vectors/matrices have an orientation associated to them. It seems as if you are using an oriented operator to act on unoriented features. Property 1 in this example is thus difficult to claim, as the property of orientation equivariance is one describing the action of oriented operators acting on oriented features, and how the choice of orientation to begin with is irrelevant to the computation. \nb. Furthermore, this problem yields a comparison for isomorphism testing incorrect, as the erroneous imposition of differently-oriented features relative to the chosen orientations could be used by SaNN to yield a \"false negative,\" i.e., saying that two isomorphic complexes are different. \nc. A more minor comment in this direction comes from the Insights section of Section 5.1. It is not correct to say that \"the superior performance of SaNN also proves the orientation equivariance of SaNN experimentally.\" Orientation equivariance is a simple mathematical property, and does not guarantee good performance, nor are all performant architectures on a given dataset orientation equivariant. These properties are possibly linked, but the claim that one proves the other in some way is not justified. \nd. Moreover, based on my reading of the appendix, many of their experimental setups for tasks other than trajectory prediction use \"unoriented data\" by simply assigning scalar values to high-order simplices, which is again incompatible with the use of oriented operators. Perhaps something in the implementation of SaNN in these examples does not use oriented operators such as the incidence matrices, but this is not clear to me. \nPlease either justify, clarify, or revise the paper's discussion regarding orientation equivariance.\n\nRelated to the above point, the claims in Section 4.2 seem reasonable at first glance, but are not explained well enough. Permutation equivariance is easily seen to hold, so is not much of a concern. Orientation equivariance is subject to the problems noted above, so more clarification on the type of simplicial features and relevant operators needs to be made. That is not to say that the result proved in the appendix is wrong, but it needs to be clarified in order to be understood in a way that acts on oriented features. Simplicial awareness is more subtle than the other two, based on the definition from (Roddenberry et. al., 2021). For instance, some of the existing convolutional-type SNNs in the literature fail to satisfy simplicial awareness if they are implemented without nonlinear activation functions, due to the fact that the square of the (co)boundary operator is the zero operator. Perhaps it is the case that the assumptions of Theorem 4.2 are sufficient to exclude such methods, but a clearer connection is needed. It would be very helpful for the authors to briefly survey some of the methods they compare to, and clarify whether Theorems 4.1 and 4.2 apply or don't apply to them.\n\n"}]}, {"name": "FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural Architecture Search", "key_word": ["Neural Architecture Search", " Performance Predictor", " Graph Neural Network"], "sn": 9483, "reviews": [{"mark": [3, 3, 2], "rate": 3, "confidence": 5}, {"mark": [2, 2, 1], "rate": 3, "confidence": 5}, {"mark": [3, 3, 3], "rate": 5, "confidence": 4}, {"mark": [2, 2, 2], "rate": 5, "confidence": 4}], "abstract": "Neural Architecture Search (NAS) has risen to prominence as a pivotal tool for identifying optimal configurations for deep neural networks suited to particular tasks. However, the process of training and assessing numerous architectures introduces considerable computational overhead. One approach to mitigate this is through performance predictors, which offer a means to estimate an architecture's potential without exhaustive training. Given that neural architectures fundamentally resemble directed acyclic graphs (DAGs), graph neural networks (GNNs) become an apparent choice for such predictive tasks. Nevertheless, the scarcity of training data can impact the precision of GNN-based predictors.\nTo address this, we introduce a novel GNN predictor for NAS. This predictor renders neural architectures into vector representations by combining both the conventional and inverse graph views. Additionally, we incorporate a tailored feature loss within the GNN predictor to ensure efficient utilization of both types of representations.\nWe subsequently assess our method's efficacy through experiments on benchmark datasets including NASBench-101, NASBench-201, and the DARTS search space, with a training data range of 50 to 400 samples. The results demonstrated a notable performance improvement, achieving an enhancement of 3%-16% in terms of prediction accuracy when compared to state-of-the-art GNN predictors across the board.\nThe source code will be made publicly available.\n", "detail": [{"summary": "This paper proposes a new GNN performance predictor for NAS that considers the forward and reverse computational graph of architectures. Furthermore, the authors also propose a loss function that minimizes the variance between the dual encodings of the forward and backward pass. Experiments in standard tabular and surrogate benchmarks show improvements NPNAS and NPENAS.\n", "strengths": "\nThe paper presents a simple and effective way to improve the predictive performance of GNNs for NAS. The empirical evaluation demonstrates that the performance increases with the number of datapoints, which is nice to see.\n\nEasy to read and clearly written.\n\nCompared to NPNAS and NPENAS, the proposed algorithm shows significant improvements.\n\n\n", "weaknesses": "\nThe proposed method to encode both the forward and backwards encoding is well-known in literature (see section 3.4 in [1] for instance) as well as in NAS [2]. The linear scalarization of the prediction loss with the loss term that minimizes the variance between the two encoders is trivial.\n\nThe authors evaluate their method on 3 tabular/surrogate benchmarks. I think this is not enough considering the diversity of available NAS benchmarks out there. There are more interesting NAS benchmarks (see NAS-Bench-Suite [3]) that also have evaluated NPENAS, and therefore makes the comparison to the proposed method possible.\n\nNo code available at submission time.\n\n\nReferences\n[1] https://arxiv.org/pdf/1904.11088.pdf\n[2] https://arxiv.org/pdf/2010.04683.pdf\n[3] https://arxiv.org/pdf/2201.13396.pdf\n", "questions": "\nCan the authors evaluate their method on the same framework as used in [4]? It would be great to see how FR-NAS performs under the same settings as those methods are evaluated.\n\nWhat is the performance of the predictor inside a NAS algorithm? Can you evaluate FR-NAS as done in NAS-Bench-Suite (see Table 2)?\n\n\nReferences\n[4] https://arxiv.org/pdf/2104.01177.pdf\n"}, {"summary": "This paper proposes FR-NAS, a neural architecture performance predictors that estimates performance using both the forward-pass and backwards-pass representation of a NAS architecture. FR-NAS uses an Instance Relation Graph (IRG) loss to train the dual encoder. The author's evaluate the method on three NAS-Benchmarks and compare to known predictors NPENAS and NPNAS, outperforming both.\n", "strengths": "There is some novelty to considering the backwards pass representation of a NAS architecture when making a prediction. \nThe evaluation shows that FR-NAS conclusively defeats NPNAS and NPENAS on NAS-Bench-{101, 201, 301} at every training dataset size.\nThere are additional ablation studies for some components.\n", "weaknesses": "The novelty of this work is somewhat limited as its really only using a dual encoder with adjacency matrix transpose, while components like the IGR loss are heavily borrowed from different work.\nThis work only considers experiments on cell-based NAS Benchmarks but not on real NAS problems, which are outperformed by macro-based NAS structures like Once-for-All/MobileNets/EfficientNets. Also, no search is applied and no found architectures are evaluated. \nThere are probably simpler ways to consider the backwards-pass representation of a NAS DAG which this paper does not consider. The trivial method would be to simply cast the DAG as a fully-directed graph (for every edge (i, j), add edge (j, i)) and still use a simpler encoder. Another way, also simpler than this would be to consider weighted edges, e.g., forward-pass edges have weight '1', backwards-pass have weight '-1', and you use torch.nn.GINEConv instead of torch.nn.GINConv. \nThe IGR loss in this paper is somewhat counter-intuitive. The intuition behind considering the transposed adjacency matrix is that you are providing the predictor with new information not found in the forward-pass adj. matrix. This information should allow the predictor to learn a better understanding of architecture performance, which would help performance. Under this assumption, you would expect the encodings of the forward encode and backwards encoder to probably be different as they should be learning on distinct information, and that the concatenation of that information (graph embeddings from each encoder) benefits predictor performance. Instead, the IGR loss is counter to this as it forces both the forward/backwards encoders to 'learn the same thing' using different views of the same data. In other words, in Figs 2-3, showing how the IRG matrix values goes down with the addition of the loss and more samples seems counter-intuitive.\nAnalysis in Figures 2 and 3 is missing NAS-Bench-201 using the IGR loss, NAS-Bench-301 400 samples with the loss, even though the manuscript is not even 9 pages.\nAuthor's mention Graph Attention Networks (GAT) a few times in the paper, but do not use them to perform any analysis on their encoders, e.g., highlighting the nodes/edges assigned high attention scores. This would be a good way to highlight how their method learns and the benefits of their design, and rebut the hypothesis that FR-NAS outperforms NPNAS and NPENAS simply because the predictor has more parameters.\nThere are a lot of missing entries in the related work section. Some of which should be added, and compared to, e.g.,\n\nTNASP [1] and PINAT [2] deal with special encodings for the adjacency matrix, like this paper.\nCDP [3] is a cross-domain predictor which cases 201 and 301 to be like 101, to deal with limited target data like this paper.\nGENNAPE [4] also deal with limited target data like CDP, but they also utilize a robust form of Computational Graph that covers the entire architecture, not just the NAS cell design.\nMulti-Predict [5] show how to leverage other information like Zero-Cost Proxies [6] and device latency/FLOPs to aid prediction - both of which this paper does not acknowledge, yet it is a critical concern of NAS.\n\nFor the above reasons I would recommend rejection of this manuscript. \nReferences:\n[1] Lu et al., \"TNASP: A Transformer-based NAS Predictor with a Self-Evolution Framework\", in NeurIPS 2021.\n[2] Lu et al., \"PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor\", in AAAI-23.\n[3] Liu et al., \"Bridge the Gap Between Architecture Spaces via a Cross-Domain Predictor\", in NeurIPS 2022.\n[4] Mills et al., \"GENNAPE: Towards Generalized Neural Architecture Performance Estimators\", in AAAI-23.\n[5] Akhauri and Abdelfattah, \"Multi-Predict: Few Shot Predictors For Efficient Neural Architecture Search\", in AutoML Conf 2023.\n[6] Abdelfattah et al., \"Zero-Cost Proxies for Lightweight NAS\", in ICLR 2021.\n", "questions": "Not a question but minor nitpick: Eq. 2 should be Enc(A_{T}, O; W_2)\n"}, {"summary": "The FR-NAS paper devised a new graph neural network (GNN) based surrogate model for neural architecture search. The adjacency matrix is passed to a GNN which encodes the forward propagation of the neural network. Its transpose is passed to another GNN which encodes the backward propagation. These encodings are passed to their respective predictors (pf and pr) and the final predictor is an average of these two. pf and pr are trained using mean squared error loss between the predicted and the true accuracies of the networks. To ensure that the forward and backward embeddings are consistent with each other, they used an additional loss to enforce that the relative distance between two architectures in the forward embedding space and the backward embedding space is similar.\n They report the results on NasBench-101, NasBench-201 and Darts search space. In their ablation studies, they bolster the case for using both the forward and the backward pass encodings.\n", "strengths": "\nUsing two encoders to capture the forward and backward propagation encodings and using the IRG loss to synchronize them is novel.\nTheir algorithm outperforms the other 2 baselines on NASBench-101, NASBench-201 and the DARTS search space.\n\n", "weaknesses": "\nPlease compare against [1], [2] which are also GCN based predictors. \nIt is also important to demonstrate that the surrogate model is competitive to other baselines such as those included in Neural architecture optimization (NAO) [3], BANANAS [4] and other predictors in  [5]\nFor all the baselines, please report the time taken to train the predictors and to compute the correlations on all the 3 benchmarks.\n\n[1] BRP-NAS: Prediction-based NAS using GCNs,  Dudziak et al.\n[2] Bridging the gap between sample-based and one-shot neural architecture search with bonas, Shi et al.\n[3] Neural Architecture Optimization, Luo et al.\n[4] BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search, White et al.\n[5] How Powerful are Performance Predictors in Neural Architecture Search? White et al.\n", "questions": "\nCan you please tabulate figure 5? Given that the NPENAS-FR and FR-NAS plots are very close to each other as the training data increases, it would be good to see the actual correlation values.\n2.Did you consider other alternatives to the feature loss?  Given that both Lpf and Lpr are predicting the accuracy of the same architecture, what would happen if you minimize the divergence between the outputs of Lpf and Lpr predictors?\nGiven that the algorithm is trained to minimize the feature loss, it would have the least Diff(i,j) when compared to those that are trained without them. So is figure 3 a fair comparison?\n\n"}, {"summary": "This paper proposed a GNN-based performance predictor for NAS, the bidirectionality information is employed for performance improvement, and some experiments are conducted for verification.\n", "strengths": "Using the bidirectionality information to improve the performance of the predictor is quite interesting because almost all existing works did not recognize this point.\n", "weaknesses": "The peer competitors used for comparison in this paper are not SOTA. This paper should not only compare the methods based on GNN but also SOTA performance predictors based on other techniques.\nThe experiments should also go to ImageNet, instead of only the measures for performance predictors, the final goal of which is for NAS.\n", "questions": "See Above\n"}]}, {"name": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models", "key_word": ["Large Language Models; Prompt Engineering; Boosting Mechanism;"], "sn": 9482, "reviews": [{"mark": [2, 3, 2], "rate": 5, "confidence": 3}, {"mark": [3, 2, 3], "rate": 8, "confidence": 3}, {"mark": [2, 2, 2], "rate": 6, "confidence": 3}, {"mark": [3, 3, 2], "rate": 5, "confidence": 4}, {"mark": [3, 3, 3], "rate": 8, "confidence": 3}], "abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with GPT-4 and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches.\n", "detail": [{"summary": "The paper proposes a new framework Boosting of Thoughts (BoT) with large language models (LLMs) for task-specific prompting. It provides how to construct prompts and use the trial-and-error reasoning approach to interact with the LLM to generate the final responses. The experiments show the effectiveness of the proposed method.\n", "strengths": "\nPrompt engineering is a non-trivial task, and crafting effective prompts may require specialized training for human experts. The paper introduces an innovative framework for iterative prompting, leveraging LLM's feedback on its own reasoning, thereby reducing the need for human prompt engineering.\n\nAddressing complex problems is crucial in LLM applications. This approach effectively demonstrates the power of prompt engineering and expands the capabilities of LLMs without the need for retraining or fine-tuning. Experiments conducted on multiple datasets show competitive performance compared to other prompting approaches.\n\n\n", "weaknesses": "\nI agree that prompt engineering is crucial for LLM applications. However, it's worth noting that prompt engineering is often model-dependent, and the techniques may evolve as LLM capabilities improve. This may not offer long-term guidance for research unless it uncovers fundamental insights. This distinction is critical in differentiating academic research from practical production. Therefore, while the paper does offer valuable techniques for prompting the model and achieving good results on evaluation sets, it lacks in-depth discussion of the underlying reasons. This makes the paper better suited for application-oriented conferences rather than ICLR.\n\nLLMs can be unstable and prone to hallucination, which could result in bad or incorrect feedback when using the Boosting of Thoughts (BoT) iterative prompting framework. Is there analysis on the impact of \"bad\" LLM feedback? Further, as the iterative produces are automatic, spurious feedback could get amplified over iterations. Some discuss may be necessary.\n\nDetails are lacking on key components like aggregation strategies and generating edge weights for trees. More analysis or ablation studies are also helpful.\n\n\n", "questions": "\nIn Section 3.2, it is not quite clear how to calculate the weights for the weighted binary tree.\n\n"}, {"summary": "The paper presents an extension of the Chain of Thought (CoT) and Tree of Thoughts (ToT) method, named Boosting of Thoughts (BoT). BoT refines the problem-solving process in large language models (LLMs). BoT harnesses error analysis to improve the LLM's problem-solving accuracy iteratively. The \"Boosting of Thoughts\" (BoT) procedure is a two-step process that first generates a diversity of reasoning paths from a Large Language Model (LLM) in the form of a weighted binary tree, enhancing problem-solving by creating a hierarchy of potential solutions. Then, it employs a novel aggregation strategy that iteratively refines and combines these paths. Through best-first and greedy aggregations, BoT selects and optimizes the most promising chain of thought, using iterative feedback to progressively improve the LLM's performance on complex problem-solving tasks. The paper reports improved performance on complex mathematical problems when tested with GPT-4 and LLAMA2, compared to CoT and ToT.\n", "strengths": "\nThis is an innovative extension of the Chain-of-Thought (CoT) and Tree-of-Thought (ToT) methods. Compared to CoT and ToT, the author adopts the idea on leveraging error analysis to refine the LLM. This can be a limitation of CoT and ToT, as they do not conduct error analysis and more importantly, learn from errors. The motivation is intuitive and clear.\n\nUnlike ToT, which expands multiple reasoning tree branches, the BoT method iteratively refines a single line of thought. This focus on iteration rather than expansion allows for a more concentrated and efficient improvement of the reasoning path. The computation moves from exploring the tree into learning from erroneous trials. \n\nThe Boosting of Thoughts (BoT) concept shows a clear advancement in problem-solving methodologies within large language models. It effectively combines generation and evaluation steps to progressively enhance reasoning, demonstrating a significant leap in the model's ability to handle complex tasks. \n\nThe experiments are clear, the results are effective. And all experiments are classic experiments from CoT and ToT, so it is clear to compare BoT\u2019s performance over CoT and ToT.\n\n\n", "weaknesses": "The mauscript need polished in their figures' presentation, e.g., the authors need give more detailed examples in Fig1.\n", "questions": "Q1: In the prompt, I wonder whether the \u201cerror input\u201d are included, or only the \u201cexperience\u201d is included? From figure 1, I only see \u201cerror report\u201d like \u201cstep 1 is not closer to 24\u201d, no \u201cerror input\u201d like what is step 1, 2, 3. How the LLM know what step 1 mean, and how can LLM learn from error, if LLM does not know specific input?\nQ2: How about you consider the entire (input, error analysis) as an In-context Learning example? Then the entire method is similar to CoT, meaning that you can manually construct an exemplar consisting of (input, error analysis) pair. Then use the CoT idea to follow the strategy to generate analysis and think about the correct answer.\n"}, {"summary": "The paper proposes a Boosting of Thoughts (BoT) framework, which aims to achieve the boosting mechanism that embraces aggregation and experience, thereby enabling the progressive refinement of unreliable reasoning steps (weak thoughts) by learning from errors to solve various problems, eventually.\n", "strengths": "This paper reiterates their proposition that a simple prompt can be enhanced by gradually accumulating error analysis on its generated thoughts to address complex tasks.  The authors present a novel framework, the Boosting of Thoughts (BoT), to implement such progressive prompt enhancement for effective thought generation with an experience-driven iteration process. Iteratively exploring and self-evaluating the generated simplistic trees of thoughts enables a simple initial prompt to be gradually enhanced by an ensemble of trial-and-error reasoning experiences, resulting in accurate solutions. \nThis work seems like a quite comprehensive investigation with well-structured and easy to read sections.\n", "weaknesses": "The paper is based on the motivation that starting with a simple prompt without human annotations for LLMs, BoT may get weak thoughts. However, with aggregation, BoT is capable of deriving a more logical and effective thought chain from them, thereby guiding the subsequent refinement.\nCould the authors expand on this statement \"Experience consistently leads to thought revision, but too much can have the opposite effect.\"? If one is looking to recreate the study, are there any guidelines or steps one could adopt to as where should be the stopping point?\nVery interesting findings, however Experimental results reported are limited. The authors evaluated LLM models only on a single testing procedure. However, the analysis doesn't seem concrete due to the smaller sample set considered and it would truly be insightful if the analysis was done on a larger dataset to infer results.\nFurther experiments should be performed using statistical metrics, and statistical distribution of the results should be extracted. These outcomes help better support the conclusions' claims.\nThe paper would be greatly strengthened if the proposed algorithm would outperform state-of-the-art methods\n", "questions": "Please review the weakness section\n"}, {"summary": "This paper proposes a new framework called Boosting of Thoughts (BoT) for complex problem solving with large language models. BoT aims to iteratively explore many possible trees of thoughts and learn from ineffective thoughts/errors to progressively refine the prompt and elicit effective reasoning from LLMs. It aggregates the best reasoning chains from the trees and analyzes them with the LLM to gain experience on errors and revisions. Experiments on mathematical reasoning show BoT matches or exceeds previous SOTA approaches without needing human annotations.\n", "strengths": "\nThe paper proposes a novel framework, Boosting of Thoughts (BoT), that utilizes an iterative trial-and-error approach to refine prompting and elicit complex reasoning from LLMs. The key idea of learning from errors/ineffective thoughts is creative and mimics human problem-solving.\nAuthors involve interesting techniques like weighted binary trees and heterogeneous growth strategies to generate diverse, shallow thought structures from a simple prompt.\nEvaluations on mathematical reasoning benchmarks demonstrate effectiveness of BoT. It matches or exceeds state-of-the-art methods without needing human annotated prompts. Also, the authors conduct ablation studies to further explain the mechanisms.\n\n", "weaknesses": "\nAlthough this article proposes several practical strategies, its reasoning framework remains inherently reliant on the Tree-of-thoughts model, thereby limiting its novelty. BoT's structure is restricted to binary trees. Expanding to more complex graph structures will further improve reasoning but is not explored.\nFor analysis, the prompts used to seed BoT could introduce biases and variances. More evaluations on OOD data would be useful to assess the robustness and generalizability of the improvements.\nThe evaluations are mainly limited to mathematical reasoning. For generality, testing BoT's performance on other domains like commonsense reasoning or symbolic reasoning is needed.\nIn the 'Competitors' paragraph, authors mentioned incorporating CoT-SC and Complex CoT as  baselines, yet CoT-SC is not shown in the 'mathematical reasoning' part. I hold the view that comparing the proposed method with prevailing baselines like SC(5) or SC(10) will offer a more direct reflection of BoT's efficacy. If it can outperform Complexity-based SC with fewer resources, it would make the work more solid.\n\n", "questions": "\nAs for the statement on page 3, 'Our paper embraces ToT due to its high ability and leaves GoT and BoT for future work,' is 'BoT' a typo error here? Or you mean combining BoT method with GoT? Regardless, I believe that including GoT in the comparison would make this work more interesting and informative.\nIn the 'Competitors' paragraph in experiments, could you clarify how many reasoning chains are sampled for Complex-CoT and PHP respectively?\nThe study conducts experiments based on GPT-4, which can lead to substantially high experimentation costs. Have the authors considered or utilized more cost-effective options like GPT-3.5-turbo? I'm aware of the recent variability in performance of this model. However, if there are experimental results showing that GPT-3.5 combined with BoT can outperform GPT-4 with CoT/CoT-SC, it would render the study's findings more convincing.\n\n"}, {"summary": "The paper looks at optimizing prompting for GPT-4 and Llama2 for solving mathematical problems. They provide an iterative strategy to prompting models for complex problems. The key challenges are SVAMP (1000 tasks), GSM8K (8500 tasks), AQUA (100 000 tasks), and MATH (12 500 tasks). The BoT, especially when enhanced with CoT, outperforms alternative methods.\n", "strengths": "When we reach the limits by simply increasing language models, optimal interaction becomes increasingly interesting. Exploring new ways of pushing the models to do more complex tasks can get more value out of existing LLMs and is highly relevant. \nThe paper provides code that is easy-to-read (although a Readme would be a nice addition).\nThe method shows that BoT and CoT perform above the comparisons.\n", "weaknesses": "The use of the term 'boosting' in the context of refining 'weak thoughts' introduces some ambiguity. In traditional machine learning, boosting involves the iterative enhancement of quantifiably weak learners. In the BoT framework, the concept of a 'weak thought' is more abstract, and its \"weakness\" is not as straightforward to measure. This led me to perceive the process more as a 'pruning of weak thoughts' rather than 'boosting' in the conventional sense. It would be beneficial for the paper to clarify how the model aggregates and refines these thoughts in the tree structure, and how the \"weakness\" of a thought is determined and improved upon.\nI think the paper comes across unnecessarily complicated, compared to the code the text is hard to fully grasp. The figures all depict Game of 24, adding examples from both a successful and a failed example of BoT for the other tasks would be beneficial.\nFor complete reproducibility and clarity, it would be beneficial to provide the full codebase, including modules like 'llmpebase\u2019s residual_tree_of_thoughts', which is referenced several times but not included.\nThe title suggests a general problem-solving approach using Large Language Models. However, the content is specifically focused on mathematical problems. It might be beneficial to make the domain-specific nature of the research clearer in the title or early in the abstract to set accurate expectations for readers.\n", "questions": "Do I understand correctly that T 10 was maximum 10 prompts and M 15 consisted of 15 instances that generated binary trees that you then averaged over?\n"}]}, {"name": "Farzi Data: Autoregressive Data Distillation", "key_word": ["Data Distillation", " Meta Learning", " Recommender Systems", " Language Modeling"], "sn": 9481, "reviews": [{"mark": [3, 3, 3], "rate": 6, "confidence": 3}, {"mark": [3, 4, 3], "rate": 6, "confidence": 4}, {"mark": [4, 4, 3], "rate": 6, "confidence": 3}, {"mark": [2, 2, 3], "rate": 5, "confidence": 2}, {"mark": [2, 2, 3], "rate": 3, "confidence": 4}], "abstract": "We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences \u2014 Farzi Data \u2014 which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 \u2212 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.\n", "detail": [{"summary": "This paper proposes FARZI, a data distillation method for auto-regressive ML tasks/event-sequence datasets. The method summarizes a large dataset into a set of synthetic sequences in latent space which can be decoded later. They show that model performance is upheld/enhanced when compared to training on the complete dataset on the downstream tasks of sequential recommendation and language modeling. For data distillation, the paper shows Adam to be better than SGD as inner loop optimizer, and derives an efficient reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps.\n", "strengths": "\nOriginality and Significance: The latent parametrization that makes FARZI optimization friendly, and the proposed trick that enables reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps are great contributions and of practical value.\nQuality and Clarity: The paper is well written with extensive experiments whose details and evaluations are that are clearly described. The results are impressive. The method is able to achieve better performance on downstream tasks compared with using the full dataset.\n\n", "weaknesses": "\nIt is not clear whether this method will be practical and scale for larger language models and larger datasets. It would be great if the authors can elaborate on this.\nThere is not a clear analysis of the total time gains of this method in comparison with training from scratch. Providing some values would make the case for this method more compelling.\n\n", "questions": "Listed in weakness section.\n"}, {"summary": "The paper introduces FARZI, a data distillation framework for machine learning tasks. The goal is to condense the original large dataset into a much smaller number of synthetic sequences, so that downstream performance on the synthetic data matches (or even improves) performance on the full real dataset. The authors cast the problem using a bi-level optimization formulation, similar to meta-model matching based dataset distillation. The naive formulation is infeasible due to the very large token vocabulary and the maximum sequence length. To address this, the authors propose to factorize the synthetic dataset into a latent data summary and a token-decoder matrix. This renders the optimization continuous (as opposed to discrete), while it provides flexibility to sample synthetic sentences from a distribution (as opposed to having a fixed small set of synthetic sentences). Furthermore, the authors suggest to replace SGD in the inner loop by the Adam optimizer. To mitigate the large memory footprint, they derive an efficient approximation for reverse-model differentiation of the Adam optimization. The authors assess FARZI on sequential recommendation and language modeling tasks, where they manage to match or even exceed the downstream full-data performance using as little as 0.1% of the original dataset. The authors conduct several experiments and ablation studies to shed light on various aspects of their framework.\n", "strengths": "The paper makes several interesting contributions. The meta-model matching based dataset distillation was originally proposed for continuous data (e.g., image data), as opposed to language data that use discrete tokens. The use of a latent space addresses this challenge by ensuring that the optimization can be performed in a continuous space, but by also allowing us to sample the synthetic sentences from a compact distribution. Furthermore, the observation that the Adam optimizer is a much better choice for the inner loop optimization (compared to SGD) is very interesting and dramatically improves downstream performance. To address the large memory footprint, the authors derive an efficient approximation of the reverse-mode differentiation of the Adam optimizer, which nicely complements their finding that Adam is better than SGD. Interestingly, this may be more broadly applicable in other bi-level optimization tasks (e.g., in a meta-learning context).\nThe paper is well written and the related work is covered quite extensively. The authors describe in detail the various insights of their framework. When it comes to the experimental evaluation, they provide a lot of information on the metrics, datasets, hyperparameters, objectives, and even architectures.\nThe experimental evaluation is quite convincing and supports the claims made by the authors. It is very interesting that FARZI can even outperform downstream performance on the full original dataset, which could indicate the improved robustness with dataset distillation. I liked the fact that the authors investigated various aspects of FARZI, such as the versatility of the synthetic data, the cross-architecture generalization, the performance of different meta-objectives, the cold start problem, and the impact of pre-trained trajectories.\n", "weaknesses": "\nEven though this paper makes interesting contributions to the DD literature for autoregressive tasks, it is not so obvious that it would be \nvery helpful for much larger text corpora and large language models with millions or billions of parameters. The memory footprint might end up being very large, rendering the whole framework infeasible. Furthermore, a compression rate of 0.1% may not be extremely helpful for very large datasets consisting of billions of sentences. This may limit the applicability of FARZI to settings consisting of \"reasonably large but not very large\" language corpora.\n\nIt was not clear to me how time-consuming the FARZI dataset generation process is. For example, how long did it take to generate the synthetic datasets for the tasks considered in this work? In particular, did FARZI improve the total runtime? For instance, if generating the synthetic data takes very long, then there may be very little benefit (if any) from this process. Furthermore, it is not automatically obvious that a smaller dataset can be trained faster than a larger one. There is the added question of the number of epochs required to reach convergence. The synthetic dataset may require more rounds. This was not obvious in the experimental evaluation. If I am not mistaken, I feel that the subject of runtime was only superficially touched in this work, and a more thorough discussion (with detailed pros and cons) would be needed.\n(Theoretically, this may not be a big issue if the same synthetic dataset could be successful used on several downstream tasks, but this is not immediately true. If we need dataset distillation for each separate task, then we may end up performing FARZI several times.)\n\n\n", "questions": "\nCould the authors elaborate more on the total runtime (total time for synthetic dataset generation + total time for downstream training with synthetic vs. full data)? It would be helpful if the authors could shed light on the various questions/comments raised in Weakness (2) above.\n\nIn Equation (2), \\Omega is a set containing initializations for the inner loop, if I understand correctly. But instead of picking the initialization randomly, these come from a small number of training trajectories on the full dataset. If that is true, then the \\theta_i in the definition of \\Omega has nothing to do with the update rule for \\theta_t in Equation (2). This may still be confusing to some readers though because the same symbols are used (theta with a subscript, so the authors may want to clarify this point (i.e., what exactly is in \\Omega).\n\nI was not clear how exactly the authors chose the final hyperparameters for each setting. Did they exhaustively try all corresponding combinations in the hyperparameter table and picked the best one?\n\nIs a new synthetic batch created at the beginning of each outer-loop step based on the latent factorization?\n\n\n"}, {"summary": "This paper proposes a method for distillation of \"auto-regressive data\", in this case meaning any data that is represented as event sequences. This can include natural language text, but also general time-series data. Their method aims to summarize a dataset into a sequence of latent embeddings (which can subsequently be decoded) given a downstream task such that they achieve similar performance to training on the complete dataset. They do this through a meta-learning procedure, optimizing directly through Adam for data which lowers downstream task loss.\n", "strengths": "My review comes from the point of view of someone familiar with training on natural language (and associated downstream evaluation), but not general event forecasting problems. I was not familiar with the benchmarks used by the author prior to reading this paper. \nOriginality and Significance\n\nThe paper seems original. Aspects of this work (e.g. using meta-learning/second order methods) for distillation have been touched on in the past, but usually for smaller datasets, and generally not for auto-regressive tasks. Most past works I have seen which work on large corpuses revolve around finding mixing coefficients for existing datasets [1]. This method doesn't work on datasets of that size, however this shows an improvement in scaling. \nGetting a meta-learning approach to work on such dataset sizes is quite difficult, given difficulties with estimating second-order components over the full dataset. Scaling this to even larger language-style datasets would be an interesting (future) contribution.\n\nQuality and Clarity\nThis paper is quite well-written. Experimental details are clear, and the method is properly motivated. Diagrams clarify the algorithm and the key difficulties to this method are highlighted appropriately.\n[1] The Pile: An 800GB Dataset of Diverse Text for Language Modeling, Gao et al. 2021\n", "weaknesses": "Weaknesses\n\nThe authors touch on language datasets as a motivation, however do not study this (or other large-sequence tasks) due to practical model/sequence length scaling constraints. Are there reasonable paths forward that would allow this to scale to longer sequence lengths/larger models? \nGiven that the outer loop evaluates across the full original dataset, and the inner loop needs to be run several times to get updated parameters (Figure 5), what's the overall cost saving versus just training a model on the original dataset for more time (until matching student performance), if any? \nHave the authors thought about cases where there is significant noise in the training corpus? Given that the loss is computed with respect to the original dataset, it seems like this could be a problem if one ever tried to directly filter a noisy web-crawl.\n\n", "questions": "All questions have been included in the \"Weaknesses\" section above.\n"}, {"summary": "The paper provides an extension of dataset distillation to sequence modeling along with a few other innovations, such as a low rank approximation of the distilled dataset and an efficient trick to save memory during meta-learning. Overall, the paper contains strong (albeit limited) empirical results on the sequence modeling (penn tree bank) and recommendation systems datasets.\n", "strengths": "\nThe high level motivation of the problem is quite the need of the hour, as with larger models we need to better understand their dependencies on the data\nPursuit of this research direction could potentially yield methods that enable us to train SOTA transformer models for a fraction of the input cost\nEmpirical results are thorough, although a bit limited in terms of number of datasets for sequence modeling (only PTB is used)\n\n", "weaknesses": "A number of points about the approach were unclear to me from the writeup, and I would appreciate clarifications from the authors:\n\nIt is said that the complexity of the dataset distillation algorithm scales by the size of the vocabulary (page. 4) and the size of the sequence that we wish to model. I can see the latter to be the case, since the loss will now be summed over the entire sequence as opposed to one forward pass (so the complexity of the forward pass is increased). However, I do not see how the time complexity increases with the vocabulary size. Do we mean space complexity? Also, more than the forward pass the dominant factor in dataset distillation is the computation of a bunch of hessian vector products in the meta gradient. Those terms do not depend on the vocabulary size either\u2026 please clarify..\n\nIt would be nice to provide an intuition for what is saving the memory, making things O(1) in memory.  Currently the big algorithm block does not provide an intuition for how this approach is O(1) in memory regardless of the number of timesteps of unrolling. This is important to clarify, since this is an important contribution, if clearly explained. If this approach is essentially gradient checkpointing, then it is worth noting that Deng and Russakovsky already implement a version of this in their code. \n\nLooking at Eqn. 2, I am a bit puzzled as to how \\Omega, namely the trajectories from the real data are incorporated in the DD process. From what I am able to understand, \\theta_0 \\sim Omega -- namely the init is sampled from the pretrained trajectories, and then from the right hand side of eqn. 2 I understand that the rest of the trajectory is obtained using Adam on the synthetic data. Where is the role of the pretrained trajectories then? Please explain..\n\nRank regularization has been done in the previous work (Deng and Russakovsky) for dataset distillation. It should be cited that this has been done, and not be presented as a novelty..\n\n\n", "questions": "My major questions concern the clarifications about the approach listed above, without which it is really hard to judge the technical correctness / soundness of the paper.\n"}, {"summary": "The authors introduce a dataset distillation (DD) method called Farzi Data for data with a \"left to right\" (autoregressive) causal structure. Their algorithm has two novel elements: 1) the parameterization of the synthetic distilled data, which allows them to apply it to discrete data (such as the tokens in language modeling); and 2) a method for computing the outer loop gradient for DD when the inner loop is performed with Adam, which has a constant memory footprint independent of the number of inner optimization steps. They conduct extensive experiments with their proposed method on language modeling and sequential recommendation tasks. Compared to existing DD methods (adapted to discrete data via their parameterization), they obtain improved performance across the tested datasets, often obtaining downstream performance better than training a model on the entire original dataset.\n", "strengths": "Algorithmic Contribution. Algorithm 1 for computing the gradient through the inner-loop optimization with Adam using constant memory is a significant contribution. Among existing dataset distillation methods, those which take into account the entire training trajectory on the distilled data tend to obtain better accuracy (as compared to other methods which use surrogates for this objective such as the gradient matching objective in dataset condensation). However, the computational burden of these methods (specifically the memory requirement, which necessitates keeping the entire computation graph) renders them infeasible for application to larger datasets. Farzi Data takes a significant step towards addressing this problem by introducing an algorithm for differentiating through an inner loop optimized with Adam, whose memory does not scale with the number of steps in the inner loop (see Fig. 5). This is an important improvement for DD to be practically useful in real ML applications.\nEmpirical Results. The empirical results are also impressive. The authors obtain better performance than competing methods across several different real-world benchmarks. There are even scenarios where their distilled data consistently outperforms training on the entire original dataset (cf. Table 1), indicating that Farzi Data implicitly promotes some sort of \"data cleaning\" whereby samples that hurt model performance are removed or discounted. This is similar to, e.g., removing mislabeled points or data with negative Shapley values, but Farzi Data is not explicitly trained for this task.\n", "weaknesses": "Presentation and Clarity. While the actual prose of the paper was generally clear and easy to read, there are some major concerns with notation/presentation that limit understanding of some of the main contributions of the paper.\nP1. There are many cases where important notation is not defined. For instance, Rep(F,D) is defined in the Appendix, but not the main text, and is critical to interpreting Theorem 3.1. It is not stated what the terms dm, dx, and dw in Algorithm 1 are supposed to be, so it is impossible to determine if the expressions are correct or not. How to construct the output of the algorithm from these quantities is also not clear. What is the correspondence of the quantities in Alg. 1 to the DD problem, i.e., what will we actually update using the meta-gradient once we know how to compute it? Some (but not all) of these details can be found in the Appendix, but as they are critical to being able to understand the results, they should be moved to the main text and given appropriate explanations.\nP2. Stylistically, there is also some nonstandard notation. For instance, O(100) (3rd bullet point, pg. 2). I suppose the authors meant \"on the order of 100x\", but big-O notation has a mathematically precise meaning that doesn't make sense here. Another instance is Proposition 3.2. \"Correctness of Algorithm 1, Line 13\" is not a complete mathematical statement (or a complete sentence). The result should be stated completely and precisely.\nTheoretical Results. There are also issues with the theoretical results.\nT1. The most critical problem is that the proof of the main theorem (Theorem 3.1) is not mathematically sound. Specifically, the authors want to show that the expected representativeness of their low-rank synthetic data parameterization is strictly less than the expected representativeness of a naive synthetic data parameterization, under some suitable conditions and for quadratic classifiers: E[Rep(F,DF)]<E[Rep(F,DN)]. (DF and DN stand for Farzi and naive data, respectively.) In their proof in Appendix B.1, they show that E[Rep(F,DF)]<B1 and E[Rep(F,DN)] for some bounds B1 and B2. Then, since B1<B2, they conclude the desired result. This is not valid: a<b, c<d, and b<d does not imply that a<c. There needs to be a lower bound on the representativeness for the naive parameterization.\nI remark that I believe the result is (at least \"morally\") correct. The theorem essentially reduces to saying that the Rademacher complexity resulting from the low-rank parameterization is smaller than the Rademacher complexity from a general parameterization, which is intuitively obvious. However, the proof has a fatal error and must be corrected somehow.\nT2. For Lemma B.3 to hold, there must clearly be some assumptions on the loss function l; in order to apply the lemma from Shalev-Shwartz, the Rademacher complexity of the loss composed with the models in F must be considered, not F itself. As stated, I believe this lemma is not correct and the loss must be accounted for. Apart from the logical error, the motivation for the use of quadratic classifiers in the theorem wasn't clear to me. What connection do such models have to the auto-regressive tasks that Farzi Data is applied to?\nT3. This is related to the presentation problems regarding the notation used in Algorithm 1, but the proof of Proposition 3.2 is also suspect. What is meant by dm=dm+\u2202wt\u2202mt\u22c5dw? Is wt supposed to be wT, or is this expression meant to be a recursive formula? What about the formulas for the other quantities, and how are these combined to compute the meta gradient?\nIf these issues can be satisfactorily addressed, along with the questions in the section below, I would be willing to raise my score to accept, given how promising the empirical results are.\n", "questions": "Q1. The authors mention that training with the reference trajectories \u03a9 is important for obtaining the best performance, as compared with training only from randomly initialized networks. However, it wasn't clear to me if this might just have been the result of a greater number of training steps when learning the distilled dataset. That is, are the results in Fig. 6(b) with the total number of meta-gradient steps constant, or do the additional precomputed trajectories result in more meta-gradient steps?\nQ2. On a related note, it was not clear to me exactly how the precomputed trajectories were used. My assumption was that instead of training the network in the inner loop only from random initializations, instead the network from the inner loop will be initialized with parameters from one of the training trajectories. Is this correct?\nQ3. Why isn't FMLP also used as a teacher network in Table 1?\n"}]}, {"name": "BATTLE: Towards Behavior-oriented Adversarial Attacks against Deep Reinforcement Learning", "key_word": ["deep reinforcement learning", " preference-based reinforcement learning", " adversarial reinforcement learning"], "sn": 9480, "reviews": [{"mark": [2, 3, 2], "rate": 5, "confidence": 4}, {"mark": [3, 2, 2], "rate": 3, "confidence": 3}, {"mark": [2, 2, 2], "rate": 3, "confidence": 4}, {"mark": [2, 2, 3], "rate": 5, "confidence": 4}], "abstract": "Evaluating the performance of deep reinforcement learning (DRL) agents under adversarial attacks that aim to induce specific behaviors, i.e., behavior-oriented adversarial attacks, is crucial for understanding the robustness of DRL agents. Prior research primarily focuses on directing agents towards pre-determined states or policies, lacking generality and flexibility. Therefore, it is important to devise universal attacks that target inducing specific behaviors in a victim. In this work, we propose BATTLE, a universal behavior-oriented adversarial attack method. In BATTLE, an intention policy is trained to align with human preferences for flexible behavior orientation, while the adversary is trained to guide the victim policy to imitate the intention policy. To improve the attack performance, we introduce a weighting function that assigns importance weights over each state. Our empirical results over several manipulation tasks of Meta-world show the superiority of BATTLE in behavior-oriented adversarial attack settings, outperforming current adversarial attack algorithms. Furthermore, we also demonstrate that BATTLE can improve the robustness of agents under strong attacks by training with adversary. Lastly, we showcase the strong behavior-inducing capability of BATTLE by guiding Decision Transformer agents to act in line with human preferences across various MuJoCo tasks. Our videos are available in https://sites.google.com/view/jj9uxjgmba5lr3g.\n", "detail": [{"summary": "This paper studies behavior-oriented attacks agains deep RL agents, where the adversary forces the victim to have specific behaviors. The proposed attack first learns an intention policy based on human preference, and then trains an adversary to perturb the victim observation such that the behavior follows the intention policy. The adversary also adopts importance weights of states to optimize the attack objective. Experiments on multiple meta-world and mujoco show that the proposed method is able to manipulate the victim with high success rates, including offline policies based on decision transformer. The method can be also used to improve the robustness of agents.\n", "strengths": "This paper proposes an interesting type of attacks that are oriented by desired behaviors. Compared to prior works focusing on reward minimizing, the proposed attack can be more widely applicable. In real-world environments where rewards are not well-defined, such attack objective can be interesting to investigate. Learning the intention policy from human preference is also an interesting idea, although I have some concerns on it (see weakness).\n", "weaknesses": "\nThe human preference-based intention policy learning brings extra requirement and uncertainty to the process - the collection of human preference data can be expensive. More importantly, to obtain human preference labels, one need to first collect diverse behavior data so that human can pick the intended policy. Would the collection of the behavior data already involve a pre-defined target policy? (If that's the case, why not directly use the target policy for attacks?)\nIn experiments, the authors mainly evaluate the attack success rate. However, it is not clear how the success rate is defined. Is it based on whether the victim acts as the intention policy suggests? But would it be biased since the intention policy is just an approximation of the real human intention? What if the intention learning does not learn a desired reward model or intention policy?\nFor baselines, the authors used the codebases of SA-RL and PA-AD and modified their attack's reward as the learned reward. However, this straightforward modification of the PA-AD baseline contradicts with the original method (PA-AD's formulation is for a reward-minimizing adversary, so directly replacing the attacker's reward may not work). Since the original PA-AD method is to use an RL director to find the target policy and to use an actor to conduct targeted attack, a more natural modification of PA-AD in the behavior attack scenario can be to directly use the learned intention policy as the target of actor (a^ in Equation (G) in the original paper).\n\n", "questions": "How are the behavior sequences generated for human preference labeling?\n"}, {"summary": "They introduce a method to attack reinforcement learning policies. There are three key components. First, they use PbRL to train a policy to exhibit the desired behavior. This is a relatively novel step because most work in adversarial RL assumes that the desired target behavior is already known and incentivized by a reward function. Second, they train a weighting function to help prioritize relevant states and keep the next step from policy drift. Finally, they attack the target policy with an adversarial policy that makes it behave similarly to the target behavior using the weighting function.\n", "strengths": "\nWorking with both classic RL agents and decision transformers makes for much more thorough experiments.\nAdversarial policies that result in targetedly bad behavior is an area of research I believe is important and neglected.\n\n", "weaknesses": "\nWriting\nI have found some of the writing to be confusing and verbose. For example, \u201cintention policy\u201d is not defined until multiple mentions in. The description of what it is in the abstract is very ambiguous. \u201cInner\u201d and outer\u201d level loss are not described. I don\u2019t see a definition for \u201csuccess rate\u201d in the paper. I don\u2019t think the paper does as good of a job as it could with laying out things in a way that is clear and quick to understand.\nI do not understand the rationale in the first paragraph of section 4.1. I do not think this makes sense as an explanation of why an intention policy is needed instead of a direct attack.\nI do not understand figure 1. Why is there an arrow between reward learning and the replay buffer?\nNumerous grammar mistakes. I would recommend using a grammarly browser plugin.\n\n\nThis may speak to either issues with my reading of the paper, its writing, or the quality of the experiments. But I am unsure why BATTLE trains an intention policy with PbRL instead of just training the adversarial policy directly with PbRL. This would seem to be substantially simpler. \nI do not understand how SA-RL can perform so poorly relative to BATTLE unless it is just due to reward shaping. What is the reward function used for SA-RL? If an adversarial policy is directly trained to minimize the agent\u2019s reward, how can this do worse than BATTLE at making the target agents\u2019 reward be minimized? If the key difference truly is just reward shaping, then this paper would just seem to be one about how PbRL makes reward shaping automatic and implicit. And if so, then this paper would seem to have no novelty.\nRelatedly, I do not understand why this paper is about adversarial attacks. BATTLE could be used to make RL policies do anything \u2014 not just to targetedly adversarially attack them. But in reality, to make RL policies do things, we just finetune them directly. This relates to why I do not understand why the intention policy was used. Why not just finetune the target angent or adversary directly wit pbrl?\n\n", "questions": "\nWhich experiments were performed with real humans and which were with synthetic feedback?\nSee weaknesses\n\n"}, {"summary": "The paper introduces BATTLE, an adversarial attack framework targeting DRL agents. The main focus is on behavior-oriented adversarial attacks, which aim to induce specific behaviors in a DRL agent, as opposed to merely reducing the agent's rewards or driving it to a pre-determined state. BATTLE uses an intention policy aligned with human preferences and an adversary to guide the victim DRL agent to imitate the intention policy. A weighting function is also introduced to optimize the effectiveness of the attack. The authors claim that BATTLE outperforms existing methods in inducing specific behaviors and can also be used to improve the robustness of DRL agents when used in an adversarial training setup.\n", "strengths": "The availability of both code and demos enhances the paper's reproducibility. The paper enhanced the proposed methodology with convergence guarantees for BATTLE, adding rigor to the work.\n", "weaknesses": "\nThe presentation quality could benefit from further refinement for better clarity and impact.\nThe method's reliance on extensive human labeling hampers its real-world applicability, raising concerns about scalability.\nThe paper could be strengthened by including more motivating examples from real-world scenarios. The assumption that an adversary can modify observations is strong and raises questions about practicality. For instance, if an adversary has the ability to control the sensor, they might as well directly control the effector, making an agent-based adversarial approach seem more practical. Additionally, the rationale for using preference-based RL remains unclear.\nThe paper lacks some critical methodological details. For example, it doesn't specify how the victim policy approximator is trained or the volume of data required, leaving gaps in the understanding of the implementation.\nThe experimental setup and evaluations could be more convincing. The choice of baselines (PA-AD and SA-RL), which are un-targeted attacks, makes the comparison seem potentially unfair. Moreover, while various defense methods like adversarial training, robust learning, policy ensemble, and policy distillation exist, the authors have limited their experimentation to ATLA.\n\n", "questions": "\nCould the authors elaborate on potential real-world applications for the proposed method and discuss the challenges that might arise in such contexts?\nExpanding the experimental results to include additional comparison metrics would be valuable. Specifically, how does PALM fare against targeted attacks and various other defense methods?\nWhat is the extent of annotation required, especially in relation to the complexity of the task at hand?\nCould you provide details on the training process for the victim policy approximator, including the amount of data needed for effective training?\nHow generalizable is the weighting function across different types of tasks and domains?\n\n"}, {"summary": "This paper introduces BATTLE, a novel universal behavior-oriented adversarial attack method designed to induce specific behaviors in deep reinforcement learning (DRL) agents. Unlike prior approaches that focus on directing agents towards predetermined states or policies, BATTLE employs an intention policy aligned with human preferences for flexible behavior orientation, guiding the victim agent to imitate it. The paper demonstrates the effectiveness of BATTLE through empirical results across various manipulation tasks in Meta-world, showing its superiority over existing adversarial attack algorithms. Additionally, BATTLE enhances the robustness of DRL agents by training with the attacker, achieving a convergence guarantee under mild conditions, and proving effective even against the latest Decision Transformer agents. In summary, the paper makes contributions in the realm of behavior-oriented adversarial attacks on DRL agents, both in theory and practical applications.\n", "strengths": "\nThe paper introduces an interesting and novel concept, proposing a new type of attack based on preference-based RL.\n\nThe design of the inner-level optimization and outer-level optimization is well-founded, and the paper provides theoretical analysis of the algorithm.\n\nThe paper sets up different scenarios for evaluating various agents and conducts detailed ablation studies.\n\n\n", "weaknesses": "\nThe writing needs improvement, particularly in clarifying several terms and diagrams. For example, some terms like \"find a precise weighting function to balance the state distribution\" need better explanation. Clarification is also needed for the diagram in Figure 2.\n\nThe presentation of experimental results is somewhat confusing. The differences of scenarios in Figure 4 and 5 are not clear, and additional explanations are required for the target coordinates mentioned for Figure 4. Captions of Figures 7 (a) and (b) might need to be swapped, and sections (c) and (d) require clearer explanations.\n\nThe paper lacks a discussion of limitations, which should be addressed.\n\n\n", "questions": "\nIt's disappointing that the paper doesn't include RADIAL-RL[1] or WocaR-RL[2] as baselines when discussing robust training. Even if only ATLA is selected as a robust baseline, it would be valuable to mention other adversarial robust RL papers in the related work.\nIn the introduction, the paper illustrates the practical implications of targeted attacks on robotics, but the concern is raised that BATTLE is a white-box attack applying perturbations to states. In the context of robotics, its practical applicability is very limited. The paper could benefit from a more thorough clarification or discussion of this concern and its potential implications for practical applications. It fails to persuasively underscore the significance and relevance of this work within the field.\n\n[1]Robust deep reinforcement learning through adversarial loss\n[2]Efficient adversarial training without attacking: Worst-case-aware robust reinforcement learning\n"}]}, {"name": "Understanding Large Language Models Through the Lens of Dataset Generation", "key_word": ["Large Language Model", " dataset generation"], "sn": 9477, "reviews": [{"mark": [3, 4, 3], "rate": 8, "confidence": 4}, {"mark": [4, 3, 3], "rate": 6, "confidence": 4}, {"mark": [2, 3, 3], "rate": 5, "confidence": 4}, {"mark": [2, 3, 2], "rate": 3, "confidence": 5}], "abstract": "There has been increased interest in using Large Language Models (LLMs) for text dataset generation subject to a desired attribute, e.g., for use in downstream fine-tuning or training. These works generally focus on a single quality metric of the generated text, typically accuracy on a downstream task. However, this fails to consider whether the model even has the ability to faithfully model the data distribution of the desired real-world domain. In contrast, in this work, we additionally focus on important distributional metrics agnostic to the downstream task, such as data diversity and faithfulness. We show that even in simple domains, generated datasets reveal inherent trade-offs between these metrics across models and training regimes. Further, we find that our metrics not only describe the generated dataset, but also capture key aspects of the underlying model. This allows us to characterize the generated datasets, individual models and by comparison the properties of different model families and training paradigms. By focusing on sub-distributions well-represented in the training data of LLMs, we can, for example, show that popular instruction-tuning techniques strongly decrease the LLM\u2019s text generation abilities, with respect to distributional aspects like diversity.\n", "detail": [{"summary": "This work studies the attributes of dataset generation, which has recently been explored as a way to train task networks without needing a natural, human-generated dataset. Particularly, this work studies 4 domains/tasks that dataset generation can be applied to (e.g. SST-2), and studies the trade-offs between different attributes: faithfulness, diversity, conformity, complexity, and performance, all of which the authors measure automatically. The authors find significant differences between different model types, especially finding that instruction-tuned models differ from classical LMs. Neither paradigm seems to completely dominate.\n", "strengths": "\nOverall, this type of contribution is sorely needed in dataset generation, which is still not a well-understood field\nThe attributes to study are diverse and relevant\nVery interesting and informative conclusions drawn about the tradeoffs, e.g. the loss of diversity in generated datasets when using instruction-tuned models\npaper is well presented and quite clear\n\n", "weaknesses": "\nI have concerns wrt the measurement of some of the aspects:\nfaithfulness is measured as the accuracy on the synthetic set with a model trained on the reference (human) set. While being unfaithful is one reason this value may be low, it is not the only one. It is easy to imagine a faithful dataset on which this classifier will perform poorly, due to issues like style shift or poor generalization of the classifier. To be more concise: staking faithfulness on the accuracy of a classifier ignores the fact that this may be an issue of the classifier rather than the dataset that is being evaluated. \nsimilar issue with complexity, which is measured as inverse accuracy on a held out chunk of the synthetic set. While I agree that lower complexity will indeed raise this accuracy, high complexity is not the only reason this accuracy may decrease.\n\n\nOverall, I would suggest renaming these metrics. They likely correlate with the values they are described as, but it is overly presumptuous to label them this way as there are many other factors. More direct names (e.g. complexity -> self-accuracy or something like this) might be more accurate, leaving discussion of factors affecting these values (like complexity) to the discussion\nTradeoffs (Figure 2) are only shown in terms of temperature, which may be a confounding factor. It would be good to show other curves, e.g. for values of top-p, because it is not clear if these tradeoffs may have to do specifically with the specific warping effect that temperature has on sampling distributions. Alternatively, being more precise in the paper text, that these are tradeoffs over temperature as the variable, rather than general tradeoffs.\n\n", "questions": "Have you tried variables besides temperature to test the tradeoffs?\n"}, {"summary": "This paper studies the text generation capabilities of various large language models, proprietary and open, instruction-tuned and vanilla, by evaluating synthetic datasets generated from them. The datasets are evaluated in terms of\n\ndiversity in vocabulary\ncomplexity, or difficulty in modeling them given by the performance of a model trained and evaluated in-distribution.\n\nBy comparing the generated datasets to existing (reference) datasets in similar tasks and domains, they are also evaluated in terms of\n3) faithfulness, given by the performance of models trained on the reference datasets and evaluated on the generated ones\n4) conformity, given by a measure of distributional similarity between the reference and generated datasets\n5) performance, given by the performance of models trained on the generated datasets and evaluated on the reference datasets\nBased on this evaluation framework, the paper discusses the tradeoffs between these aspects of generation quality, how they change across model families, and how instruction tuning affects these tradeoffs.\n", "strengths": "The evaluation framework is sensible and analyzing the capabilities of language models in terms of the tradeoffs between various aspects of generation quality is quite informative. The results of studying the effect of model size, the impact of instruction tuning, and that of the level of instruction tuning can potentially inform how to finetune future versions of language models.\n", "weaknesses": "This study has some missing details, several limitations, and potential confounders not accounted for in the experiments.\nMissing details\nMD1:The evaluation is done over four classification datasets, but the actual details of the tasks are missing in Section 4. Particularly for AGNews and ELI5, it is unclear what is being classified After reading the Appendix, the AGNews task seems to be some news genre classification, and the ELI5 task seems to be subreddit classification (maybe it should just be called \"subreddit classification\"?) This issue can easily be fixed by including explicit details in Section 4.\nMD2: The motivation behind the chosen evaluation metrics is somewhat unclear. Particularly, faithfulness, conformity, and performance seem to be measuring the difference between the generated and reference data distributions. Why do we need these three variants? Relatedly, one would expect these metrics to correlate highly with each other. Analyzing this further would be helpful.\nLimitations and potential confounders\nL1: It is unclear how noise in the datasets (due to inaccurate labels) affects the trends seen in tradeoffs. For example, is the increase in diversity beyond the the conformity threshold in Fig 2 simply be due to noise? Having humans classify (subsets of) the generated datasets, and introducing the accuracy of the synthetic datasets as an additional metric could make this clearer.\nL2: The biases in the reference datasets could also be affecting conformity, faithfulness and performance. It might help to include multiple reference datasets per domain-task combination to evaluate whether the trends hold across them.\nL3: It is possible that the models used for generating datasets have seen the reference datasets either during pretraining or instruction-tuning. This would inflate the quality measures according to conformity, faithfulness, and performance. This issue cannot be dealt with directly, but it would help to check the zero-shot performance of the large language models on the reference datasets, and take it int account while inferring the tradeoffs.\n", "questions": "\nIt would be helpful to put the reported diversity and complexity values in context. What are these values for the reference datsets?\nCan you elaborate on the motivation behind the three metrics comparing generated and reference datasets (see MD2)?\n\n"}, {"summary": "This work studies the quality of synthetic data generated by LLMs. The major contribution of this work is proposing a framework to evaluate LLM's ability to generate synthetic data for specific tasks, and compare behavior across different LLMs. The evaluation framework consists of five different axes: performance, complexity, conformity, diversity and faithfulness. These properties are either evaluated using accuracy-based metrics, or modified version of existing tools (e.g., distict-n, mauve, etc.). Using this framework, this work compares LLMs with different size, from different model families and with or without instruction tuning. The empirical study reveals interesting tradeoffs among the five axes, and also report general performance trends on overall performance.\n", "strengths": "\nGenerating synthetic datasets is a very popular application of LLMs. This work provides a useful framework on evaluating this ability of LLMs.\nThe empirical study shows interesting tradeoff from the models, and the reported performance trends can be useful for related applications.\n\n", "weaknesses": "\nI like the general idea of the proposed evaluation framework, but my biggest concern about this framework is the heavy use of DistilBERT accuracies in the evaluation framework. For the faithfulness metric, the framework is evaluating the performance of DistilBERT on the generated dataset. This confounds faithfulness with the difficulty (or complexity) of the dataset. This makes some of the finding questionable. For example, is there really a tradeoff between faithfulness and diversity/complexity, or is this correlation comes from the correlation between difficulty and diversity/complexity? I wonder if the authors can provide gold evaluation results for the DistilBERT models. \nThis study only focuses on synthetic data generation for relatively simple classification tasks. It would be great if this work can include evaluation on some more complex tasks.\nWhile this paper proposes four other properties addition to the performance. There is not much discussion on the relationship between these properties and the final performance. So while this study show many interesting findings, it is unclear what users should do besides checking the performance rankings.\n\n", "questions": "\nFor the value k in the diversity metric, are you keeping the example size the same, or the token size same?\nHow do design or select prompts for the study conducted in your paper? Have you checked the sensitivity of the findings with respect to different prompts?\n\n"}, {"summary": "This paper examines the generation of text datasets using Large Language Models (LLMs) with a focus on distributional metrics like data diversity and faithfulness. It reveals trade-offs between these metrics across different LLMs and training methods, highlighting the impact of popular instruction-tuning techniques on LLM text generation abilities.\n", "strengths": "\nThe studied task on using LLMs for data generation is interesting and can be useful for the research community.\n\nThe authors conduct experiments on various datasets and LLMs (including both open-sourced and close-sourced models).\n\nThe paper is overall easy to read.\n\n\n", "weaknesses": "\nThe authors only consider the most simple prompts for the target tasks. However, there are several works that aim to improve the quality of prompts to yield higher-quality datasets, some examples include:\n\n\nChung et al. \"Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions.\" ACL 2023.\n\nYu et al. \"Large language model as attributed training data generator: A tale of diversity and bias.\" NeurIPS D&B Track, 2023.\n\n\nIt is also important to note that some dimensions (e.g. diversity) have already been studied in this work. As a result, some of the conclusions in this paper are already known and there are not many new insights about using LLMs for data generation.\n\nUnsupported Claims. The paper raises a claim that \"reinforcement learning with human feedback (RLHF) in ChatGPT leads to a significant degradation in synthetic dataset generation capabilities.\" However, the paper lacks a clear explanation of how the authors attribute this performance drop specifically to RLHF. A more detailed description of the experimental setup and results related to this assertion would enhance the paper's clarity and credibility.\n\nIn the main paper, the author only considers the average performance over different patterns, which can be less informative as different datasets show diverse patterns (according to Figure 5).\n\nFor the metrics, it is somehow not clear why using unique number of tokens as the metrics of Diversity.\n\n\n", "questions": "\nCould you elaborate on why this paper primarily relies on simple prompts for target tasks, especially when recent research has emphasized advanced prompt engineering techniques for improving dataset quality? How might incorporating more sophisticated prompts affect the study's outcomes?\n\nGiven that some dimensions, like diversity, have already been studied in this work, what new insights or contributions does this paper bring to the field of using LLMs for data generation? \n\nIn the paper, you assert that \"reinforcement learning with human feedback (RLHF) in ChatGPT leads to a significant degradation in synthetic dataset generation capabilities.\" Could you provide a more detailed explanation of the experimental design and results that support this claim?\n\nWhat conclusions can be made after your study? What are the recommendations for practitioners to use LLMs for training data generation? Currently, it is not very clear after reading this paper, so I feel readers will not benefit much from this paper.\n\n\n"}]}, {"name": "Temporal Parallelization for GPU Acceleration of Spiking Neural Networks", "key_word": ["Spiking neural networks", " High-performance computing", " GPU acceleration"], "sn": 9476, "reviews": [{"mark": [1, 2, 2], "rate": 3, "confidence": 3}, {"mark": [2, 1, 2], "rate": 3, "confidence": 4}, {"mark": [3, 2, 2], "rate": 3, "confidence": 4}, {"mark": [3, 1, 2], "rate": 3, "confidence": 5}], "abstract": "Inspired by neurobiological structures, Spiking Neural Networks (SNNs) are heralded as a significant advancement in deep learning, given their potential for superior computational efficiency. However, this potential often remains untapped on contemporary hardware platforms. Specifically, when deployed on standard GPUs, SNNs tend to require extended computation times, placing them at a disadvantage compared to traditional Artificial Neural Networks (ANNs). Such inefficiencies have somehow diminished enthusiasm for SNN research and presented the tangible challenge to achieving scalability. To address such a challenge, this study introduces a temporal parallelization method specifically tailored for accelerating the propagation dynamics of SNNs on GPUs. Furthermore, we furnish two distinct implementations\\footnote{The source code will be made publicly available.} based on the CUDA and JAX frameworks respectively, ensuring adaptability across both single and multi-GPU setups. When benchmarked against several established SNN implementations, the empirical analysis confirmed the efficacy of our proposed method. Notably, with the Leaky Integrate-and-Fire model as a test case, the CUDA-based implementation achieved 5\u00d7 to 40\u00d7 acceleration on the A100 GPU.\n", "detail": [{"summary": "The authors describe a method and code for accelerating spiking neural\nnetworks (SNNs) on GPUs. They first claim to parallelize the temporal\nmembrane integration of a layer in an SNN and secondly divide the\ncompute onto multiple GPUs. They provide a template how to implement\nit in common ML frameworks such as JAX. Finally, they show that their\nimplementation outperforms other toolboxes.\n", "strengths": "The implementation seems to outperform current toolboxes in terms of runtime.  The authors show that this layer-first\napproach gives a considerable speedup on GPUs due to reduced memory movement.\n", "weaknesses": "While better implementation of simulating SNNs using GPUs has its merits,\nthe task is merely a software\nengineering task. The paper does not add any value in terms of novel\ninsights. This is in particular true since the \"temporal\nparallization\" argumentation is indeed a misnomer as the temporal dimension is not\ncomputed in parallel, but instead time of one layer is simply handled\nwithin one GPU kernel (but still computed sequentially if I understand it correctly).\nIf one wanted to design a custom CUDA kernel and would assume that\nonly feed-forward layers are allowed, this would be just the standard\napproach to do, I don't see any innovative aspects here. In\nparticular, equation 3 is just a re-writing (inserting) of x(t\u22121,n),\nthere is no \"transformation\" I can see. Note that vi(t,n) still\nis a function of previous times, vi(t\u22121,n). All what is done is to\ncompute all time steps per layer first before sending the full output\nspike train to the next layer. This will obviously not work for\nrecurrent SNNs. \nAlso, the authors do not even provide their own optimized CUDA kernel\n(which would have more merit), but instead rely on generic toolboxes\nlike JAX. The code listings do not provide any details of the\nimplementation and are more like a tutorial how to use it.  \nOverall, while the implementation might be useful as it improves the\nruntime of SNNs compared to the (apparently very non-optimized)\nstandard SNN packages, the paper does not provide any new scientific\ninsights. It is also not discussed that the approach works only for\nfeed-forward SNNs. Moreover, the presentation of \"temporal\nparallelization\" is not correct (as it just points to a fused\nsequential CUDA-kernel). Finally, the layer-first approach (fusing\nkernels to reducing memory operations) and dividing the compute for\nmultiple GPUs are rather standard practices in GPU programming in\ngeneral and not novel enough for a research oriented conference\ncontribution in my opinion.\n", "questions": "\nIn Eq 3: xi(t,n) should be xi(t,n\u22121)\n\n"}, {"summary": "This paper trys to use a temporal parallelization method to accelerate the propagation dynamics of SNNs on GPUs. The feature it claims is a cross-timestamp acceleration of LIF model. With the Leaky Integrate- and-Fire model as a test case, the CUDA-based implementation achieved 5\u00d7 to 40\u00d7 acceleration on the A100 GPU.\n", "strengths": "The author proposed temporal parallelisation method tailored for universal SNN units on single and multiple GPUs. It supports both CUDA and JAX frameworks.\n", "weaknesses": "\nThe motivation behind this paper lacks clarity. Spiking Neural Networks (SNNs) are not typically intended for deployment on GPUs, meaning that a GPU is not the most suitable platform for SNN deployment. Without a demonstration of the clear benefits of utilizing GPUs for SNNs deployment as opposed to other platforms, the paper's underlying motivation remains unconvincing.\n\nHow does this paper leverage GPU to implement true spiking mechanism? It is not clear or discussed. Is it only considering simulating the mechanism of the Leaky-Integrate-and-Fire model behavior? Plus, there\u2019s no true spiking signals in GPU, addressing the temporal information is not really Spiking implementation. This paper doesn\u2019t clarify the basic concept. \n(In some sense, parallelizing temporal information is possible, but there\u2019s conversion between spiking temporal information and the muti-bit digital temporal information for GPU? Then what\u2019s the conversion cost?)\n\nAlthough this paper is based on the computational model of LIF, but it does not clearly describe how training and inference is done, respectively. Training an SNN is hard, and it is not discussed at all in this paper, so it\u2019s only about inference, or even, the simulation of inference?\n\nLast but not least, most importantly, this paper does not provide any AI-model based results, such as accuracy, performance, respective speed-up, etc, let alone thorough analysis based on the comparison of results. The only result is a table based on a single-layer toy model? For multi-GPU, where is Fig. 4, seems this paper is incomplete?\n\n\n", "questions": "\nHow SNN neuron spiking behaviour described in Eq.1 and Eq. 2 reflected in GPU?\n\n"}, {"summary": "This paper presents an SNN-based acceleration strategy with parallelized temporal computation that supports both single and multiple GPUs.\n", "strengths": "Largely improved SNN inference speed compared to the previous implementation. \nCompatibility with both single and multi-GPU processing.\n", "weaknesses": "W1: Figure 4 is missing. \nW2: The biggest bottleneck of this work is that the accuracy benchmarking is completely missing in the paper. I understand the inference speed-up is very important in SNN, but I cannot see the reason why the paper chose not to report the accuracy. It is important to verify the proposed implementation with different SNN model architectures. E.g.. ResNet vs. VGG. \nW3: It seems like the implementation can only accelerate the inference rather than training, which I think is not powerful enough.\n", "questions": "Please refer to the Weakness.\n"}, {"summary": "This paper proposes a temporal parallelization method for SNNs that can accelerate SNNs on both single or multi GPUs with up to 40x acceleration.\n", "strengths": "The training of deep SNNs requires much more time and memory consumption. Thus, it is meaningful to explore the acceleration of simulating SNNs on GPUs.\n", "weaknesses": "The details of the proposed method are not described clearly in this paper. To make matters worse, the Supplementary Material is the same as the main paper.\n", "questions": "In Figure 3, how the propagation of the spiking neuron layer is paralleled? I assume that V[t] is still computed in serial. For an input sequence with length T, the time complexity is still O(T).\nIn section 3.2, the authors claim that the SNN accelerated by pipeline in multiple GPUs may have a faster speed than using a single GPU. However, I am afraid that the communication time between GPUs will be the bottleneck. According to my experience, the communication time is much longer than any other time. Thus, the pipeline method is seldom used in training, and the Distributed Data Parallel is the mainstream.\nIn Table 1, the time of SpikingJelly with or without CuPy does not have much difference, which is against my experience. \nWhere is Figure 4?\n"}]}, {"name": "FSN: Feature Shift Network for Load-Domain Domain Generalization", "key_word": ["Fault diagnosis", " Deep learning", " CNN", " Domain Generalization", " Load-domain Domain Generalization"], "sn": 9472, "reviews": [{"mark": [2, 1, 1], "rate": 3, "confidence": 3}, {"mark": [2, 2, 2], "rate": 5, "confidence": 3}, {"mark": [2, 2, 2], "rate": 3, "confidence": 4}, {"mark": [2, 1, 2], "rate": 3, "confidence": 4}, {"mark": [2, 2, 2], "rate": 3, "confidence": 4}, {"mark": [2, 2, 2], "rate": 3, "confidence": 3}, {"mark": [1, 2, 1], "rate": 3, "confidence": 5}, {"mark": [2, 4, 2], "rate": 6, "confidence": 4}], "abstract": "Conventional deep learning methods for fault detection often assume that the training and the testing sets share the same fault pattern spaces and domain spaces. However, some fault patterns are rare, and many real-world faults have not appeared in the training set. As a result, it\u2019s hard for the trained model to achieve desirable performance on the testing set. \nIn this paper, we introduce a novel domain generalization, Load-Domain (LD) domain generalization, which is based on the analysis of the CWRU bearing dataset and its domain division method. For this scenario, we propose a feature shift model called FSN (Feature Shift Network). In the bearing dataset, domains are divided based on different operating conditions which have specific loads, so it\u2019s equivalent to load-based domain division. Moreover, the domain label corresponds to the actual load magnitude, making it unique as it contains physical information, which can boost detection accuracy on unknown domain beyond the training set. According to the knowledge above , FSN is trained for feature shift on adjacent source domains, and finally shifts target domain features into adjacent source domain feature space to achieve the purpose of domain generalization.\nExtensive experiments on CWRU demonstrate that FSN is better than the existed models in the LD domain generalization case. Furthermore, we have another test on MNIST, which also shows FSN can achieve the best performance.\n", "detail": [{"summary": "This paper targets a domain generalization problem for fault diagnosis of the bearing dataset and proposes a new model, called the feature shift network (FSN), to adjust the features between source and target domains. The numerical experiment using CWRU and MNIST datasets is conducted to evaluate the effectiveness of the proposed FSN.\n", "strengths": "\nThe motivation to exploit the additional information of the problem by assuming the specific task of fault diagnosis in bearing datasets is good.\nThe domain generalization problem treated in this paper is important.\n\n", "weaknesses": "\nThe paper is not well written and has a lot of unclear points. For example, the detailed setting, such as loss function and calculation of each model component, is omitted. It is hard to understand the technical novelty and advantages of the proposed method.\nI cannot find the formal definition of the load-domain (LD) generalization problem treated in this paper.\nIn Table 1, the performance gain of the proposed FSN variants is marginal.\nThe motivation of the evaluation using the MNIST dataset is unclear.\n\n", "questions": "\nWhat does the \"Relation\" mean in Table 1?\nHow does the proposed FSN exploit the physical meaning of domain labels for model training?\nIs the target domain data accessible in the load-domain generalization? In general, the target domain data is not accessible in domain generalization.\n\n"}, {"summary": "Traditional deep learning methods for fault detection usually assume that the training set and the test set share the same fault mode space and domain space. Based on the analysis of CWRU bearing data set and its domain division method, this manuscript proposes a Feature Shift model called FSN (Feature Shift Network) to improve the detection accuracy of unknown domain, which can divide domains according to different operating conditions with specific loads, and take advantage of the physical significance of domain labels.\n", "strengths": "1.This manuscript proposes the idea of \"exploitability\" of domain-related information, and it may be a point worth exploring further.\n2.This manuscript is written in a standard and clear hierarchy,  and the structure is easy to follow.\n", "weaknesses": "1.In this manuscript, a parameter with physical significance is used as a domain label, and then the related features of the domain are used to assist classification. But now that labels have physical meaning, what happens if they are input directly to the network with other data? We didn't see the related comparison experiments. So it is not  convincing.\n2.Few comparison experiments are conducted.\n3.The detailed design of the model, including the loss function, is not explained in sufficient detail.\n", "questions": "1.Why label a domain with a piece of information that can be numerically and physically meaningful, and how is that different from feeding it directly into a neural network?\n"}, {"summary": "The paper aims to address the multi-source domain generalization problem by proposing a feature shift network (FSN). Distinct from traditional domain generalization, the problem of interest has additional information of inter-domain linear relations in the form of domain labels. By taking advantage of such information explicitly in the proposed model, superior performance can be achieved.\n", "strengths": "--The paper attempts to addresses the domain generalisation problem derived from a real-world application. The problem itself is somewhat novel and has not been extensively studied and hence solving such a problem is of great significance.\n", "weaknesses": "--The introduction section lacks essential information of problem definition, description of methods and brief experimental results. This makes it less readable to the readers. For example, it is not clear what the \"Load-Domain\" means and how the feature shift model handles the problem.\n--The section of related work is not well organized. More focus should have been put on the most closely related works (i.e. domain generalization in fault detection problems) rather than a broad review of fault detection methods. In addition, the relations between existing works and this work should also be discussed.\n--In table 1, Multi FSN does not perform the best as Multi DANN has a result of 83.3.\n--The authors fail to compare with SOTA domain generalization methods.\n--There exist language issues/typos/notation inconsistency in the manuscript. E.g., \"P(1), P(2), ..., P^{(K)}\"; \"edge distribution\" should be \"marginal distribution\"; \"1 data of source domain a+1 and 9 data...\"; \"This thesis DANN domain adaptive network...\"; \"the ERM said empirical ...\";\n", "questions": "\nWhat is the loss function of the network?\nWhat is the input (images?) and the output of the model?\n\n"}, {"summary": "This paper introduces a special domain generalization scenario termed Load-Domain domain generalization and proposes a new model, the Feature Shift Network (FSN), tailored for this scenario. The authors conduct experiments on the CWRU bearing dataset and the MNIST dataset, comparing FSN with classical fault diagnosis methods and other domain generalization methods.\n", "strengths": "The experimental results show good performance of FSN in certain scenarios and hence its potential for practical applications in fault diagnosis and domain generalization.\n", "weaknesses": "\nThe writing of the paper should be substantially improved. It reads like bad machine translation and has a lot of grammatical and terminological errors.\nThe theoretical foundation of the FSN model could be explained in more detail. The model architecture shown in Fig. 3 requires further clarification and motivation.\nExperiments are only conducted on two datasets, with limited baseline methods for comparison. It is unclear how the proposed method performs on other datasets that also have \"linear\" domain labels.\nThe accuracy values do not have confidence intervals.\n\n", "questions": "See above.\n"}, {"summary": "The paper introduces a novel approach called Load-Domain (LD) domain generalization for fault detection in situations where the training and testing sets have different fault pattern spaces and domain spaces. The authors propose a feature shift model called FSN (Feature Shift Network) specifically designed for LD domain generalization. The model is trained on adjacent source domains to learn feature shifts and then applies these shifts to target domain features, enabling generalization beyond the training set. The approach is validated through extensive experiments on the CWRU bearing dataset and the Rotation MNIST dataset, demonstrating superior performance compared to existing models in LD domain generalization scenarios.\n", "strengths": "\nThe topic of Domain Generalization is highly relevant and of significant interest to the research community. Furthermore, the paper addresses the important aspect of leveraging domain information during the training process, which has gained increased attention in recent times.\nThe paper is well-written and easily comprehensible, effectively conveying its ideas and findings to the readers.\n\n", "weaknesses": "\nThe LD domain generalization problem setting addressed in the paper is acknowledged as a highly specialized case, which limits its broader contribution to the field.\nThe experimental results presented in the paper lack persuasiveness. Firstly, the dataset used is small and may not accurately represent real-world scenarios. Additionally, the chosen baselines are outdated, primarily predating 2018, despite the emergence of numerous domain generalization methods since then. It is recommended to refer to recent surveys for a comprehensive overview of the latest approaches, e.g., [1].\nWhile the main idea of the paper is generally understandable, there are instances where sentences may create misunderstandings, and insufficient definitions of the problem and task settings are found throughout the paper. Notably, the captions for Figure 1 may inaccurately describe Figure 1(b), potentially causing confusion among readers.\n\n[1] Domain Generalization: A Survey\n", "questions": "Please review the weakness section, and kindly correct any misunderstandings that may exist in my assessment.\n"}, {"summary": "In this paper, the authors introduce a new Load-Domain (LD) domain generalization setting, where the domain label corresponds to actual load magnitude. To serve the scenario, the authors propose a feature shift model (FSN) to learn feature mapping between adjacent domains according to the physical meaning in domain labels. Experiments are carried out on CWRU bearing dataset and rotated MINST datasets to showcase the performance.\n", "strengths": "\nThe idea of employing the physical meaning of domain labels to achieve generalization on consecutive domains makes sense to me. \nThe new setting of LD domain generalization also seems suitable for the benchmark of CWRU dataset, where the physical meaning of domain labels can be clearly defined.\n\n", "weaknesses": "\nThe paper\u2019s writing and formatting are not good enough, making it sometimes hard to read and understand. There exist many formatting errors (e.g., the use of spaces in Sections 1 and 2, the math symbol consistency in Section 4.1: P(1) and P^{(K)}). The caption of Figure 1 is duplicated. The abbreviation in Paragraph 3, Section 2 might be RAP. In the second line of paragraph in section \u201cDistribution Law Conjecture\u201d, F_p should be the the mapping relation between features, not labels.\nBesides the ones from 1., the Experiment part (e.g., paragraph 4 in \u201cClassical Model Contrast\u201d part) is hard to understand. Bad formatting and discontinuous sentences make it unreadable to me.\nKey references are absent. In the first paragraph of Section 4.2, methods aligning the three types of distributions should be cited to justify the categorization. The compared methods in \u201cComparison with Classical Models\u201d should also be referred to, and the Li et al. (2018) reference in this paragraph is wrongly cited, this paper should be \u201cDeep Domain Generalization via Conditional Invariant Adversarial Networks\u201d.\nAs to the experimental results, the metric of Table 1 is not clearly stated. Also, the experimental details like backbone choice, learning rate, optimization schedules are not provided.\nThe setting is still too limited. The current method and experiments only focus on generalizing from highly relevant and sequential source domains to a target domain close to the last seen source domain. The performance of the proposed method should be further evaluated on broader settings where the relationship of source and target domains is not fixed. Also, the setting seems much relevant to that of Continuous Domain Generalization, which should be discussed.\n\n", "questions": "Apart from those in weakness, the authors are encouraged to experiment on larger domain generalization benchmarks. Moreover, the compared methods are not recent enough, therefore more experiments should be added to provide a more comprehensive comparison.\n"}, {"summary": "The paper proposes a feature shift network (FSN) for a new domain generalization task called load-domain (LD) generalization based on analyzing the CWRU-bearing dataset. The key idea is to leverage domain labels to shift target features into adjacent source domains. The method is evaluated on the CWRU and rotated MNIST datasets by comparing them to existing domain generalization techniques.\n", "strengths": "The application of domain generalization on the problem of fault detection is novel.\n", "weaknesses": "The paper lacks novelty as feature shift is explored in prior work. The theoretical analysis relies on unproven conjectures and lacks rigor. More comprehensive empirical evaluation on realistic datasets and comparisons to recent benchmarks are needed to demonstrate effectiveness.\n", "questions": "The novelty of the proposed feature shift network (FSN) is questionable given prior work on feature shift for domain generalization. The current paper does not sufficiently differentiate FSN from these existing methods. \nThe paper also lacks theoretical analysis regarding the generalization abilities of FSN.\n Empirically, evaluation is limited to two small datasets, which cannot sufficiently demonstrate effectiveness and robustness for the load-domain generalization task. More extensive testing on diverse realistic datasets are needed.\n"}, {"summary": "n/a\n", "strengths": "The paper addresses a challenge in deep learning-based fault detection, where real-world faults may not always appear in the training set. This limitation makes it difficult for conventional models to generalize well to unseen fault patterns. To address this, the authors introduce a domain generalization method, Load-Domain (LD) domain generalization, specifically designed based on the CWRU bearing dataset. In this method, the domains are divided based on different operating conditions that have specific loads, which correspond to the actual load magnitude. This inclusion of physical information helps enhance the model's accuracy for unknown domains. The authors propose the Feature Shift Network (FSN), which is trained to shift features between adjacent source domains and the target domain for better generalization. The effectiveness of FSN is shown through experiments on both the CWRU bearing dataset and the MNIST dataset, where FSN outperforms existing methods.\n", "weaknesses": "\nThe authors tackle a crucial issue in fault detection where conventional models may not perform well on unseen fault patterns.\nThis novel domain generalization approach, based on real physical properties (load magnitude), can potentially be more representative and robust than abstract or purely data-driven domain divisions.\nThe model's applicability on both the CWRU bearing dataset and the MNIST dataset suggests it is versatile and not limited to one type of data.\nThe authors compare their model with classical fault diagnosis methods, showing its superiority in specific scenarios.\nThe paper appears to have a well-structured format, with sections dedicated to reviewing the current state of research, introducing their novel domain generalization method, and discussing experimental results.\n\n", "questions": "\nThe assumption that domain label corresponds to actual load magnitude might not hold for all real-world scenarios. It may be beneficial to test scenarios where this is not the case.\nIntroducing domain-specific information like load magnitude could risk overfitting to specific domain characteristics. The generalization capability of the model in truly unseen domains is a concern.\n\n"}]}, {"name": "Ask Again, Then Fail: Large Language Models\u2019 Vacillations in Judgement", "key_word": ["Large Language Models", " Uncertainty", " Evaluation", " In-Context Learning", " Alignment", " Multi-round dialogue", " Robustness"], "sn": 9468, "reviews": [{"mark": [3, 4, 3], "rate": 6, "confidence": 4}, {"mark": [3, 3, 2], "rate": 6, "confidence": 4}], "abstract": "With the emergence of generative conversational large language models (LLMs) like ChatGPT, serving as virtual assistants in various fields, the stability and reliability of their responses have become crucial. However, during usage, it has been observed that these models tend to waver in their judgements when confronted with follow-up questions from users expressing skepticism or disagreement. In this work, we draw inspiration from questioning strategies in education and propose a \\textsc{Follow-up Questioning Mechanism} along with two evaluation metrics to assess the judgement consistency of LLMs before and after exposure to disturbances. We evaluate the judgement consistency of ChatGPT, PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning benchmarks. Empirical results show that even when the initial answers are correct, judgement consistency sharply decreases when LLMs face disturbances such as questioning, negation, or misleading. Additionally, we study these models' judgement consistency under various settings (sampling temperature and prompts) to validate this issue further, observing the impact of prompt tone and conducting an in-depth error analysis for deeper behavioral insights. Furthermore, we also explore several prompting methods to mitigate this issue and demonstrate their effectiveness.\n", "detail": [{"summary": "The research addresses a critical concern in the use of generative conversational large language models (LLMs) like ChatGPT, focusing on their judgement consistency when faced with follow-up questions expressing skepticism or disagreement. Drawing inspiration from educational questioning strategies, the study proposes a FOLLOW-UP QUESTIONING MECHANISM and introduces evaluation metrics to assess LLMs' consistency before and after disturbances. The study evaluates ChatGPT, PaLM2-Bison, and Vicuna-13B across reasoning benchmarks, revealing a decline in judgement consistency even when initial answers are correct. The research explores the impact of disturbances, sampling temperature, and prompts, conducting an in-depth error analysis. Moreover, it introduces and evaluates various prompting methods to mitigate this issue, demonstrating their effectiveness.\n", "strengths": "\nComprehensive Evaluation: The research evaluates multiple LLMs (ChatGPT, PaLM2-Bison, and Vicuna-13B) across eight reasoning benchmarks, ensuring a comprehensive analysis of their performance under different conditions.\nThorough Analysis: The study conducts a detailed analysis of disturbances, sampling temperature, prompts, and prompt tone, offering valuable insights into the factors affecting judgement consistency.\nEffective Solutions: The research explores various prompting methods and demonstrates their effectiveness in mitigating the issue, suggesting practical solutions for enhancing LLMs' reliability.\n\n", "weaknesses": "\nLimited Scope of LLMs: The study evaluates a specific set of LLMs (ChatGPT, PaLM2-Bison, and Vicuna-13B), potentially limiting the generalizability of the findings to other models in the rapidly evolving landscape of conversational AI.\nScope of Disturbances: While disturbances like questioning, negation, and misleading are considered, the study might benefit from exploring a wider range of disturbances to provide a more comprehensive understanding of LLMs' judgement consistency challenges.\nLack of Real-World Application: The research focuses on theoretical evaluation and proposed mechanisms; it would strengthen its impact by discussing practical implications and real-world applications of the proposed solutions.\n\n", "questions": "\nConsidering the rapid advancements in AI technologies, how might the results differ when applied to newer or upcoming LLMs? Is there room for future research to address this limitation?\nCan you provide insights into how the proposed mechanisms and solutions could be practically applied in real-world scenarios, especially in fields where LLMs are extensively used, such as customer support or healthcare?\n\n"}, {"summary": "This paper investigates the problem of answer consistency in large language models (LLMs), especially when prompted with questioning, disagreement, or misleading input. The authors designed a follow-up questioning mechanism, inspired by questioning strategies in education, to experiment with LLMs. After an initial correct response, the authors attempted prompts of questioning, disagreement, or misleading input in two different ways, one of the three and all of the three in a sequential manner. The authors conducted experiments on ChatGPT, PaLM2-Bison and Vicuna-13B using four kinds of objective reasoning questions: arithmetic reasoning, commonsense reasoning, symbolic reasoning, and knowledge reasoning. They found that a significant decrease in judgement consistency occurred after the models were prompted with questioning, disagreement, or misleading input, both in isolation and in sequence. The authors also tried some mitigation methods, but there is still room for improvement\n", "strengths": "\nThe paper is clearly written and easy to follow. \nIt addresses the critical issue of trustworthiness in large language models. \nThe well-designed experiments and mitigation approaches clearly demonstrate the problem of LLMs and draw attention to its importance.\n\n", "weaknesses": "\nI do not see a major problem with the paper. While some people may prefer a paper that proposes a new model, this investigative paper could still be a valuable contribution to the field.\n\n", "questions": "\nI didn't understand the second sentence in footnote 1.\n\nModification Rate (M. Rate) was not clear to me.\n\n\n"}]}, {"name": "Do Current Large Language Models Master Adequate Clinical Knowledge?", "key_word": ["Large Language Model", " Medical Large Language Model", " Clinical Knowledge", " Knowledge Graph"], "sn": 9466, "reviews": [{"mark": [2, 3, 2], "rate": 5, "confidence": 4}, {"mark": [3, 2, 2], "rate": 5, "confidence": 4}, {"mark": [2, 1, 2], "rate": 3, "confidence": 2}], "abstract": "Large Language Models (LLMs) show promising potential in solving clinical problems. Current LLMs, including so-called medical LLMs, are reported to achieve excellent performance on certain medical evaluation benchmarks, such as medical question answering, medical exams, etc. However, such evaluations cannot assess whether LLMs have mastered sufficient, compressive, and necessary medical knowledge for solving real clinical problems, such as clinical diagnostic assistance. In this paper, we propose a framework to assess the mastery of LLMs in clinical knowledge. Firstly, we construct a large medical disease-based knowledge base, MedDisK, covering 10,632 common diseases across 18 clinical knowledge aspects, which are crucial for diagnosing and treating diseases. Built on that, we propose a MedDisK-based evaluation method MedDisKEval: We prompt LLMs to retrieve information related to these clinical knowledge aspects. Then, we evaluate an LLM's mastery of medical knowledge by measuring the similarity between the LLM-generated information and the content within our knowledge base. Our experimental findings reveal that over 50% of the clinical information generated by our evaluated LLMs is significantly inconsistent with the corresponding knowledge stored in our knowledge base. We further perform a significance analysis to compare the performance of medical LLMs with their backbone models, discovering that 5 out of 6 medical LLMs perform less effectively than their backbone models in over half of the clinical knowledge aspects. These observations demonstrate that existing LLMs have not mastered adequate knowledge for clinical practice. Our findings offer novel and constructive insights for the advancement of medical LLMs.\n", "detail": [{"summary": "The paper introduces a large-scale medical disease-based knowledge base MedDisK, covering 10,632 common diseases and 18 clinical knowledge to evaluate LLMs. The purpose of the dataset is to  (a) include common diseases (b) involve disease base knowledge and (c) ensure that the sourcing of the dataset is such that it remains publicly inaccessible to prevent leaks during testing.\t\nFirst filter common diseases (determined by clinical experts based on ICD10 databases and frequency in EHR)  resulting in 10,632 common diseases. Then  employ clinical experts to define 18 disease-based clinical knowledge aspects that are crucial to medical decision-making (diagnoses, examinations, treatments) for each of the diseases. They use this database to probe LLMs and evaluate the mastery of clinical knowledge. They show that their scoring measures are in high agreement with clinical experts' subjective evaluation.  \nUsing the evaluation metrics they show that existing LLMs have not mastered adequate knowledge for clinical practice (showing that over 50%  of the generated information is not consistent with their KB) and are not ready to be foundation models for clinical domain.\n", "strengths": "The paper does a good job in communicating the ideas. I agree with the author's motivation that for the LLMs to be accepted as foundation models they need to have mastered adequate clinical knowledge. This is an important question and needs comprehensive evaluation.\n", "weaknesses": "The paper could provide a more thorough justification for the introduction of the new medical dataset, especially in the context of existing evaluation datasets. The paper mentions that the existing evaluation datasets cover only some common diseases and lack extensive coverage of knowledge across various aspects of diseases. This reviewer feels that this needs to be substantiated with more thorough comparison. \nWhile it is surprising that most of the LLMs perform poorly (with over 50%) predicted to be completely wrong. The evaluation procedure used to arrive at this conclusion requires further elaboration.\nOverall I am not fully convinced that this dataset MedDisK  and the outlined evaluation procedure is robust for determining LLMs clinical knowledge yet. \nThis reviewer has listed all the concerning questions in detail below.\n", "questions": "What is the source of the EHR resource used in the preliminary making of the dataset?\nThe authors state \u201cThe existing medical evaluation benchmarks are predominantly based on question-answering (QA) tasks. These benchmarks collect questions from diverse sources, including medical examinations, electronic health records, online resources, and expert crafting\u2026\u2026cover only some common diseases and lack extensive coverage of knowledge across various aspects of diseases. \u201d  Can you compare each of these resources the paper is referring to in this sentence with MedDisK in terms of coverage of common diseases, disease base knowledge and public availability?  How does this compare with other existing medical relational databases - MIMIC, i2b2, iBKH KG etc?\nI understand that the paper does interval sampling (10 examples from each interval) and engages clinical experts to provide a categorical standard - wrong, correct or partially correct.  And this resulted in the following standard (0-0.3 is wrong) and (0.3 to 0.8 is partially correct) and (0.8 to 1.0 is correct). How representative are these categories? Did the experts find that all the samples in 0.8 to 1.0 are correct and correspondingly all in 0-0.3 are wrong? Can you provide more representative examples or more thorough classification of the \u201cCompletely Wrong\u201d category?\t\nSince LLMs response is post-processed using the NER model I think the NER model's performance is extremely crucial to evaluation. How well does it perform in identifying medical entities? From the analysis conducted in Table 8, it appears that all the LLMs are underperforming in identifying symptoms, affected sites, etc., while they generally perform well in recognizing population ages involving numeric entities. \nWould it be considered a correct hit if the model predicts 'GI tract' instead of 'digestive system' in the examples from Table 3? What kind of standardization was performed in evaluating LLMs response with the experts output?\nWhat according to the authors are the limitations of the dataset and the evaluation procedure outlined here?\n"}, {"summary": "To evaluate whether LLMs have mastered sufficient clinical knowledge, the authors first propose a large-scale medical disease-based knowledge base named MedDisK. They then develop MedDisKEval, a method that prompts LLMs to retrieve information on clinical knowledge aspects and measures the similarity between LLM-generated information and MedDisK. Results show that most of the current LLMs do not have sufficient clinical knowledge.\n", "strengths": "\nThe motivation is clear, and it is interesting to know whether current LLMs have mastered sufficient domain knowledge to help in the medical domain.\nThe authors conduct extensive experiments with 12 LLMs, which include general LLMs and medical LLMs.\nThe authors provide sufficient examples of prompt instructions, knowledge aspects, and LLM responses, which make it easier for readers to grasp the basic idea of the paper.\n\n", "weaknesses": "\nOne significant issue with this paper is that the authors may overstate the implications of their evaluation results. The experiments are exclusively conducted in Chinese. However, this critical detail is not adequately emphasized in the main paper, particularly in their conclusion that \"none of the evaluated LLMs have mastered sufficient knowledge to handle real clinical problems effectively.\" Based on their evaluation, the valid conclusion should be that LLMs do not possess adequate clinical knowledge in the Chinese language, and this finding cannot be generalized to other languages.\n\nIn the second paragraph of the introduction, the authors claim that current QA-based medical evaluation datasets cannot evaluate whether LLMs have mastered sufficient medical knowledge because those datasets cover only some common diseases. It would be more robust if the authors could further justify this statement with some analysis (e.g. to quantitatively show the coverage of diseases in the existing benchmarks).\n\nFor the proposed knowledge base MedDisK, it would be better for authors to include more details of the construction process. For example, how is the agreement among the clinical experts, is there any strategy used to tackle disagreement, and will this process introduce any additional human bias?\n\nIn section 3.2.1, the authors \"employ a specialized NER model to identify and extract medical entities from the text\". However, the exact name and citation of the used NER model are missing, and it will be more convincing to include an analysis of the accuracy of the NER model as incorrectly recognized entities could impact the evaluation results of LLMs.\n\n\n", "questions": "\nThe authors claim that current QA-based medical evaluation datasets cover only some common diseases. However, in section 3.1 where the authors introduce their proposed knowledge base, it is said that \"We first select a subset from the ICD10 database according to whether the diseases are common in clinical (determined by clinical experts) and are statistically frequent in EHR (Electronic Health Record), resulting in 10,632 common diseases.\" I wonder why they also consider common diseases in their knowledge base?\n\nIn the section of \"Disease-Knowledge-based Automated Scoring\", are there any better metrics to evaluate the similarity? The token-level BLEU-1 and ROUGE-1 cannot consider semantic meaning, and the M3E model is described as a sentence-level metric, whereas the evaluation in this context focuses on the meaning of individual tokens.\n\nIn Table 3, one completely wrong example of LLM response is \"ok, I see\". Since the authors mention that they employ a specialized NER model to identify and extract medical entities from the text, I wonder why the NER model could extract such words from the responses.\n\nIn section 4.2.2, the authors assign scores of 0, 5, and 10 to \u201dCompletely Wrong,\u201d \u201dPartially Correct,\u201d and \u201dBasically Correct,\u201d respectively, to calculate a total score. It's important to clarify how they arrived at the values \"0, 5, 10\" for this scoring system.\n\n\n"}, {"summary": "The paper evaluates the current performance of medical LLMs by creating a benchmark and testing existing said LLMs against it. This work creates MedDisK, a database designed to test the medical knowledge of LLMs on different \"clinical knowledge aspects\". These properties are not limited to those used just for diagnosis; example properties include patient population, treatment principles, departments (relevant medical departments), etc. This work also introduces MedDisKEval, a method that includes automated and clinical-expert-dependent steps to grade the performance of LLMs. Notably, the paper concludes that most current medical LLMs do not perform better than the base LLMs they are built upon.\n", "strengths": "\nThe development of a medical knowledge benchmark involved consulting 20 clinical experts over 10 months is good. This paper focuses largely on Chinese data/expert consult, but the presentation itself features relevant English translation.\nCreating a clear evaluation method combining automated/expert consultation is also useful. \nThe conclusions of the evaluation point out specific flaws in existing medical LLMs; certain models evaluate different features poorly, for example. This provides a concrete criticism/evaluation of those methods that can be built upon.\n\n", "weaknesses": "\nThe creation of a medical LLM benchmark itself does not make fundamental improvements over existing benchmarks developed in medical LLM research. As an example, the Singhal et al. 2023a paper also tested modern LLMs with human evaluation (MultiMedQA). Creating another benchmark by itself is not a conceptually novel improvement, and this work did not sufficiently argue for its improvement above these existing models/evaluations.\nThis work does not go into as much detail about the representation of medical knowledge in LLMs, providing only a benchmark without technical insight of what the LLMs might be doing or how they encode medical information.\nUsing MedDisKEval seems expensive or possibly unreliable. Someone seeking to use this evaluation method may need to consult expert opinion themselves, just to calibrate the alignment scores. The motivation behind the linear combination of BLEU-1, ROUGE-1 and cosine sim is empirically driven and is not inherently convincing as a metric.\n\n", "questions": "\nThe database MedDisK was constructed with \"clinical experts and machine assistance.\" Further clarification is required; what was the exact process of constructing the database, and how was machine assistance used? \nThis work focused on a set of LLMs that is somewhat disjoint from existing popular medical LLMs. For example, MedPaLM?\n\n"}]}, {"name": "SIMULTANEOUS GENERATION AND IMPROVEMENT: A UNIFIED RL PARADIGM FOR FJSP OPTIMIZATION", "key_word": ["Reinforcement Learning", " Flexible Job Shop Schedule Problem", " FJSP"], "sn": 9463, "reviews": [{"mark": [1, 1, 2], "rate": 3, "confidence": 3}, {"mark": [2, 2, 3], "rate": 3, "confidence": 3}, {"mark": [2, 1, 2], "rate": 3, "confidence": 3}, {"mark": [1, 1, 2], "rate": 3, "confidence": 4}], "abstract": "We present an end-to-end reinforcement learning framework designed to address the Flexible Job Shop Problem (FJSP). Our approach consists of two primary components: a generative model that produces problem solutions stepwise, and a secondary model that continually refines these (partial) solutions. Importantly, we train both models concurrently, enabling each to be cognizant of the other's policy and make informed decisions. Extensive experimentation demonstrates that our model delivers better performance in shorter time on several public datasets comparing to baseline algorithms. Furthermore, we highlight the superior generalizability of our approach, as it maintains strong performance on large-scale instances even when trained on small-scale instances. It is worth noting that this training paradigm can be readily adapted to other combinatorial optimization problems, such as the traveling salesman problemand beyond.\n", "detail": [{"summary": "The paper introduces a reinforcement learning framework tailored for the Flexible Job Shop Problem (FJSP). The methodology leverages graph neural networks, allowing the model to handle FJSP instances of varying scales. The main novelty consists of simultaneous generation and improvement: a generative model sequentially produces solutions while an improvement model refines them. Both models are trained concurrently via reinforcement learning. The approach is at least an order of magnitude faster than metaheuristics and outperforms dispatching rules and some previous RL approaches in terms of solution quality.\n", "strengths": "The tackled problem is important in several practical scheduling applications. Unlike previous approaches that either only generate solutions in one shot or only learn to improve, the proposed approach trains two models to generate and improve at the same time, which could potentially provide the \u201cbest of both worlds\u201d, i.e., speed of one-shot generation and solution quality of improvement methods. The proposed two-stage approach and training is novel for scheduling problems to the best of my knowledge.\n", "weaknesses": "My biggest concern is that the proposed approach seems to be only applicable to a specific scheduling problem (FJSP) with no variation in terms of constraints. In the abstract, the authors state that:\n\nIt is worth noting that this training paradigm can be readily adapted to other combinatorial optimization problems, such as the traveling salesman problem and beyond.\n\nhowever, there is 1) no empirical evidence to justify the claim and 2) no explanation of how this can actually be done. For instance, how can the improvement step be applied to the traveling salesman problem (TSP), especially considering the reward function? In several combinatorial optimization problems, it is hard to define a step-wise reward function, such as in routing problems such as the TSP. Moreover, the specific design of Section 3 seems to be over-fitted to the FJSP, with new problems requiring a substantial restructuring of the model.\nAnother important point is that the proposed generation-improvement method is not well justified in terms of performance. There is no ablation study on just using the generator model without any improvement:\n\nFrom the design of our framework, it can be seen that the generative model and the improved model can run independently, which means that we can only use the generative model to generate a complete solution or use the improved model individually to improve any feasible initial solution generated by other methods (such as random generation or PDRs).\n\nbut there is no result about this; in Table 3 only the improvement method alone is shown with other models. How would the generate only perform? Moreover, a natural question would arise, namely why authors decided to go for a potentially more burdensome generate+improve method (in which the generator may potentially be worse due to over-reliance on the improvement model), and not just a generator. In these regards, it would be interesting to see how the same model with the generation part only would do.\nThe experimental section seems to be lacking some baselines - for instance, Table 1 only compares against OR-Tools and dispatching rules, but not against RGA and 2SGA and the 2 DRL baselines. Also, OR-Tools is missing the solution time, so it is difficult to assess how the proposed approach compares in solution time (given that the quality is already worse than the OR-Tools metaheuristics). Finally, no standard deviation has been reported nor multiple runs.\nIn terms of the quality of the paper, there is room for improvement. Aside from several typos, the writing feels sloppy, and there are missing references ((ref)  in the paper, mk[?] in Table 2 and more), so I would suggest some revision. More importantly, in Algorithm 1:\n\nStore Transition \u03c4t+1g:<St;\u00a0St+1>into\u00a0EPI\n\nI believe this should be, in fact, EPI , given that EPG is not being used here.\nFinally, no code has been provided to reproduce the results.\n", "questions": "\nWhy did you decide to use DuelingDQN, and not actor-critic algorithms such as PPO [29] or policy gradient methods as done in MatNet [17]*?\nAs in the \u201cweaknesses\u201d section, how would the model perform if only the generator was trained? And what if we trained with generator+improve but only used the generator for the solution?\nWhat is the number of improvement iterations nt, and how was it selected?\nHow would the proposed method fare in larger-scale instances? [29] studies scale up to 100\u00d760.\n\n\n*Note: MatNet [17] is cited in the manuscript but not referenced throughout the text. It may be useful to at least briefly introduce the differences with the proposed method in the related works.\n"}, {"summary": "This paper proposes an end-to-end RL framework to solve the Flexible Job-Shop Problem (FJSP). The framework consists of two major components: a generation model that produces an assignment of operations that updates the partial solution, and an improving model that refines the current partial solution. By repeating the generation and improving steps until the complete solution is found, the proposed framework finds a solution for FJSP.\nThe authors evaluate the proposed framework with various-sized FJSP instances, and it is shown to outperform compounded Priority Dispatching Rules (PDR) but underperform Meta-heuristics (e.g., OR-tools).\n", "strengths": "\nThe proposed framework suggests a novel perspective for solving FJSP. Unlike the majority of iterative improving approaches that often perform improvement steps from a complete solution, the proposed framework employs \"improving\" actions during solution construction.\n\n", "weaknesses": "\nThe current manuscript still has room for improvement, including a more detailed explanation of the training.\nThe performance evaluation of the proposed framework seems quite limited, especially as the baselines are overly simplified in Table 1.\n\n", "questions": "\nIt seems the number of improvement iterations nt would play a crucial role within the proposed framework. Could the authors provide further details on how to decide nt? In the current manuscript, it is simply mentioned as a hand-crafted function depending on the iteration index t.\nWhat is GIM in Table 3? From the context, I assume it is the proposed method, but the acronym is never introduced.\nWhat is \"Generate+improve\" in Table 3? Is it different from GIM?\n\n"}, {"summary": "This paper proposes an RL based scheduling methods for Flexible Job Shop Problem. The approach empolys two graph neural network models, a generative model and an improvement model, which collaboratively solve the problem. At each timestep, the generative model progressively constructs a partial solution by adding a new component into the existing partial solution, and the improvement model refines this partial solution for better performance. Both models are designed to leverage inductive biases from the problem and its current partial solution, e.g., neighbor nodes from different types of edge. The models are trained end-to-end using the reward signal for each model, in an alternating manner to stabilize the learning of two models.\nThe proposed algorithm is evaluated with two experiments, one for synthetic datasets and the other for public benchmarks, and it showed superiority over several heuristics and DRL-based methods in terms of solution quality. Also, though the method failed to outperform the meta-heuristic algorithms, it showed comparable result while spending much less time than the meta-heuristics.\n", "strengths": "\nThis work proposed a new RL-based framework for solving FJSP, which combines the construction and the improvement processes so that they can be trained in end-to-end manner.\nThe method utilizes different graph representation that corresponds to a single partial solution, providing each model with relevant information. This approach is both interesting and convincing. \nAblation study for the two distinct models provides a good empirical evidence for the proposed architecture.\n\n", "weaknesses": "[Methods and Experiments]\n\nThis paper doesn't provide a clear rationale or justification for the use of various embeddings. Also, there's no ablation study for these design choices.\nThe method is evaluated only two public benchmarks, whereas the DRL baseline [1] has been tested on a more extensive set of benchmarks. This raises concerns about the comprehensiveness of the evaluation and potentially limits the generalizability of the proposed method's performance.\nThe reported performance of DRL baseline [1] is based on the greedy selection, while the method of this paper leverages sampling for improvement steps. For fairer comparison, the results from both greedy and sampling decoding should be included. Note that sampling performance reported in [1] for v_la task is better than the proposed method, while consuming more computation time.\n\n[Writings]This paper has significant defects with clarity. \nFirst of all, there are too many typos, wrong spacing and inconsistent notaions. Below are some of them:  \n\npage 1: 'PRD' \u2192 'PDR'\npage 2: There are many wrong spacing in Sec. 2, e.g, 'O_i,which', 'operations,O_{ij}', or \"...end of production.These two ...\"\npage 3: There is a wrong figure reference, '(in figure)' \npage 4: 'avenger' \u2192 'average'\npage 4: GAT has no reference\npage 5: 'avitation' \u2192 'activation'\npage 5: 'M_{ij}' suddenly pops up, which supposedly typo of {A_I}_{ij}, and suddenly A_J is used, which is definitely a typo.\npage 7: In the Algorithm 1, 'EP_I' \u2192 'EP_G' for the transition of generative model.\npage 7: There are several '(ref)'s in Sec 5.1, which should have been replaced by appropriate reference.\npage 8: In the text they say they use Gurobi Solver, but they report OR-Tools in the table.\nThroughout the paper, the authors use abbreviations without declare it, e.g., DRL in page 3, GAT in page 4, and GIM in page 7 (Algorithm 1)\n\nMoreover, the models are not clearly described, which makes it hard to fully understand the algorithm.For example, in GAT Module section in page 5, it is unclear whether W is shared among different u's or not.Also, the reward for each model is not stated mathematically, which introduces an ambiguity.\n[1] Song, Wen, et al. \"Flexible job-shop scheduling via graph neural network and deep reinforcement learning.\" IEEE Transactions on Industrial Informatics 19.2 (2022)\n", "questions": "\nHow long it takes for training?\nWhy the 40 x 10 result is missing for OR-Tools?\nHow can this work be extended to other scheduling or CO problems?\n\n"}, {"summary": "The author proposed a deep reinforcement learning model to address the FJSP problem. This approach involves the simultaneous application of construction heuristics and improvement heuristics, enabling it to achieve better performance in shorter time on several public datasets.\n", "strengths": "\nBased on the claim of paper, it seems good to use construction heuristic to construct a better partial solution and use improvement heuristic to improve the partial solution.\n\n", "weaknesses": "\nActions (in Section 3.1) are critical, but not defined clearly. I have no problems with actions for construction heuristics, but actions for improvement heuristics are not well defined. In Section 3.2 \u201cInsertion Position Embedding\u201d (P5), the definition of insertion position is undefined clearly, and why the number of choices is (n+m) for each operation. Besides, it is also unclear about why the total number of insertion positions is equal to n\u00d7(n+m). For these unclear descriptions, there is no clue to understand the proposed method. Note that in Section 3.2 \u201cPolicy Model\u201d (P6), there is no way to understand the description \u201cObviously, there are at most m different insertion schemes for each improvement decision.\u201d\n\nFigure 2 is confusing and unconvincing. For example, why is 31 moved to the position after 11, not before 11? If it can also be moved to that before 11, I don\u2019t see the strategy.\n\nThe representation of operations is inconsistent and thus makes it hard to understand how the Insert Position Embedding works (There are Oij,Oj,Oj(i),Oi in the article).\n\nLack of test results for public benchmark dataset. With comparison to [29], you should also compare with la(edata) and la(rdata). And you may test on the dataset which is referenced by [29] to improve the reliability of your method.\n\n\nPresentation comments: \n\nLack of spaces in many places. E.g., \u201cBoth the generative model and the improvement model will use formula(4) to select the action to be executed in the current state st at step t on their respective feasible action sets.The advantage value function is fitted by a parametric MLP\u201d \n\nIn section 5.1, \u201c In addition, we also used (ref1),(ref1),\u201d, and \u201cmk [? ]\u201d in Table2. Please carefully check the content.\n\nIn Algorithm1, \u201cE%K\u201d => \u201ce%K\u201d, the second \u201cEPI\u201d => \u201cEPG\u201d, etc. There should be more that you need to find out for fixing. \n\nThere is no data in some places in the tables, such as 40x10 for OR-Tools in Table1 and mk[?] for UB* in Table2.\n\n\n", "questions": "\nI am still wondering about your method for the Machine Process Queue Embedding:\nIs Mij=1 if Oj is processed on Machine i, or Oj \u201ccan be\u201d processed on Machine i? What is the concept of model designing (or why it is designed in this way)?\n\nIt\u2019s not clear that \u201cJob Sequence Embedding\u201d, if Oij (j-th operation of Job i) is processed then AJ(Ji,Oij) = 1?\n\n\n"}]}, {"name": "Model-Agnostic Shift-Equivariant Downsampling", "key_word": ["Shift equivariance", " Shift invariance", " Downsampling", " Convolutional neural networks"], "sn": 9459, "reviews": [{"mark": [4, 2, 3], "rate": 8, "confidence": 4}, {"mark": [2, 2, 2], "rate": 3, "confidence": 4}, {"mark": [3, 3, 3], "rate": 5, "confidence": 4}, {"mark": [2, 1, 2], "rate": 5, "confidence": 4}, {"mark": [4, 3, 3], "rate": 6, "confidence": 4}], "abstract": "The performance of convolutional neural networks (CNNs) are thought to be insensitive to image shifts. However, recent studies have revealed that downsampling layers in CNNs result in inconsistent outputs for shifted input images. In this\nstudy, we present an approach for performing downsampling that ensures absolute shift equivariance. By employing model-agnostic downsampling method that leverages origin selection functions obtained from coordinate-independent statistics of the feature map, we can achieve perfect shift equivariance, while still adhering to the conventional downsampling procedures. Our method allows CNNs to exhibit both improved accuracy and perfect shift invariance for image classification, while also achieving shift equivariance in semantic segmentation benchmarks. Furthermore, we introduce a methodology for achieving shift equivariance without the need for any additional training process. This is accomplished by transferring pretrained weights and replacing existing layers with shift-equivariant\ncounterparts. Additionaly, we show that fine-tuning of the modified CNNs leads superior performance compared to previously proposed models.\n", "detail": [{"summary": "The authors propose and implement MASS, a method for downsampling that is exactly equivariant to shifts in images, making CNN networks exactly equivariant (or invariant) with respect to this symmetry in their downstream tasks (which are typically image classification). The method is implemented in a \u201cmodel-agnostic way\u201d, and can be used to invariantize pre-trained CNNs. Its performance is demonstrated on standard data sets.\n", "strengths": "The problem of imposing exact shift equivariance (in CNNs) is important in current literature and applications. The proposed solution is simple, robust, and easily applicable to pre-existing methods.\n", "weaknesses": "Main\n\nThe algorithmic/mathematical presentation should be clearer. Occasionally notation appears that has not been precisely defined (e.g. So on pg. 4 or the use of the y variable). I specifically find Figure 1 hard to understand. \n\nA substantial effort is made to separate the proposed method from general polyphase sampling, but the exact reason behind the latter\u2019s \u201cperformance degradation\u201d should be explained more rigorously, as this is what would set it apart in applications.\n\n\nMinor\n\nThere are several typos, and all acronyms need to be defined upon first appearance.\n\n", "questions": "Regarding the choice of selection rule:\n\nIn what way does the choice of function matter? Can it be determined adaptively, if the noise distribution is known?\nAnd finally, is it always possible to find a good function regardless of the level of noise?\n\n"}, {"summary": "The paper presents an approach to shift equivariant up- and downsampling in convolutional neural networks.\n", "strengths": "\nThe paper presents a very compelling motivation for shift equivariance based on industrial applications\nThe paper seems to provide the shift-equivariant scheme that is consistent by design and also compatible with multiple architectures\nThe proposed approach in principle could be used without fine-tuning if some accuracy loss is acceptable, unlike some other methods in the literature\n\n", "weaknesses": "\nThe description of theory is not at all clear, especially the part related to figures. Maybe if I read this paper 2 or 3 times, I will eventually understand what color schemes imply in Figures 1,2,4. But I want to be able to understand this from the first glance. Figures should serve the purpose of clarifying things and not making them more obscure. Without the explanation of what colors are supposed to signify and explain in the figures - it is impossible to quickly understand what they are supposed to clarify.\nThe compelling motivation for shift equivariance is not supported by problem specific datasets. All experiments are done on generic datasets. CIFAR-10 does not seem to fit the motivation at all with its 32x32 images. It seems like a misfit for the purpose of the paper. I expect that the industrial applications involve high-resolution imagery. If authors can provide results on high-resolution datasets, especially from the industrial domain this will make the results a lot more compelling. There is a recent dataset described here: https://arxiv.org/pdf/2303.06673.pdf. I am sure that more search will reveal more datasets like this. I remember encountering similar problems on kaggle.\nConsistency metric defined in equation (4) does not make any sense to me. What does it measure, what is x and y? Is it pointwise pixel match, if so, why there is no summation over pixels? One of the closing brakets is missing.\nBaselines used in experimental tables are not explained well. As a result, the experiments do not seem persuasive\nWhat is the reference for DDAC and LPF?\nWhy LPS is not included in Tables 2,3?\nWhy Table 3 does not contain same baselines as Table 2? It seems that a few of the baselines in Table 2 are very effective. It may well be that APS with enhancements presented in Table 3 might be as effective or better than MASS?\n\n\nResults in Table 2 are marginal and statistically insignificant. To me, the value of this result is approaching 0, because most confidence intervals overlap. It does not make sense to use bold font to signify the best model, when the best model is not significantly different than another model.\nFrom Tables 2-4, I do not see a decisive value of the proposed approach with respect to other approaches such as LPS-DDAC-3 or ASP-DDAC-3. Why do we need this approach, what is the value?\nI am not sure I see value in using the pretrained version of any of the approaches discussed in the experimental section. What is the point, can you explain in detail the actual use case? When the networks are fine-tuned properly, many of them achieve very similar results.\nTable 5 confirms previous concerns.\n\n", "questions": "\npage 2: missing reference. \"2016; ?), and group operations\"\npage 6: typo \"Schemetic of MASS\" -> \"Schematic of MASS\"?\nis your approach compatible with visual transformers?\n\n"}, {"summary": "The proposed work suggests a downsampling technique that is shift equivariant by equivariant origin alignment. This method is well suited for adapting to the existing layer and can utilize the weights of pre-trained models. Moreover, the proposed models outperformed other shift equivariant techniques without introducing more learnable parameters.\n", "strengths": "The proposed method achieves perfect shift equivariance and performs better in classification tasks, even without any learnable sampling parameters (compared to the LPS). It is also well-suited for reusing pre-trained weights.\n", "weaknesses": "\nThe proposed method resembles building equivariant layers with canonical functions [a]. The proposed method can be seen as a special case of the mentioned work for shift equivarinace. This limits the contribution of the paper. \n\nThe benefit of the proposed technique compared to the existing method (APS, LPS) is poorly described.\n\nThe improvements are marginal.\n\n\na. Equivariance With Learned Canonicalization Functions\n", "questions": "\nFigure 2 caption: \u201cOn the other hand, in the case of polyphase sampling, only data from specific regions is retained, which may result in potentially suboptimal representations. MASS-Max-pool combines the advantages of selecting suitable representatives and ensuring shift equivariance.\u201d\u2014 I do not entirely understand the statement. We can perform convolution with max filter followed by LPS. What is the extra benefit of MASS-Max-pool?\nSection 4.1 \u201c there exist only s2 unique sampling origins represented as o \u2208 {(0, 0),(0, 1), ...,(s, s)}.\u201d \u2014 should it be \u201c{(0, 0),(0, 1), ...,(s-1, s-1)}.\u201d?\nWhile training from scratch as the MASS-Max-pool shifts the input to match the new calculated origin, does it likely introduce unwanted data augmentation? Especially if the pooling window is large.\n\n"}, {"summary": "This paper addresses the problem of how to preserve shift-equivariance property for convolutional neural networks. Specifically, the authors merely consider circular shift operation over the input image sample, and simply extend an existing method APS (adaptive polyphase sampling, proposed by Anadi Chaman and Ivan Dokmani\u0107 in their CVPR 2021 work) by incorporating a pre-defined selection function for determining the origin which can accurately fit the shift operation. Experimental validation is conducted on image classification and semantic segmentation tasks.\n", "strengths": "\nThe problem, i.e., how to preserve shift-equivariance property for convolutional neural networks, is critical.\n\nThe proposed method is simple and hand-crafted even though its implementation is not clear.\n\nComparative experiments are conducted on both image classification (with CIAFR-10 and ImageNet-1K datasets) and semantic segmentation (with PASCAL VOC dataset) tasks.\n\n\n", "weaknesses": "\nThe method and presentation.\n\nIn this work, although the authors addresses a fundamental research problem, how to preserve shift-equivariance property for convolutional neural networks, the proposed method called MASS is rather incremental, lacking new tech insights. To the best of my knowledge, MASS is merely a simple modification of existing work APS (adaptive polyphase sampling) proposed by Anadi Chaman and Ivan Dokmani\u0107 in their CVPR 2021 paper. Specifically, the authors use a pre-defined selection function for determining the origin with ASP which can accurately fit circular shift operations over the input image sample.  Generally, I have not seen any insightful differences against APS. \nThe presentation of the method is poor: 1) usually no explanations for notations and terms appeared in formulas; 2) no explanations/details on the formulation of the proposed MASS; 3) rather coarse descriptions for Figure 1 and Figure 2; 4) some sub-figures are wrong, e.g., two sub-figures for MASS in channel 1 of Figure 1 are not consistent to the others.\nThe writing of the paper is also poor. Please see my comments in \"Others\" part for details.\n\nThe limitations.\n\nThe authors did not discuss on the limitations of the proposed method.\n\nThe experiments.\n\nNote that the authors claim that the pre-defined selection function for determining the origin can accurately fit the shift operation. However, the authors did not provide any details on how to implement it in experiments. This makes experimental comparison confusing. \nComparison is limited to APS.\nExperiments are not convincing. On CIFAR-10 dataset, the proposed MASS brings very marginal gains to APS. On PASCAL VOC dataset, MASS performs worse than APS. However, on ImageNet-1K dataset, MASS is much better than APS. What are the root reasons? \nThere is no ablation to study how does the proposed method MASS work. \nHow about the performance of MASS under other shift operations to the input image sample instead of circular shift operations?\n\nOthers.\n\nThe writing can be improved significantly. There exist numerous typos, grammar errors and inaccurate descriptions throughout the whole paper. Here, I just list some example errors in the \"Related Works\" section:\n\n\"Cheng et al.,2016; ?\" -> an inaccurate citation;\n\"a lack of shift equivariance occur\" -> \"a lack of shift equivariance occurs\";\n\"While careful augmentation strategies substantailly improves\" -> \"While careful augmentation strategies substantially improve\";\n\"Another line of research is to apply anti-aliasing low-pass filter, which originate\" -> \"Another line of research is to apply anti-aliasing low-pass filter, which originates\";\n\"This anti-aliasing concepts are\" -> \"This anti-aliasing concept is\";\n\"The first absolute shift-invariant method for image classification tasks are proposed \" -> \"The first absolute shift-invariant method for image classification tasks is proposed\";\n\"While the selection of the polyphase components of APS is based on the l2 norm, learnable polyphase sampling (LPS) generalize to select\" -> \"While the selection of the polyphase components of APS is based on the l2 norm, learnable polyphase sampling (LPS) is generalized to select\";\nMany citations are not formal, even to APS.\n\n", "questions": "Please refer to my detailed comments in \"Weaknesses\" for details.\n"}, {"summary": "The paper presents a strategy to achieve perfect equivariance in convolutional neural networks. That is, the model produces exactly the same output when the input image is shifted horizontally or vertically. This is achieved by preserving statistics of the positions of the downsampling process in the pooling layers of convolutional networks. The results indicate that the method works perfectly for downsampling (classification) and upsampling (segmentation) operations without the need to re-train the models.\n", "strengths": "\nThe problem is important and the paper is well motivated.\nThe solution is simple and generic, can be applied to any convolutional model.\nThe results are strong. 100% equivariance is achieved in all experiments, demonstrating the effectiveness of the proposed solution.\nThe approach works on downsampling and upsampling paths of CNNs. The evaluation includes image classification and semantic segmentation.\n\n", "weaknesses": "Main comments:\n\nEquivariance is demonstrated only for inference (test) time. It is unclear how the method would facilitate equivariance during training. In other words, by implementing MASS in all pooling layers, what augmentations would be unnecessary when training a new model? The only experiments that involved training a model from scratch were conducted with the CIFAR dataset, but the augmentation procedure was not explained. More analysis of equivariance during training would be informative.\nIn general, the explanation of the method has a few gaps that could be better presented and clarified. For instance, the paper indicates that previous work ignores classical sampling theory, but how MASS uses classical sampling theory is not explained later. Also, it is unclear what the authors mean by \"MASS meticulously preserves the initial downsampling process\". The introduction indicates that MASS uses input data statistics to select the origin, but these statistics are not clearly defined later. The procedure could be more formally presented to avoid confusions.\nThe paper mentions that non-equivariant methods can display severe accuracy drops, but this does not seem to be reflected in the results. The consistency of other methods is usually above 80% and the classification rate remains high. If shifts are introduced randomly, the accuracy of a non-equivariant method can change every time. Reporting how the results change with the amount of shift introduced to break the classification of a model would be informative.\n\nOther comments:\n\nIt is unclear if the method results in computing or memory overhead (even if minimal, what additional operations / variables are added compared to regular pooling).\nSome acronyms are not clearly defined, such as APS, LPF and DDAC. Table 2 uses them extensively without citations or explanations to what they refer to exactly.\nSome minor typos: equivaiant, \"architectures.architectures\", experimants, inializing.\n\n", "questions": "\nDoes the proposed method remove the need for using certain augmentations during training?\nAre the accuracy results reported without shifts? The experimental procedure is unclear, please explain.\nCan you add results of how shifting affects accuracy in shift-sensitive models?\n\n"}]}, {"name": "Visuo-emotional perception and Human Cognition to engineer content-generation using Generative AI", "key_word": ["creative content", " digital creatives", " attention", " personalization", " content optimization", " content generation", " generative AI"], "sn": 9458, "reviews": [{"mark": [2, 2, 2], "rate": 3, "confidence": 2}, {"mark": [1, 3, 2], "rate": 1, "confidence": 5}, {"mark": [3, 3, 2], "rate": 3, "confidence": 4}, {"mark": [1, 2, 1], "rate": 3, "confidence": 3}], "abstract": "Media platforms compete for users\u2019 attention. Their reach crucially depends on algorithmic real-time bidding and efficiency of hyper-personalized, rapidly generated, and user-optimized content. Attention is, although, a scare and fleeting quantity, often awarded less than 1 second per stimulus. Thus, the current strategy is to rely on the vast amount of user-generated data to mimic the content to the user. The underlying assumption is that this is sufficient incentive for attention. This strategy has evidently failed. As witnessed by the alarmingly low or short-lived successes of campaigns in recent times. This mismatch is exacerbated because most content consumed today is digital. Whereas strategies for digital content mimic our past understanding from mass-media. Hence, we formalize a new understanding of communication, specifically for the digital mediums. We prove that the digital medium needs a new understanding of communication protocols. To that end, we take a first principles approach to the new communication protocol: the neurological representations of communication, specifically, where the communication happens in less than 1 second per stimulus. First, we break down and elaborate on this neurological representation of decision-making. Next, we proffer use of our behavioural communication model for generation and optimization of content creatives. To that end, we elaborate methods for rapid, AI-generation content, increasing the efficiency of visual communication on digital media. Within this exploration we include themes of Hyperpersonalization and Search-engine optimization. Thus, we find that strategically produced content exhibits stronger associations to users\u2019 nonconscious needs, wants and goals, which elicits user attention and content-diversity significantly.\n", "detail": [{"summary": "The paper proposes a method for rapid, AI-generated content, increasing the efficiency of visual communication on digital media. Within\nthis exploration the authors include themes of Hyperpersonalisation and Search-engine optimation.\n", "strengths": "I have not found any strengths of this paper.\n", "weaknesses": "\nThe theme of this paper may not be closely related to the conference, as it is only an engineering specification and lacks theoretical explanation.\n\nThe presentation of the paper is chaotic, making it difficult to read.\n\nThe method mentioned in the paper, which utilizes ChatGTP to generate accurate prompts and generates high-quality digital advertisements using this prompts and a large text-to-image model, has been widely applied in the engineering field and therefore lacks innovation.\n\n\n", "questions": "Please refer to Weaknesses.\n"}, {"summary": "This paper mainly investigates whether generative AI can produce content to attract the user and explains the procedure of content generation from the neuroscience perspective.\n", "strengths": "\nThis paper introduces a communication protocol to explain how the brain has been triggered, the entire process is fluent and reasonable.\nThe content of the pre-research is sufficient.\nThe strategy of the prompt is meaningful.\n\n", "weaknesses": "\nThis work is too simple, just using the existing GenAI to produce the context and comparing it with the corresponding items.\nThe prompt is hand-crafted, and cannot be applied flexibly.\nThe number of samples in the experiment is too small, and the experiments should cover more scenarios.\n\n", "questions": "\nHow do you confirm the prompt is reliable and the output of the GenAI is following the rules?\nSince it's an online experiment, why not invite more people?\n\n"}, {"summary": "The paper proposes a framework that leverages Generative AI to create Ad creatives that aim to increase the Click-through rate of advertisements. The framework and ad creative generation leverage four principles: 1) the evolutionary category need, 2) past memories and brand guidelines; 3) the strongest emotional memory, and 4) context with photographic details.\n", "strengths": "The paper does a good job of motivating and explaining the problem, as well as providing all the necessary details and motivation to understand the necessary background. Also, the paper focuses on an interesting aspect of generative AI and how it can be used to generate ad creatives with the goal of increasing click-through rates that, over the years, have been declining. Overall, I think that this work has the potential to inform various interested stakeholders, including advertisers, policymakers, and social media operators. Also, I like the paper\u2019s approach that aims to leverage the power of Generative AI (particularly ChatGPT) to generate content based on principles obtained from the neuroscience field.\n", "weaknesses": "My main concerns with the paper are related to the framework\u2019s evaluation. I believe that the evaluation is quite limited and simplistic, given that the sample of the recruited participants is biased (the overwhelming majority being from India) and the evaluation focuses on only two products. I suggest that the authors explain and motivate how they perform the user recruitment procedure and the reason why they selected the two products. Overall, given these limitations, it\u2019s unclear whether the paper\u2019s results are generalizable.\nAdditionally, the paper fails to explain how this study is different from previous efforts that aim to understand the use of neuromarketing methods without the use of Generative AI to create the ad creatives. The presented framework can also be applied by people to generate ad creatives, so its unclear if the novelty of this work lies in the formulation/use of the framework or the combination of the framework with Generative AI models like ChatGPT. I suggest to the authors to better contextualize their work and better explain the novelty of this work. \nAlso, the paper does not explain how the envisioned framework will be applied in practice. The paper\u2019s evaluation defines a set of prompts that are very specific to the products that are studied and generates creatives that are then subsequently used to compare the user perceptions vs. ad creatives that simply show the product with a white background. Overall, it\u2019s unclear on whether the envisioned framework can be applied in practice without great input and effort from experts that will guide the generation of the ad creatives. \nIn addition, there is a disconnection between the motivation of the work and the framework/evaluation. The framework does not account for user personalization, which is an important aspect when considering the ad ecosystem. So I am wondering how the paper is planning to incorporate user personalization in this framework and how Generative AI models can assist in this, especially when considering the privacy concerns that may arise from sharing user-specific data with companies that offer LLM solutions (e.g., OpenAI).\nTo summarize, I believe that this work is interesting and important, however, at this stage, I believe that the paper is not ready for publication. In addition to the above concerns, I would like to make the following suggestions to the authors (mainly minor issues):\n\nThere are a couple of references listed as Anonymous, when they are not Anonymous so I suggest fixing these issues.\nConsider not using pie charts for the evaluation results, given that it is one of the worst visualization methods.\n\n", "questions": "\nHow did you recruit participants, and why most of them are from India? How can the recruitment approach affect the presented results? \nHow are the two products selected? Are these products popular in India, where most participants are from?\nHow is this study different from previous efforts studying the use of neuromarketing methods vs. plain advertisements like the ones shown to the participants (plain background with the product in the middle)? Is the novelty of the work the use of ChatGPT to generate the ad creatives?\n\n"}, {"summary": "This paper leverages the content generated by generative AI to enhance the critical last moments of decision-making. Grounded in the understanding that long-duration decisions are the cumulative result of numerous micro-decisions, the author dissects the final seconds of an e-commerce purchase into multiple stimuli. Subsequently, the author introduces a four-point prompt strategy, informed by the outcomes of this analysis. After that, to validate the efficacy of this prompt strategy, the author conducted a series of experiments.\n", "strengths": "Thanks for your interests in ICLR! Overall, this is an interesting paper on a topic which is of interest to ICLR Conference. It offers valuable insights into the application of neuroscientifically designed content to enhance ad click-through rates. The paper astutely recognizes the promise of leveraging Generative AI for this purpose. Building upon this foundation, the author thoughtfully presents four distinct prompt strategies and supports them with well-structured experiments, thus substantiating the validity of their approach.\n", "weaknesses": "While the author presents a comprehensive theoretical framework and provides clear and detailed insights into the prompt strategies, there is room for improvement in the experimental validation of the proposed techniques. As outlined in the paper, the experiments are limited to a single product tested on a sample of 236 participants. Given the potential applicability of this technique to a wide range of products, the scope of experimentation appears somewhat narrow. Expanding the experiment set to encompass a more diverse array of products would strengthen the paper's claims.\nAdditionally, the manual design of prompts by the author may not be a scalable solution when considering the need for ad design across a vast array of products. Further exploration of automated or semi-automated prompt generation methods could enhance the paper's practicality and applicability in real-world scenarios.\n", "questions": "\nHave you explored the possibility of automating the prompt generation process for various products?\n\nCould you provide insights into any supplementary experiments conducted to further validate the effectiveness of the prompt strategy outlined in your paper?\n\n\n"}]}, {"name": "Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships", "key_word": ["large language model", " training data extraction", " fine-tuning", " pseudo-labeling with membership", " privacy"], "sn": 9456, "reviews": [{"mark": [3, 2, 3], "rate": 6, "confidence": 2}, {"mark": [3, 3, 3], "rate": 6, "confidence": 3}, {"mark": [2, 3, 2], "rate": 5, "confidence": 4}, {"mark": [2, 3, 3], "rate": 6, "confidence": 3}], "abstract": "Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities. Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure. We discuss potential mitigations and suggest future research directions.\n", "detail": [{"summary": "This work investigates how model fine-tuning may potentially make the models more vulnerable to leaking their pre-train dataset. The authors apply the machine-generated text with more like human-written to fine-tune the language models. Reinforcement learning with self-generation is employed to fine-tune the models. To demonstrate the effectiveness of their approach, the author conducts experiments on six datasets over 6 language models with different amounts of trainable parameters.\n", "strengths": "\nThis work proposes a new perspective to make data extraction attacks on pre-training language models easier.\nThis study performs experiments across diverse datasets and various models, enhancing the generalizability of the empirical analysis.\n\n", "weaknesses": "\nWhile the author explores various models in the experiments, there is a noticeable lack of diversity in their architectures; all the studied models originate from the same architectural family.\nIt would be valuable if the authors could show some qualitative results, e.g., reconstructed text in the model fine-tuning with their approach and the standard approaches.\nThere is no model utility performance comparison between this work and the other work.\n\n", "questions": "see weakness.\n"}, {"summary": "The paper proposes a new attack strategy to increase the exposure of private training data from pre-trained language models. The main contributions are:\n\nThe paper introduces a novel scenario where an attacker fine-tunes a pre-trained language model with self-generated texts that are pseudo-labeled based on their machine-generated probabilities. The paper assumes that texts with lower machine-generated probabilities are more likely to contain training data.\nThe paper uses a zero-shot machine-generated text detection method (DetectGPT) to calculate the perturbation discrepancy of each generated text, and a reinforcement learning from human feedback method (RLHF) to fine-tune the target language model to favor texts with lower perturbation discrepancy.\nThe paper evaluates the proposed attack strategy on six versions of the OPT language model and shows that it can amplify the training data exposure by four to eight times compared to the reference models. The paper also analyzes the extracted samples and discusses potential mitigations and future research directions.\n\n", "strengths": "\nOriginality: The paper introduces a novel attack scenario where an adversary fine-tunes a pre-trained language model to amplify the exposure of its training data. This strategy differs from prior studies by aiming to intensify the model\u2019s retention of its pre-training dataset. The paper also proposes a two-step approach to achieve this goal, involving pseudo-labeling based on machine-generated probabilities and reinforcement learning with self-generations. To the best of my knowledge, this is the first work to explore such an attack strategy and demonstrate its feasibility and effectiveness.\nQuality: The paper is well-written and provides sufficient technical details and empirical evidence to support its claims. The paper follows the standard structure of an ICLR submission and adheres to the formatting guidelines. The paper also discusses potential mitigations and countermeasures against the proposed attack, as well as open questions for future research. The paper uses appropriate references and citations to acknowledge previous work and situate its contribution in the literature.\nClarity: The paper is clear and easy to follow. The paper defines the threat model, the adversary\u2019s capabilities and objective, and the main steps of the attack strategy in a precise and coherent manner. The paper also explains the rationale and intuition behind each step of the attack, as well as the challenges and assumptions involved. The paper uses figures, tables, and equations to illustrate the key concepts and results. The paper also provides qualitative analysis of extracted samples and discusses the limitations and implications of the attack.\nSignificance: The paper addresses an important and timely problem of training data extraction attacks on neural language models, which pose serious privacy risks for both data owners and model users. The paper demonstrates that such attacks can be amplified by adversarial fine-tuning, which can increase the exposure of sensitive training data by up to eight times. The paper also provides insights into the factors that affect the vulnerability of language models to such attacks, such as model size, training dataset type, and perturbation function. The paper contributes to advancing the understanding and mitigation of privacy threats in language modeling.\n\n", "weaknesses": "\nThe paper does not specify how the adversary evaluates the effectiveness of the TDE attack, and what are the assumptions and limitations of the attack scenario. The paper also does not compare or contrast its attack strategy with existing TDE attacks in terms of feasibility, scalability, and practicality.\nThe paper relies on a single zero-shot machine-generated text detection method (DetectGPT) to pseudo-label the self-generated texts, without considering other possible methods or evaluating the robustness and reliability of DetectGPT. The paper also does not explain how the perturbation discrepancy correlates with the membership probability or the presence of training data in the generated texts. The paper does not account for the potential confounding factors or sources of bias in its experiments, such as the choice of prompts, sampling methods, hyperparameters, datasets, and evaluation metrics.\nThe paper does not discuss the ethical and social implications of its attack strategy. The paper proposes a novel form of TDE attack that can amplify the exposure of sensitive and private information from pre-trained LMs, but does not address the potential harms or risks that such an attack can pose to individuals, organizations, or society at large.\n\n", "questions": "\nIn Figure 1, perturbed LM generations are divided into two classes: \"good answer\" and \"bad answer,\" based on the value of d(x). Was the threshold for d(x) chosen empirically?\nIn Table 1 for Epoch 1, the three values with the lowest test accuracy are highlighted. In contrast, for Epoch 2, the highlighted values represent the top-3 highest test accuracy. There are no highlights in Epoch 0 and Epoch 3. Should the highlighting approach be consistent, or was this variation done intentionally for a specific reason?\n\n"}, {"summary": "Not applicable\n", "strengths": "This paper focuses on amplifying training data memorization in terms of the extraction attack performance. The goal is to put the target model in a state where it is more likely to regurgitate training data.\nMore specifically, the authors propose a fine-tuning method, using reinforcement learning, text generation and machine-text generation detection to condition the model such that it is likelier to regurgitate its training data. They do this with restricted whit-box access to the model, and no access to the training data. They attempt to achieve this by doing the following steps: 1) generating many samples from the model 2) using detectgpt to give scores on how likely each generation is to be human written, intuition being that human written text is more likely to have been pre-training data. 3) create pairs of training/generation data 4) training a reward model to distinguish between the generation and pseudo training data. 5) fine-tune the target model using the reward model. 6) taking samples from the new model and comparing tot the non-trained reference model.\nThe authors then test the performance of the proposed method by taking samples from the fine-tuned model and then measuring exact matches with training data and reporting the values. They compare these numbers to those of a non-fine tuned model. They also study the performance of the reward model separately.\n", "weaknesses": "\nThe approach/way of looking at the extraction problem is novel, prior work usually focuses on coming up with post-hoc extraction and not fine-tuning-based methods, where the decoding process is modified such that it incentivizes training data extraction. This paper however, tries to change the model so that its more likely to generate training data.\n\nThe problem is also an important problem, as current extraction methods are not very successful, most of them demonstrating low extraction rates.\n\n\n", "questions": "\nLack of enough experiments and ablations to support the main claim of the paper, that the method amplifies memorization. See questions 1-3 below. This is my main concern with the approach, as the model might as well just be regurgitating the same set of n-grams, over and over and as the reported is not measured over deduplicate generations based on n-grams (it seems like the only deduplication performed is wrt to full matched strings with training data), nor is there a diversity metric reported. Intuitively, I would assume that the fine-tuning is going to get the model to collapse on the set of generations used for RM training/FT. I also wonder why the authors did not use a metric similar to BLEU.\n\nThe structure of the paper is hard to follow and its not really well written. Some of the results are not explained well, also the way the deduplication is performed is not fully clear.  See questions 3 and 4 below.\n\nSection 5.2 only shows how well a reward model can learn to differentiate between machine generated and human written text. It does not provide any evidence to support the claims of the paper regarding training data. It is simply an ablation. I am not sure what it is included as one of the first results. The fact that the reward model can differentiate between different texts does not necessarily translate to it being better at incentivizing the target model to regurgitate training data.\n\n\n"}, {"summary": "\nCan the authors disentangle how much of the extractions overlap with the generated text that they fine-tuned with, and how much of the extracted text is non-overlapping and actually a generation of the model that is due to the amplification. Right now the main remaining question is does this method actually reinforce memorizations or is it just overfitting to the pseudo labeled data? (this corresponds to weakness 1 from above)\n\nHow many of the generated samples after fine-tuning differ from the reference model generations? as mentioned in question 1, if the model is collapsing, the new generations that the fine-tuned model has would overlap a lot with the generations from the refenrece model. it would be interesting to see if that is the case, or if there are any entirely new generations.\n\nOne main problem with the results is that the generation deduplication is happening on a full string level, and not n-gram overlaps. Same as point 1, I think there is probably huge overlap, what is the diversity of generations? There are no ablations here. We need a lot more ablations on the experiments. \n\nsection 5.2 please elaborate on the duplicate token overlaps, and the intervals. I went over the text multiple times but did not realize what the point of that experiment is.\n\n\n", "strengths": "The paper presents a novel attack strategy aimed at increasing the vulnerability of pre-trained language models to training data extraction attacks. By adversarially fine-tuning the LMs, the authors claim to amplify the exposure of sensitive pre-training data. They propose the use of pseudo-labels to help fine-tune the model in a way that favors text likely to have originated from the pre-training dataset. Their experiments suggest that this approach can lead to a significant increase in training data exposure, particularly in large models with over 1 billion parameters.\n", "weaknesses": "\nGenerally, the paper is well-written and easy to follow. \nThe paper introduces a unique and unexplored attack vector that goes beyond the traditional post-hoc data extraction methods.\nGiven the widespread use of large LLMs, the paper addresses a timely and significant issue of data privacy.\n\n", "questions": "\nThe paper assumes the availability of restricted white-box capabilities, which may not always be the case in real-world scenarios.\nAlthough the author provided extensive empirical study results, I'm still curious about the underlying mechanism behind the attack. Could the author elucidate how the proposed adversarial fine-tuning method effectively amplifies data exposure? It might be helpful to use a naive linear classification task as an illustrative example.\n\n"}]}, {"name": "FairPATE: Exposing the Pareto Frontier of Fairness, Privacy, Accuracy, and Coverage", "key_word": ["fairness", " privacy", " pate", " pareto frontier"], "sn": 9455, "reviews": [{"mark": [3, 4, 2], "rate": 5, "confidence": 4}, {"mark": [3, 3, 1], "rate": 3, "confidence": 3}, {"mark": [2, 3, 2], "rate": 3, "confidence": 4}], "abstract": "Deploying machine learning (ML) models often requires both fairness and privacy guarantees. In this work, we study the challenges of integrating group fairness interventions into the Private Aggregation of Teacher Ensemble (PATE) framework. We show that in the joint fairness-privacy setting, the placement of the fairness intervention before, or after PATE\u2019s noisy aggregation mechanism (which ensures its differential privacy guarantees) leads to excessive fairness violations, or inefficient privacy budgeting, respectively. With this in mind, we present FairPATE which adds a rejection mechanism due to fairness violations. Through careful adjustment of PATE\u2019s privacy accounting, we match the DP-SGD-based state-of-the-art privacy-fairness-accuracy trade-offs (Lowy et al., 2023) in demographic parity, and improve on them for equality of odds with 2% lower disparity at similar accuracy levels and privacy budgets. We also evaluate FairPATE in the setting where exact fairness guarantees need to be enforced by refusing to provide algorithmic decisions at inference-time (for instance, in a human-in-the-loop setting) thus trading off fairness with coverage. Based on our FairPATE, we provide, for the first time, empirical Pareto frontiers for fairness, privacy, accuracy, and coverage on a range of privacy and fairness benchmark datasets.\n", "detail": [{"summary": "The proposes a framework to integrate fairness into PATE. The proposed method is a simple adaptation of PATE which incorporates fairness constraints into the model's query rejection mechanism.\n", "strengths": "\nThe paper tackles a highly relevant issue in ML, addressing both theoretical and practical implications of fairness and privacy.\nThe proposed framework is a simple adaptation of the existing PATE. Simplicity is a plus in my book.\n\n", "weaknesses": "\nFairness is \"enforced\" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.\nA discussion on when to use the proposed framework in contrast to other frameworks is absent (see also my questions below).\nThe experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below).\n\n", "questions": "\nI can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP? \nHow does the framework work in case of some distribution shift? This is especially important in the context of my question above. \nFor the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible?\nFig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence.\nWhy Tran et and Jagielski et al. are not reported for the UTK-dataset experiment? \nPaper Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility discusses a similar topic (although the contributions from this work are different) and it could be added to your Related work section.\n\nMinor comments:\nA lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly.\n"}, {"summary": "The work considers the inclusion of fairness constraints into a method for differentially private\n(DP) training (or data generation from private data) based on transfer learning. The paper argues\nthat in this \"PATE\" approach which accounts of privacy concerns using DP, there is only one\nsensible place to incorporate fairness using an intervention (i.e. adjusting what/whether) data\nproceeds to subsequent PATE steps. This step is the point after the transfer from an ensemble of\nteachers is made. The paper puts a mechanism there that will reject some queries/instances if they\nresult in violations of fairness which is a function of all of the prior decisions of the\nmechanism. The work evaluates this approach relative to 2 other DP-based systems that incorporate\nfairness showing mostly preferable trade-offs between fairness, accuracy, and privacy; though this\nbenefit is small.\n", "strengths": "\nFairly well written and easy to follow.\n\nThe points of intervention discussions give a nice overview of the PATE approach and ways in\nwhich additional mechanisms can be independently injected. Note, however, the independent\nintervention assumption is a weakness below.\n\nRejection for fairness does give additional options for achieving fairness though this too comes\nwith a weakness below.\n\n\n", "weaknesses": "\nThe implications of rejecting for fairness are not considered. Rejection for privacy has\nimplications in terms of privacy budget and likewise rejections for fairness come with\nimplications and ignoring them might be responsible for the observed gains on the Pareto\nfrontier. Consider the noted rejection example:\n  \"If at inference-time a decision cannot be made without violating a pre-specified fairness\n   metric, then the model can refuse to answer, at which point that decision could be relegated\n   to a human judge\"\nThe important implication here is that there will still be a judgement; it is just that the model\nwill not be making it. Regardless of whether the result of the human judgement will produce fair\nor unfair overall statistics (that consider ultimate judgement whether by model or human), those\ndecisions need to be incorporated into subsequent fairness calculus. Even if a query is rejected\ndue to privacy, and if a decision is made for it subsequently, it would need to be accounted for\nin subsequent fairness decisions.\nSuggestion: incorporate ultimate decisions, whether by model or human, into the rejection\nmechanism; i.e. update counts m(z, k) based on human decisions. Given that humans might put the\ngroup counts into already violating territory, it may be necessary to rewrite Line 7 of Algorithm\n1 to check whether the fairness criterion is improving or not due to the decision and allow\nqueries that improve statistics even though those statistics already violate \u03b3 threshold.\nHandling rejection in experiments will also need to be done but unsure what the best approach\nthere would be. Perhaps a random human decision maker?\n\nIn arguments for intervention points, assumptions are made which preclude solutions. They assume\nthe intervention need to be made independent of other mechanisms in PATE. That is, they cannot\nconsider information internal to decision making that is not described by Figure 1 like\nindividual teacher outputs. This leaves the possibility that some fairness methods might be able\nto integrated with PATE in a closer manner than the options described. One example is that they\nmight include the teacher outputs instead of operating on the overall predicted class like\nAlgorithm 1 assumes presently. C3 in particular suggests that some interventions will not account\nfor privacy budget correctly due to special circumstances and suggests at Point 4, they can be\nbudgeting can be handled correctly. Nothing is stopping a design from refunding privacy budget if\na query is rejected subsequently to an intervention point.\nSuggestion: rephrase arguments for why some intervention points are bad to make sure they don't\nalso make assumptions about how the interventions are made and whether they can interact with\nprivacy budget.\n\nResults in the Pareto frontier show small improvements, no improvements, and in some cases worse\nresults than prior baselines.\nSuggestion: Include more experimental samples in the results to make sure the statistical\nvalidity of any improvement claims is good. This may require larger datasets. Related, the\nexperiments show error bars but how they are derived is not explained.\n\nComparisons against methods in which rejection due to fairness is not an option may not be fair.\nSuggestion: either integrate suggestion regarding accounting for rejection above, or incorporate\nsome form of rejection (or simulate it) in the existing methods being compared to. It may be that\nthe best methodology is not FairPATE but some existing baselines if adjusted to include fairness\nrejection option.\n\n\nSmaller things:\n\nRejection rate is not shown in any experiments. One could view a misclassification as a\nrejection, however. Please include rejection rates or view them as misclassifications in the\nresults.\n\nThe distribution whose fairness need to be protected is left to be guessed by the reader. For\nprivacy, it is more clear that it is the private data that is sensitive and thus privacy\nbudgeting is done when accessing that private data as opposed to the public data. For fairness,\nthe impact on individuals in the private dataset seems to be non-existent as the decisions for\nthem are never made, released, or implemented in some downstream outcome. I presume, then, it is\nthe fairness needs to be respected on the public data.\nAlgorithm 1 and several points throughout the work hint at this. However, there is also the\nconsideration of intervention points 1,2,3 which seem odd as they points seen before any\nindividual for whom fairness is considered is seen. That is, fairness about public individuals\ncannot be made there, independent of any other issues such as privacy budgeting. Further, Theorem\n1 discusses a demographic parity pre-processor which achieves demographic parity on private data\nwhich I presume is irrelevant.\n\nThe statement\n  \"PATE relies on unlabeled public data, which lacks the ground truth labels Y\"\nis a bit confusing unless one has already understood that fairness is with respect to public\ndata. PATE also relies on private labeled data to create the teachers.\n\nThe Privacy Analysis paragraph could be greatly simplified to just the last sentence regarding\npost-processing.\n\n\nSmallest things:\n\nDouble \"violations\" near \"violations of demographic disparity violations\".\n\nThe statement \"DP that only protects privacy of a given sensitive feature\" might be\nmischaracterizing DP. It is not focused on features or even data but rather the impact of\nindividuals on visible results.\n\n\n", "questions": "Question A: Is reasonable to ignore downstream decisions from queries rejected due to fairness\n  (i.e. contrary to my suggestion in the weaknesses above)?\nQuestion B: C1 makes a point that adding privacy after fairness may break fairness. What about in\n  expectation? Were one to view the demographic statistics defining fairness measures in\n  expectations, wouldn't they remain fair?\nQuestion C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter\n  degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point\n  of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE?\n  Unrelated, what is \"ordering defined over the input space X\" and why is it necessary?\n"}, {"summary": "The paper discusses the problem of interactions between fairness, privacy, and accuracy constraints for PATE (Private Aggregation of Teacher Ensembles) type algorithms for differentially private learning. PATE style of algorithms first uses the private data (partitioned into several small partitions) to train base classifiers (teachers). Then, the algorithm uses these teachers to label some public data in a privacy preserving way. In particular, it cleverly chooses which points it can label without sacrificing too much privacy. Then another classifier (student) is trained on this newly privately labelled dataset, which is then released to the user. In this paper, this labelling step is used to also incorporate privacy constraints. Finally, the paper uses empirical evidence to suggest that their algorithm achieves a better privacy fairness accuracy trade-off than Loewy et. al. 2023.\n", "strengths": "\nThe paper is written quite clearly and is easily readable. The arguments of the authors come out clearly without ambiguity and the reader can easily follow the train-of-thought. I appreciated that very much.\nI also found the main algorithmic idea of this paper quite nice. The idea of that the algorithm chooses which points to label not only on the bassi of the privacy constraint but also the fairness constraint is quite neat and could be useful in other contexts. I appreciated this.\n\n", "weaknesses": "Despite the interesting idea of the paper, I am unable to support this paper for acceptance. The four main reasons are as follows (in decreasing order of severity).\n\nW1 Significance: The paper aims to convey the significance of its contribution through experimental results (and I don't think there is anything wrong with it and in general support extensive empirical results), but the experiments present a rather bleak picture of the advantages of FairPATE. \n\nMinimal improvements Most importantly, there are rarely any results where the improvements of PATE over baselines is larger than 1%. For example in 1. Credit card dataset and 2. parkinson's dataset, there results differ by less than 1.\nSeveral examples of underperformance Several examples also show that FairPATE performs significantly worse than competitor. Examples include Demographic Parity for Adult dataset and UTK Face (\u03f5=5).\nMisleading regarding diversity of experiments In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on CheXpert, CelebA, FairFace, and Retired-Adults are not available in the main text and it is very hard to interpret the results presented for this in the appendix.  In fact, there are no results of FAIR-PATE on CheXpert in the paper \nPlease show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art.\n\n\n__ W2__ Wrong Conclusion from Theorem 2 I did not go through the detail of Theorem 1 but I assume it is correct and follows from Group privacy arguments. However, the sentence above Theorem 1 (Point C2) claims that the result proves that  \" pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism\". How ever this is not what the Theorem shows. The theorem only proves a privacy guarantee of M\u2299Ppre not that this is the tightest privacy guarantee possible for the composed mechanism. Am I missing something ?\n\nW3 Abstaining from prediction for fairness reasons The introduction justifies this as \"if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge\". If I have understood this correctly, this is a flawed argument. For example, consider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains. Nevertheless, the prediction is easy for this group and the judge nevertheless predicts on them correctly and the overall fairness is still violated. Isn't this pointless then to relegate this to the judge ? Intuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead.\n\nW4 Unfair Comparisons Fair-PATE  and Loewy et. al's algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn't and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.).\n\n\n", "questions": "\nMotivation I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ?\n\nMissing baselines Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison.\n\n**FairDP-SGD\" What is FairDP-SGD ? It doesn't seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE's result on CheXpert ?\n\nIn addition to this, please also also address W1,W2,W3,W4.\n\n\n"}]}, {"name": "Real-time computer vision on low-end boards via clustering motion vectors", "key_word": ["Coreset", " Motion vectors", " Segments", " Robotics", " Structure from motion", " non-convex optimization"], "sn": 9453, "reviews": [{"mark": [2, 1, 2], "rate": 3, "confidence": 3}, {"mark": [3, 2, 3], "rate": 6, "confidence": 4}, {"mark": [1, 1, 2], "rate": 1, "confidence": 4}, {"mark": [1, 1, 2], "rate": 3, "confidence": 4}], "abstract": "In this work, we suggest computer vision methods, specifically for video tracking and map creation from video.\nTo this end, we utilize motion vectors and clusters, which are computed very efficiently in standard video encoders, usually via dedicated hardware.\nWe suggest a provably good tracking algorithm for clustering these vectors, by considering them as segments.\nFor this, we utilize a definition of a \\emph{coreset} which is essentially a weighted set of points that approximates the fitting loss for every model, up to a multiplicative factor of 1\u00b1\u03b5.\nOur method supports M-estimators that are robust to outliers, convex shapes, lines, and hyper-planes.\nWe demonstrate the empirical contribution of our clustering method for video tracking and map creation from video, by running it on micro-computers (Le-Potato and Raspberry Pi) on synthetic and real-world videos with real-time running time.\n", "detail": [{"summary": "The paper focuses on a tracking algorithm that takes as input motion vectors obtained from standard video encoders.  The main contribution is the leveraging of the notion of coresets applied to segments, obtaining representative clusters and tracking. The bulk of the paper is on the extension of coresets for point sets to segments.  The tracking algorithm result is illustrated in two examples - big buck bunny video example, and 3D map estimation from drone video.\n", "strengths": "The main claim of the paper is in the generalization coreset ideas for points to segments and in the derivation of a tracking algorithm that is computationally efficient.  Certain claims are made about generalization of previous theoretical work (that I am not fully familiar with and cannot comment).\n", "weaknesses": "While I understand the rationale and setup of the problem for translating motion vector inputs as coresets and tracking,  the  results on the two datasets are not convincing.  While the paper talks a lot about how this approach is substantially better in comparison to neural-net based methods, it fails to refer to any of the classic methods in tracking where clustering, robust statistical methods are used.  The paper does refer to a review paper and states that there are over 1000 articles on the subject.  However, if the central aim of the paper is to demonstrate the advancement in tracking algorithms the paper should demonstrate the effectiveness of the algorithm designed by comparing it with at least one alternative (e.g. mean-shift based tracking ,  Comaniciu et al (CVPR 2000)).  I note that the mean-shift based tracker performed in real-time in low computational power settings for given candidate regions in a video over two decades ago.\n", "questions": "I have several questions that will help me identify what the central contributions are and on how the proposed method outperforms over other methods in the state of the art.\n\nIs your contribution mainly the extension of coreset idea to segments?   There has been work on coresets for sets of lines (e.g. Coreset for Line-Sets Clustering, Lotan et al, 2022).  Please elaborate on how your method is different.\nHave you compared your tracker with other methods in opencv and if so, what was the outcome? You refer to OpenCV in your paper and it is not clear from the paper how it was used in your experiments.\nCan you elaborate on the tradeoff between computational complexity of your technique and (epsilon, delta) choices during coreset construction?\n\n"}, {"summary": "This paper introduces innovative computer vision techniques that integrate classical machine learning strategies to enhance efficiency and robustness. The authors showcase the practical impact of their clustering method in video tracking and map creation from video, successfully executing it in real-time on micro-computers. The contributions of the paper encompass a novel clustering algorithm for motion vectors, a coreset-based approach that reduces the computational complexity of the clustering algorithm, and the implementation of the clustering algorithm on low-end boards, enabling real-time performance.\n", "strengths": "S1. The paper is well-written and most of the content is quite easy to follow. \nS2. The main contribution of this work is significantly interesting by incorporating traditional machine learning techniques in the age of deep learning. \nS3. The proposed vector clustering is theoretically sound, I tried my best to examine most of them and did not find obvious errors. \nS4. Overall, I have significant concerns regarding the experimental section of the paper. Firstly, the proposed method is only validated in three application scenarios, and the experimental results are not extensively reported or analyzed, neither in the main text nor in the supplementary material.\n", "weaknesses": "W1. The proofs in Section 2 are rather obscure and difficult for readers without relevant background knowledge to comprehend. Additionally, many crucial steps are relegated to the supplementary material, greatly impacting the readability of this paper.\nW2. This paper lacks an introduction and discussion of related works, making it challenging for readers unfamiliar with the field to fully understand the contributions of this article.\n", "questions": "Please check the weaknesses listed above.\n"}, {"summary": "The paper introduces a fully polynomial randomized approximation scheme for the clustering of motion vectors, which is then applied to the motion vectors produced by standard video encoders to the problem of visual tracking. The approximation scheme is an adaptation of the results of Feldman and Schulman (2012), which is concerned with robust clustering of points in arbitrary metric spaces, to segments as defined by the paper. Specifically, points on segments are sampled at uniform intervals, under a condition on the number of points \u03f5\u2032 such that the approximation in Feldman and Schulman (2012) is preserved.\n", "strengths": "Originality\nThe idea of directly using vector motions produced by video codecs as inputs to computer-vision tasks is interesting, as it is the broader approach of designing FPRAS for computer vision problems.\nQuality\nThe paper brings a broad review of the literature and is self-contained, including detailed proofs of its several lemmas and theorems.\nClarity\nEvery term is defined, and the illustration in Figure 2 helps the reader to understand the geometric meaning of the cost function defined in Equation 1.\nSignificance\nThe paper brings to the attention of the computer-vision community an important class of \"probably approximately correct\" algorithms, as in the title of Valiant's book.\n", "weaknesses": "General\nThe paper is not well organized. The first two sections of the Introduction, titled \"Video Tracking\" and \"Motion Vectors,\" do not describe the problem addressed by the paper. The subsection \"Our Approach\" does not describe the approach at all but introduces and illustrates the definition of a cost function which is discussed only much later in the paper. The subsection \"Coresets\" brings a definition of coreset, followed by an unusually long quote from the paper by Denisov et al. (2023). That section makes a reference to a \"segment clustering problem stated in Section 2.1\" that has a small coreset, but I was not able to parse the remaining of that paragraph. The references to Jubran et al. (2021) and Rosman et al. (2014) seem unnecessary, as they refer to exceptions (or so I understood) to the stated goal of having coresets which are weighted subsets of the inputs.\nAlgorithm 1 should be replaced for the simple formula that computes \u03f5\u2032. This value is then used to produce samples at uniform intervals on the motion vectors. It is not clear how the claimed novelty of Algorithm 1 generalizes, as stated, previous work by Rosman et al. (2014), which is concerned with fitting segments to points, rather than sampling from segments.\nThe structure of Algorithm 2 is not at all illustrated by Figure 3, as attempted. A key component of that algorithm (Feldman and Shulman CORESET algorithm (2012)) was replaced in Figure 3 with a different method (Bachem et al. (2018)) for \"easier implementation.\"\nEvaluation\nExperimental evaluation is insufficient. There is scant comparison, and no quantitative evaluation other than an unusual computation of frames-per-second (FPS). It is not valid to subtract all computing times but clustering from the pipeline, divide the number of frames by whatever remains and claim that as an FPS.\nThe role of Artuhr and Vassilvitskii (2007) in the empirical evaluations is unclear since the output of Algorithm 2 should be a clustering of the segments. One the other hand, there is no mention of Algorithm 2 in that section, only of Algorithm 1.\nThe steps of the video tracking method are unclear. The is no explanation for what \"Add for each motion vector its degree to (0, 10 and (1, 0)\". The is no discussion of how one moves on beyond k=2.\nThe jump from clustering of motion vectors to map creation leaves a gaping hole in the paper. The empirical evaluation of 3D map creation follows similar steps, which are repeated almost verbatim and should be omitted.\nThere are citations that are unusual to the computer-vision community: the OpenCV library, the Python 3 reference Manual, an Ubuntu Linux guide, the Rasberry Pi user guide, Vigdear manual, CutstomTkInter, and others.\nThe Conclusion section of the paper cannot be moved to an appendix.\n", "questions": "\nIt is curious that the number of samples on a segment does not depend on the length of the segment, according to Algorithm 1. Is there any intuition for why?\n\nStill in Algorithm 1, it is correctly stated that r is defined in Definition 2.3; however, given the comment on the second paragraph following the description of the algorithm (\"Note that r in Algorithms 1...\"), it should be provided as an input, since the function D to which r corresponds is not.\n\nI assume the word \"tracing,\" which appears twice in Section 3, is at typo, and \"tracking\" was meant instead?\n\nWas Algorithm 2 used at all? What is the purpose of using Arthur and Vassilvitskii (2007) if Algorithm 2 already produces a clustering? How is it possible for Arthur and Vassilvitskii (2007) algorithm to have been implemented in Bradski (2000)?\n\nHow is a motion-vector clustering algorithm applied to map creation? Why computations on Raspberry-Pi and utilization of gyroscope contribute to \"fair comparison\"?\n\n\n"}, {"summary": "The paper proposes a clustering approach based on the idea of coresets. It is demonstrated in the paper that the proposed formulation helps perform tracking and 3D map creation from videos in real time. Few experimental results are shown to demonstrate the claims made in the paper.\n", "strengths": "\nReal-time solution to a couple of popular computer vision problems.\n\n", "weaknesses": "\nNot a well-written paper. So many typos and grammatical mistakes.\nThe paper widely discusses the existing literature in theory and emphasizes less of the actual contributions of the paper other than making few methods real-time.\nThe results are poorly demonstrated. I am unable to conclude how good of a map is obtained using the proposed method.\nAlso confusion about video tracing and tracking \u2014see Sec. 3.\n\nRefer Questions section for more comments.\n", "questions": "Abstract:\n\nTo this end, we utilize motion vectors and clusters. What clusters authors are referring to. I believe it should be clustering algorithms/methods.\n\nwith real-time running time -> that gives real-time performance.\n\n\nIntroduction\n\nA meta-survey on such approaches Zou et al. (2019) states that in recent years -> kindly use \\citep{} to put parentheses for citation or rewrite this line.\n\n\u201cfool\" -> use `` and \u2019\u2019 for the apt quotes.\n\nFigure 1 -> the blue motion vector is hardly visible. Furthermore, kindly use a different color for the blue motion vector as it correlates with the flower in the background. \n\nFigure 2 -> figures are placed side to side, whereas captions suggest top and bottom. Kindly correct.\n\nThere are many grammatical mistakes in the paper. Kindly improve the writing of the paper.\n\n\nGeneral Comment:\nWith all due respect, tracking and map creation is not computer vision. These are a couple of  problems studied in computer vision. Kindly modify your paper title.\n"}]}]