name,key_word,sn,reviews
Modelling Microbial Communities with Graph Neural Networks,"['graph neural networks', ' microbial communities', ' microbiology', ' genomes']",9504,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 1], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}]"
TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023,"['tabular', ' tabular data', ' architecture', ' deep learning', ' neural networks']",9502,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [4, 4, 2], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}]"
Neural Evolutionary Kernel Method: A Knowledge-Based Learning Architechture for Evolutionary PDEs,"['Numerical PDE', ' structure preserving neural network', ' operator learning', ' boundary integral']",9498,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}]"
PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs,"['PINN', ' machine learning', ' physics-informed machine learning']",9493,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 6, 'confidence': 3}, {'mark': [2, 4, 1], 'rate': 1, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
SaNN: Simple Yet Powerful Simplicial-aware Neural Networks,"['Graph Neural Networks', ' Higher-order Representation Learning', ' Simplicial Complexes', ' Simplicial Neural Networks', ' Weisfeiler-Lehman Isomorphism Test']",9491,"[{'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 2}, {'mark': [4, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural Architecture Search,"['Neural Architecture Search', ' Performance Predictor', ' Graph Neural Network']",9483,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models,['Large Language Models; Prompt Engineering; Boosting Mechanism;'],9482,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
Farzi Data: Autoregressive Data Distillation,"['Data Distillation', ' Meta Learning', ' Recommender Systems', ' Language Modeling']",9481,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 2}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}]"
BATTLE: Towards Behavior-oriented Adversarial Attacks against Deep Reinforcement Learning,"['deep reinforcement learning', ' preference-based reinforcement learning', ' adversarial reinforcement learning']",9480,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}]"
Understanding Large Language Models Through the Lens of Dataset Generation,"['Large Language Model', ' dataset generation']",9477,"[{'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}]"
Temporal Parallelization for GPU Acceleration of Spiking Neural Networks,"['Spiking neural networks', ' High-performance computing', ' GPU acceleration']",9476,"[{'mark': [1, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 5}]"
FSN: Feature Shift Network for Load-Domain Domain Generalization,"['Fault diagnosis', ' Deep learning', ' CNN', ' Domain Generalization', ' Load-domain Domain Generalization']",9472,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 4, 2], 'rate': 6, 'confidence': 4}]"
"Ask Again, Then Fail: Large Language Modelsâ€™ Vacillations in Judgement","['Large Language Models', ' Uncertainty', ' Evaluation', ' In-Context Learning', ' Alignment', ' Multi-round dialogue', ' Robustness']",9468,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
Do Current Large Language Models Master Adequate Clinical Knowledge?,"['Large Language Model', ' Medical Large Language Model', ' Clinical Knowledge', ' Knowledge Graph']",9466,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 2}]"
SIMULTANEOUS GENERATION AND IMPROVEMENT: A UNIFIED RL PARADIGM FOR FJSP OPTIMIZATION,"['Reinforcement Learning', ' Flexible Job Shop Schedule Problem', ' FJSP']",9463,"[{'mark': [1, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 4}]"
Model-Agnostic Shift-Equivariant Downsampling,"['Shift equivariance', ' Shift invariance', ' Downsampling', ' Convolutional neural networks']",9459,"[{'mark': [4, 2, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 4}]"
Visuo-emotional perception and Human Cognition to engineer content-generation using Generative AI,"['creative content', ' digital creatives', ' attention', ' personalization', ' content optimization', ' content generation', ' generative AI']",9458,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [1, 3, 2], 'rate': 1, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 3}]"
Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships,"['large language model', ' training data extraction', ' fine-tuning', ' pseudo-labeling with membership', ' privacy']",9456,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}]"
"FairPATE: Exposing the Pareto Frontier of Fairness, Privacy, Accuracy, and Coverage","['fairness', ' privacy', ' pate', ' pareto frontier']",9455,"[{'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Real-time computer vision on low-end boards via clustering motion vectors,"['Coreset', ' Motion vectors', ' Segments', ' Robotics', ' Structure from motion', ' non-convex optimization']",9453,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [1, 1, 2], 'rate': 1, 'confidence': 4}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 4}]"
Beyond Memorization: Violating Privacy via Inference with Large Language Models,"['Privacy', ' Large Language Models']",9451,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}]"
AdaLomo: Low-memory Optimization with Adaptive Learning Rate,"['Memory-efficient', ' Optimization', ' Large language models']",9450,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}]"
Deep Network Partition Density Exhibits Double Descent,"['Double Descent', ' Partition Density', ' Linear Regions', ' Local Complexity']",9446,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Locality Sensitive Sparse Encoding for Learning World Models Online,"['model-based rl', ' online learning', ' incremental learning', ' catastrophic forgetting']",9441,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}]"
Towards Relaxing the Unbiasedness Condition of Doubly Robust Estimators for Debiased Recommendation,"['Recommender system', ' Selection bias', ' Doubly robust']",9439,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Instruct2Act: Mapping Multi-modality Instructions to Robotic Arm Actions with Large Language Model,"['large language models', ' robotic manipulation', ' code generation']",9438,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Beyond Disentanglement: On the Orthogonality of Learned Representations,"['Disentanglement', ' Orthogonality', ' Unsupervised Learning', ' Representation Learning', ' DCI', ' DCI-ES']",9436,"[{'mark': [3, 1, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}]"
Directional Rank Reduction for Backdoor Defense,"['backdoor defense', ' backdoor attack', ' neuron pruning']",9434,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Regulation Games for Trustworthy Machine Learning,"['privacy', ' fairness', ' regulation', ' game']",9431,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}]"
3D Molecular Pretraining via Localized Geometric Generation,['molecular repsentation; self-supervised learning'],9430,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}]"
CLIP Exhibits Improved Compositional Generalization Through Representation Disentanglement,"['Compositional generalization', ' Out-of-distribution generalization', ' Vision-language models', ' CLIP', ' Disentangled representations', ' Language supervision', ' data-centric AI']",9428,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Disentanglement Learning via Topology,"['representation learning', ' variational autoencoders', ' disentangled representations', ' topological data analysis']",9426,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}]"
Object-Centric Noise Filtering in Neural Radiance Fields via Influence Functions and Segmentation,"['NeRF', ' robust learning']",9421,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Unified Language Model Alignment with Demonstration and Point-wise Human Preference,"['Large Language Model', ' Alignment', ' Point-wise preference']",9420,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 4], 'rate': 6, 'confidence': 3}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 3}]"
DisFormer: Disentangled Object Representations for Learning Visual Dynamics Via Transformers,"['Unsupervised Visual dynamics prediction', ' object centric representation', ' disentangled representation']",9419,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}]"
Why are hyperbolic neural networks effective? A study on hierarchical representation capability,"['Hyperbolic space', ' hyperbolic neural networks', ' hierarchical structure']",9418,"[{'mark': [3, 3, 3], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}]"
Large Language Models as superpositions of cultural perspectives,"['Large Language Models', ' context-dependence', ' controllability', ' cultural values', ' personal values', ' personality traits', ' societal considerations', ' Shalom H Schwartz', ' Geert Hofstede', ' Big Five']",9417,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}]"
InstaTAP: Instance Motion Estimation for Tracking Any Point,"['Video Point Tracking', ' Point Tracking', ' Tracking', ' Spatial-Temporal Vision', ' Segment Anything Model']",9415,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}]"
An Enhanced Gromov-Wasserstein Barycenter Method for Graph-based Clustering,"['Gromov-Wasserstein Learning', ' Graph-based Clustering', ' Non-convex Optimization']",9414,"[{'mark': [4, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}]"
Physics-informed neural networks for transformed geometries and manifolds,"['physics-informed', ' neural networks', ' transformation', ' manifold', ' diffeomorphism', ' parametrized', ' geometry', ' reference domain', ' free boundary', ' shape optimization']",9410,"[{'mark': [2, 2, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}]"
Enhancing Neural Subset Selection: Integrating Background Information into Set Representations,"['Neural Set Function', ' Hierarchical Structure', ' Invariance', ' Subset Selection']",9406,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Causal Impact Index: A Causal Formulation of Citations,"['High-dimensional causal inference', ' text analysis', ' matching', ' citation analysis', ' science of science']",9405,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Node-CwR: Node Classification with Reject Option,"['Node classification', ' graph attention networks', ' reject option', ' label noise', ' label smoothing', ' robust learning']",9404,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Segment Anything Meets Universal Adversarial Perturbation,"['Universal Adversarial Perturbation', ' Adversarial Robustness', ' Segment Anything']",9403,"[{'mark': [2, 2, 2], 'rate': 1, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 4}]"
Episode Transformer: Model-based Episodic Reinforcement Learning,"['Episodic RL', ' Model-based RL', ' Movement Primitives']",9399,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}]"
Everyone Deserves A Reward: Learning Customized Human Preferences,"['Human Preference Alignment', ' Large Language Model', ' Data Efficiency']",9397,"[{'mark': [4, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 2}]"
Bridging Vision and Language Spaces with Assignment Prediction,"['Multimodal learning', ' vision-language tasks', ' frozen LLMs', ' optimal transport', ' assignment prediction']",9396,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 6, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Fair Domain Generalization with Arbitrary Sensitive Attributes,"['domain generalization', ' fairness', ' multiple sensitive attributes']",9395,"[{'mark': [1, 2, 2], 'rate': 1, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Metanetwork: A novel approach to interpreting ANNs,"['AI interpretability', ' Model representation', ' Model capability', ' Autoencoder', ' Meta learning']",9394,"[{'mark': [1, 1, 2], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 3}]"
Generative Judge for Evaluating Alignment,"['Generative', ' Evaluation', ' Alignment']",9392,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}]"
Rethinking and Extending the Probabilistic Inference Capacity of GNNs,"['graph neural networks', ' expressiveness', ' approximate inference']",9389,"[{'mark': [3, 2, 3], 'rate': 8, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
Recurrent Distance-Encoding Neural Networks for Graph Representation Learning,"['Recurrent Neural Networks', ' Graph Neural Networks']",9385,"[{'mark': [4, 3, 4], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Learning model uncertainty as variance-minimizing instance weights,"['loss reweighting', ' epistemic uncertainty', ' bi-level optimization', ' model calibration', ' bayesian neural networks']",9383,"[{'mark': [4, 2, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}]"
TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale,"['Video Representation Learning', ' Out-of-the-box Video Representation', ' Scalable Video Pre-training']",9381,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
Controlled Text Generation via Language Model Arithmetic,"['Controlled text generation', ' LLM', ' Natural Language Processing']",9377,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 3}, {'mark': [2, 4, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
Tracking Cognitive Development of Large Language Models,"['Cognitive ability', ' benchmark', ' Large Language Models', "" Piaget's theory of cognitive development""]",9374,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 5}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 5}]"
Black-box Targeted Adversarial Attack on Segment Anything (SAM),"['Black-box attack', ' adversarial robustness', ' segment anything']",9373,"[{'mark': [2, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 1, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization,"['Forward-only learning', ' Biologically inspired learning', ' Artificial neural networks', ' Analytical characterization']",9371,"[{'mark': [4, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}]"
Do not Start with Trembling Hands: Improving Multi-agent Reinforcement Learning with Stable Prefix Policy,"['MARL', ' Trembling Hands', ' Exploration', ' Exploration', ' Prefix Policy']",9370,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}]"
Chain-of-Symbol Prompting for Spatial Relationships in Large Language Models,"['Large Language Models', ' Prompting', ' Spatial Planning', ' Reasoning']",9367,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}]"
Lightweight Graph Neural Network Search with Graph Sparsification,"['graph neural network', ' neural architecture search', ' graph sparsification']",9366,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}]"
RLP: A reinforcement learning benchmark for neural algorithmic reasoning,"['reinforcement learning', ' benchmark', ' algorithmic reasoning', ' logic puzzles']",9365,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Vision Transformer with Irregular Attention,"['Deep Learning', ' DNN', ' Transformer', ' ViT', ' DeiT', ' Tensor Decomposition', ' Tensor Network', ' BTD', ' BTD-LL1', ' CPD', ' DNN Compression', ' DNN Acceleration']",9363,"[{'mark': [3, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}]"
Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning,"['hamiltonian dynamics', ' cross domain generalization', ' learning physics', ' meta learning']",9361,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
UBERT: Unsupervised adaptive early exits in BERT,"['Early exits', ' Deep Neural Networks', ' BERT']",9356,"[{'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 1}, {'mark': [1, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
PEACH: Pretrained-embedding Explanation Across Contextual and Hierarchical Structure,"['Interpretability', ' Text classification', ' Global interpretation', ' Local interpretation']",9354,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
"ReLiK: Retrieve, Read and LinK: Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget","['Information Extraction', ' Entity Linking', ' Relation Extraction', ' Natural Language Processing', ' NLP']",9353,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}]"
What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning,"['data selection', ' instruction tuning', ' large language models']",9349,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}]"
Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks,"['Dynamic Graph', ' Graph Explanation', ' Graph Neural Network', ' Causal Inference']",9348,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Dissecting learning and forgetting in language model finetuning,"['language models', ' domain adaptation', ' catastrophic forgetting']",9346,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
MetaFormer with Holistic Attention Modelling Improves Few-Shot Classification,"['Meta-Learning', ' Vision Transformers']",9345,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
TinyTrain: Deep Neural Network Training at the Extreme Edge,"['Tiny Machine Learning', ' On-device Training', ' Personalisation', ' Edge Computing', ' Microcontrollers']",9344,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 8, 'confidence': 4}]"
Test-time Adaption against Multi-modal Reliability Bias,"['Test-time adaption', ' Imbalanced multi-modal learning']",9339,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 6, 'confidence': 5}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 5}]"
Fully Identical Initialization,"['Initialization', ' Idetity Matrix', ' Dynamic Isometry']",9336,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 4], 'rate': 6, 'confidence': 4}]"
LatentCBF: A Control Barrier Function in Latent Space for Safe Control,"['Representation Learning', ' Reinforcement Learning', ' Optimal Control', ' End-to-End Learning', ' Convex Optimization', ' Control Barrier Function', ' Autonomous Driving', ' CARLA', ' Robotics']",9335,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Mirage: Model-agnostic Graph Distillation for Graph Classification,"['graph distillation', ' graph classification', ' frequent pattern mining']",9334,"[{'mark': [2, 3, 1], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels,"['Model stealing', ' Adversarial images', ' Timing side-channel', ' Transfer Learning']",9333,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 1, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}]"
A Weakly Supervised and Globally Explainable Learning Framework for Brain Tumor Segmentation,"['brain tumor segmentation', ' weakly supervised learning', ' explainable learning', ' counterfactual generation', ' class association embedding', ' topological data analysis']",9332,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Exploring mechanisms of Neural Robustness: probing the bridge between geometry and spectrum,"['Latent Geometry', ' Latent Spectrum', ' Adversarial Robustness', ' Mechanistic Model', ' Unsupervised Learning', ' Local Learning', ' Jacobian Regularization', ' Spectral Regularization']",9331,"[{'mark': [3, 1, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}]"
Two-shot learning of continuous interpolation using a conceptor-aided recurrent autoencoder,"['Conceptors', ' Few Shot Learning', ' Recurrent Neural Networks', ' BPTT', ' Motion Modelling', ' Low Dimensional Dynamics']",9329,"[{'mark': [2, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 1}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
On the Learnability of Watermarks for Language Models,"['watermarking', ' large language models', ' distillation']",9328,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}]"
Personalized Language Generation via Bayesian Metric Augmented Retrieval,"['Retrieval Augmented Generation', ' Bayesian Metric Learning']",9327,"[{'mark': [3, 4, 3], 'rate': 8, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}]"
Prompt Sketching for Large Language Models,"['large language models', ' prompting', ' decoding']",9320,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision,"['Graph anomaly detection', ' consistency training', ' learnable data augmentation']",9315,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
AN ENTROPY PERSPECTIVE IN KNOWLEDGE DISTILLATION,['Knowledge Distillation'],9314,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
OPTIMIZING STABILIZATION IN SINGULARLY PER- TURBED PROBLEMS WITH SUPG SCHEME,"['Convolutional Neural Network', ' Singularly Perturbed PDEs', ' Stabilization Scheme']",9313,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 3, 1], 'rate': 3, 'confidence': 5}]"
Bellman Optimal Step-size Straightening of Flow-Matching Models,"['flow matching', ' generative model', ' efficient sampling', ' distillation', ' responsible ML']",9312,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}]"
Distributional Structured Pruning by Lower bounding the Total Variation Distance using Witness functions,"['Pruning', ' Structured Pruning', ' Total Variation Distance']",9311,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
ZGS-Based Event-Driven Algorithms for Bayesian Optimization in Fully Distributed Multi-Agent Systems,"['distributed machine learning', ' Bayesian optimization', ' multi-agent systems', ' zero-gradient-sum optimization', ' event-driven mechanism']",9310,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [1, 1, 2], 'rate': 1, 'confidence': 3}]"
Boosted Long Short-Term Memory with Additional Inner Layers,"['Recurrent Neural Networks', ' Long Short-Term Memory', ' Sequence classification', ' Boosted architectures']",9308,"[{'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 2}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 5}]"
An Optimization-Based Framework for Adversarial Defence of Graph Neural Networks Via Adaptive Lipschitz Regularization,"['Graph Neural Networks', ' Robustness', ' Lipschitz Regularization']",9303,"[{'mark': [1, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}]"
Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning,"['Large Language Models', ' Complex Instructions', ' Reinforcement Learning']",9302,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 3}]"
Diffusion with Synthetic Features: Feature Imputation for Graphs with Partially Observed Features,"['Graph neural networks', ' Missing features']",9301,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [1, 2, 1], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}]"
LST-Bench:A Benchmark for long sequence time-series forecasting Task,"['Time Series', ' Deep Learning', ' Neural Networks', ' Data Mining']",9299,"[{'mark': [1, 2, 1], 'rate': 1, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}]"
Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction,"['Extreme Classification', ' Extreme Multi-Label Learning']",9297,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 2}]"
Time2Image: A Unified Image Representation Framework for Time Series Classification,['Time series classification; Time series image representation; Adaptive time series gaussian mapping; Vision Transformer'],9293,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 5, 'confidence': 4}]"
FruitBin: A tunable large-scale dataset for advancing 6D Pose estimation in fruit bin picking automation,"['Datasets and Benchmarks', ' 6D Pose estimation', ' Robotic', ' Bin Picking', ' Occlusion']",9289,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}]"
Uncovering the Spectrum of Graph Generative Models: From One-Shot to Sequential,"['Graph generation', ' One-shot generation', ' Autoregressive generation', ' Unified framework', ' Diffusion Model', ' Molecule generation']",9287,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Universal Algorithm for Extreme Bandits with the Minimal Complexities,"['extreme bandits', ' online optimization', ' heavy-tails', ' non-iid data', ' non-parametric']",9286,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 3}, {'mark': [3, 1, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Summing Up the Facts: Additive Mechanisms behind Factual Recall in LLMs,"['Mechanistic Interpretability', ' Interpretability', ' Fact', ' Factual Recall', ' LLM', ' Explainability', ' Transparency']",9284,"[{'mark': [2, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Task-Oriented Multi-View Representation Learning,['Multi-view learning; Meta learning; Feature modulation; Task adaptation'],9283,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Understanding the Mechanics and Dynamics of Memorisation in Large Language Models: A Case Study with Random Strings,"['language models', ' memorization']",9281,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}]"
NeFL: Nested Federated Learning for Heterogeneous Clients,"['federated learning', ' system heterogeneity']",9278,"[{'mark': [4, 3, 4], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching,"['Mechanistic Interpretability', ' Natural Language Processing', ' Large Language Models']",9274,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 4, 4], 'rate': 8, 'confidence': 2}]"
Fixed-Budget Best Arm Identification with Variance-Dependent Regret Bounds,['Best arm identification'],9273,"[{'mark': [3, 1, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems,"['mean-field Langevin dynamics', ' minimax optimization', ' zero-sum games', ' Markov games']",9271,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 4], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 2}]"
Causal Inference Using LLM-Guided Discovery,"['Causal Inference', ' Large Language Models', ' Causal Discovery', ' Causal Order']",9268,"[{'mark': [1, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [4, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Learning Predictive Checklists with Probabilistic Logic Programming,"['Predictive Checklists', ' Interpretability', ' Fairness', ' Probabilistic Logic Programming']",9267,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}]"
Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions,"['large language model', ' instruction tuning', ' multi-turn conversation']",9266,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}]"
Knowledge Graph Completion by Intermediate Variables Regularization,"['Knowledge Graph Completion', ' Tensor Decomposition', ' Regularization']",9265,"[{'mark': [3, 4, 1], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 8, 'confidence': 4}]"
Attributed Graph Clustering via Coarsening with Modularity,"['Graph Clustering', ' Graph Neural Networks', ' Convex Optimization', ' Non-Convex Optimization', ' Graph Coarsening']",9264,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [1, 3, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}]"
MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Deep Neural Networks,"['shift detection', ' dimensionality reduction', ' neural networks', ' activation graphs']",9263,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Prototypes-Injected Prompt for Federated Class Incremental Learning,"['federated class incremental learning', ' federated learning', ' class incremental learning', ' continual learning', ' prompt', ' prototype']",9261,"[{'mark': [3, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 1}]"
Demonstration-Regularized RL,"['reinforcement learning', ' regularization in reinforcement leaning', ' learning with demonstrations', ' reinforcemenet learning with human feedback']",9260,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}]"
Explorative Latent Self-Supervised Active Search Algorithm (ELSA),"['Computer Vision', ' Active Learning', ' Interactive Labeling', ' Self-Supervised Learning']",9259,"[{'mark': [2, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 1, 1], 'rate': 1, 'confidence': 4}]"
Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models,"['generative models', ' diffusion models', ' score-based models', ' image generation', ' image editing']",9257,"[{'mark': [3, 4, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 4}]"
"GatedMTL: Learning to Share, Specialize, and Prune Representations for Multi-task Learning","['Multi-task learning', ' Gated networks', ' Sharing', ' Pruning', ' Sparsity', ' MTL']",9256,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
PDE-Diffusion: Physic guided diffusion model for solving partial derivative equations,"['AI for science', ' PDE', ' diffusion model', ' generative model']",9255,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [1, 2, 2], 'rate': 1, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}]"
BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving,"['Autonomous Driving', ' BEV', ' Retrieval', ' Multi-modal', ' LLM', ' prompt learning']",9254,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Differentiable Optimization in Plane-Wave Density Functional Theory for Solid States,"['AI for Science', ' Quantum Chemisty', ' Density Functional Theory', ' Deep Learning', ' Kohn-Sham Equation', ' Solid-State Physics']",9253,"[{'mark': [3, 3, 3], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Multilingual Jailbreak Challenges in Large Language Models,"['multilingual', ' safety', ' large language models']",9250,"[{'mark': [2, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}]"
Instruction-tuned LLMs with World Knowledge are More Aligned to the Human Brain,"['large language models', ' instruction-tuning', ' world knowledge', ' neuroscience', ' neuroAI']",9249,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Brain-inspired Geometry Constrain on Represention for Compositional Generalization,"['Represention Learning', ' Compositional Generalization.']",9248,"[{'mark': [3, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 1, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 1], 'rate': 1, 'confidence': 3}]"
Categorical Features of entities in Recommendation Systems Using Graph Neural Networks,"['Graph Neural Networks', ' Representation learning', ' recommender engines', ' Hyper edges']",9246,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}]"
Generalized Policy Iteration using Tensor Approximation for Hybrid Control,"['Optimal Control', ' Hybrid Actions', ' Robotics', ' Approximate Dynamic Programming', ' Tensor Approximation']",9245,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}]"
t3-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence,"['Variational autoencoder', ' Information geometry', ' Heavy-tail learning', ' Generative model']",9244,"[{'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
Neuron to Graph: Interpreting Language Model Neurons at Scale,"['Mechanistic Interpretability', ' Visualisation']",9243,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}]"
Residual Factorized Fourier Neural Operator for simulation of three-dimensional turbulence,"['factorized fourier neural operator', ' fourier neural operator', ' navier stokes', ' three-dimensional turbulence prediction']",9242,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Adjustable Quantile-Guided Diffusion Policy for Diverse Behavior Generation in Offline RL,"['offline reinforcement learning', ' diffusion']",9241,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 5, 'confidence': 4}]"
Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging,"['snapshot compressive imaging', ' hyperpectral imaging', ' prompt learning', ' federated learning']",9239,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}]"
Prompt Optimization via Adversarial In-Context Learning,"['Prompt Optimization', ' Adversarial Learning', ' In-Context Learning', ' Large Language Model']",9238,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
FEATHER: Lifelong Test-Time Adaptation with Lightweight Adapters,"['test-time adaptation', ' source free test-time domain adaptation', ' parameter efficient test-time adaptation']",9237,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 1], 'rate': 3, 'confidence': 4}]"
FLAT-Chat: A Word Recovery Attack on Federated Language Model Training,"['Label inference attack', ' Large-scale language model', ' Matrix flattening']",9236,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 5}]"
Chain-of-Thought Predictive Control,"['Hierarchical Imitation Learning', ' Robotic Manipulation']",9232,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Adapting ConvNets for New Cameras without Retraining,"['Convolutional Networks', ' Pretrained', ' Wide FOV', ' Fisheye', ' Segmentation', ' Rectification']",9231,"[{'mark': [2, 2, 2], 'rate': 1, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}]"
Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability,"['Graph neural networks', ' Graph canonization', ' Stability']",9229,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
Relating Implicit Bias and Adversarial Attacks through Intrinsic Dimension,"['Implicit Bias', ' Adversarial Attacks', ' Intrinsic Dimension', ' Neural Networks', ' Fourier Transform']",9228,"[{'mark': [2, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}]"
Rethinking Actor-Critic: Successive Actors for Critic Maximization,"['Off-policy reinforcement learning', ' actor-critic methods', ' TD3', ' discrete action spaces', ' continuous action spaces']",9227,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Gradual Optimization Learning for Conformational Energy Minimization,"['energy minimization', ' conformational optimization']",9224,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 4, 3], 'rate': 6, 'confidence': 4}]"
AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection,"['Anomaly detection', ' Zero-shot anomaly detection', ' CLIP', ' Industrial defect inspection']",9222,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 3}]"
Chat Vector: A Simple Approach to Equip LLMs With New Language Chat Capabilities,"['RLHF', ' LLM']",9220,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Generative Models are Self-Watermarked: Intellectual Property Declaration through Re-Generation,"['Watermark', ' Generative Model', ' Re-generation', ' Fixed-point Theory']",9219,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Nonlinear Inference Learning for Differentially Private Massive Data,"['Differential privacy', ' Nonlinear Inference', ' Massive Data', ' Bag of Little Bootstraps']",9218,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 3, 'confidence': 5}, {'mark': [2, 1, 1], 'rate': 1, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}]"
Boosting Selective Rationalization with Shortcuts Discovery,"['Selective Rationalization', ' Shortcut']",9216,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}]"
RL4CO: a Unified Reinforcement Learning for Combinatorial Optimization Library,"['Reinforcement Learning', ' Neural Combinatorial Optimization', ' Combinatorial Optimization', ' Library', ' Benchmark']",9212,"[{'mark': [3, 3, 3], 'rate': 3, 'confidence': 3}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
IKL: Boosting Long-Tail Recognition with Implicit Knowledge Learning,['Long-tail Recognition'],9210,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}]"
GPT Can Solve Mathematical Problems Without a Calculator,['Large Language Model; Mathematical Reasoning; Arithmetic Tasks; Math Word Problem'],9208,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
S4G: Breaking the Bottleneck on Graphs with Structured State Spaces,"['GNN', ' over-squashing', ' state-space models']",9207,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects,"['pretraining', ' robotics', ' manipulation', ' object representation', ' representation learning']",9206,"[{'mark': [4, 4, 4], 'rate': 1, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 4}]"
An Intuitive Multi-Frequency Feature Representation for SO(3)-Equivariant Networks,"['Equivariant networks', ' SO(3) Equivariance', ' Fourier features']",9205,"[{'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}]"
CodeComplex: A Time-complexity Dataset for Multi-language Source Codes,"['Code complexity', ' Dataset', ' Neural network']",9201,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Implicit Regularisation in Overparametrized Networks: A Multiscale Analysis of the Fokker-Planck equation,"['overparametrized networks', ' optimisation', ' implicit regularization', ' multiscale', ' fokker-planck equation']",9200,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 3], 'rate': 3, 'confidence': 3}]"
KoLA: Carefully Benchmarking World Knowledge of Large Language Models,"['Large Language Model', ' World Knowledge', ' Evolving Benchmark']",9199,"[{'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}]"
Screening Unlearnable Examples via Iterative Self Regression,"['data poisoning attack', ' iterative self regression', ' availability attacks detection', ' unlearnable examples']",9197,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 1], 'rate': 1, 'confidence': 5}]"
Graph Parsing Networks,"['GNN', ' graph pooling', ' parsing']",9196,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}]"
Retrieving Texts by Abstract Descriptions,"['similarity', ' descriptions', ' LMs', ' retrieval']",9195,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}]"
TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts,"['Traffic Predictoin', ' Deep Learning', ' Spatio-Temporal data modeling']",9189,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}]"
Improving SAM Requires Rethinking its Optimization Formulation,"['Sharpness aware minimization', ' generalization', ' supervised learning', ' optimization', ' bilevel optimization']",9188,"[{'mark': [4, 4, 4], 'rate': 1, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
RedMotion: Motion Prediction via Redundancy Reduction,"['Motion prediction', ' self-supervised learning', ' trajectory forecasting', ' self-driving']",9187,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 5}]"
TILDE-Q: A Transformation Invariant Loss Function for Time-Series Forecasting,"['Time-Series Forecasting', ' Deep Learning', ' Loss functions', ' Time-series Similarity']",9186,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Learning From Simplicial Data Based on Random Walks and 1D Convolutions,"['simplicial complex', ' simplicial neural network', ' random walks']",9185,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 5}]"
Adaptive Environmental Modeling for Task-Oriented Language Agents,"['Large Language Model', ' Environmental Adaptation', ' Agents', ' Interactive Decision Making']",9182,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition,"['hyperspectral imaging', ' optical modulation', ' real-time detection', ' vision transformer', ' pre-acquisition modulation', ' learnable mask', ' weight binarization']",9181,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}]"
Sensitivity-Aware Differentially Private Decentralized Learning with Adaptive Noise,"['decentralized learning', ' differential privacy', ' adaptive noise', ' time-varying topology']",9180,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 3, 1], 'rate': 3, 'confidence': 4}]"
EvIL: Evolution Strategies for Generalisable Imitation Learning,"['Reinforcement Learning', ' Inverse Reinforcement Learning', ' Imitation Learning', ' Evolutionary Strategies']",9179,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 5}]"
RILe: Reinforced Imitation Learning,"['Reinforcement Learning', ' Imitation Learning', ' Deep Reinforcement Learning']",9178,"[{'mark': [1, 1, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}]"
Few Heads are Enough,"['transformers', ' attention', ' moe', ' mixture of experts', ' efficient transformers', ' language modelling']",9177,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 3}]"
LMExplainer: A Knowledge-Enhanced Explainer for Language Models,"['Explainability', ' XAI', ' Language Model']",9174,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Optimal spherical codes for locality-sensitive hashing,"['Optimal spherical codes', ' locality sensitive hashing', ' similarity search', ' sparse coding']",9173,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 5}, {'mark': [2, 1, 3], 'rate': 3, 'confidence': 4}]"
ProtChatGPT: Towards Understanding Proteins with Large Language Models,"['Large Language Models', ' ChatGPT-like system', ' Protein Understanding']",9172,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
Bayesian Vector Optimization with Gaussian Processes,"['Vector Optimization', ' Bayesian Optimization', ' Gaussian Processes', ' Ordering Cones']",9167,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}]"
Meta- (out-of-context) learning in neural networks,"['LLMs', ' large language models', ' in-context learning', ' meta-learning', ' world models', ' internalization', ' consistency', ' learning factual associations']",9166,"[{'mark': [2, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
On the Long Range Abilities of Transformers,"['Transformers', ' Long Range', ' LRA Benchmark']",9164,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}]"
Social-Transmotion: Promptable Human Trajectory Prediction,"['human trajectory prediction', ' robot navigation', ' autonomous driving', ' attention mechanism']",9161,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
Efficient architectural aspects for text-to-video generation pipeline,"['text-to-video', ' video generation', ' temporal consistency', ' frames interpolation', ' Inception Score', ' CLIPSIM', ' MoVQ video decoder']",9160,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}]"
Gandalf: Learning label correlations in Extreme Multi-label Classification via Label Features,"['Extreme Multilabel Classification', ' Key-phrase ads matching', ' short-text classification']",9159,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}]"
Robust Classification via Regression-Based Loss Reweighting and Label Correction,"['label noise', ' noisy labels', ' robustness', ' Gaussian noise', ' classification']",9158,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features,"['subject-specific prediction', ' random effect', ' high-cardinality categorical feature', ' count data', ' clustered data', ' hierarchical likelihood', ' deep learning']",9157,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
ProteinAdapter: Adapting Pre-trained Large Protein Models for Efficient Protein Representation Learning,"['Pretrained Large Models', ' Parameter-Efficient Fine-tuning', ' Protein Representation Learning']",9155,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 3}]"
Let's reward step by step: Step-Level reward model as the Navigators for  Reasoning,"['Large Launage model', ' Process-Supervised Reward Model', ' Multi-step Reasoning']",9154,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}]"
Generalization error of spectral algorithms,"['gradient descent', ' kernel ridge regression', ' optimal algorithm', ' generalization', ' asymptotic error rates', ' power-laws']",9152,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
Slot Structured World Models,"['world models', ' model-based reinforcement learning', ' object-centric representation learning']",9151,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}]"
Observer Uncertainty of Learning in Games from a Covariance Perspective,"['covariance', ' symplectic Euler method', ' follow-the-regularized-leader (FTRL) algorithm', ' uncertainty', ' zero-sum games']",9150,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
Contrastive Implicit Representation Learning,"['Implicit neural representations', ' self-supervised-learning', ' contrastive learning', ' neural fields', ' multiplicative filter networks', ' SimCLR']",9148,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 1, 'confidence': 3}]"
LLM+A: Grounding Large Language Models in Physical World with Affordance Prompting,"['Large Language Model', ' Robotic Control', ' Affordance Prompting']",9146,"[{'mark': [2, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
DAS2C: A Distributed Adaptive Minimax Method with Near-Optimal Convergence,"['Minimax Optimization', ' Distributed Learning', ' Nonconvex Optimization', ' Convergence Analysis', ' Stepsize Inconsistency']",9142,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 1, 3], 'rate': 6, 'confidence': 3}]"
Learning to Reject with a Fixed Predictor: Application to Decontextualization,"['Rejection', ' abstention', ' loss function', ' consistency', ' learning theory', ' decontextualization', ' natural language processing']",9141,"[{'mark': [3, 3, 3], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}]"
Disentangling Covariates to Predict Counterfactuals for single-cell data,"['single-cell', ' computational biology', ' causal inference', ' generative model', ' variational inference', ' variational autoencoder', ' fairness', ' representation disentanglement']",9140,"[{'mark': [1, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 4}]"
All Languages Matter: On the Multilingual Safety of Large Language Models,"['LLMs', ' Safety', ' Multilingual']",9139,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 3], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 4], 'rate': 5, 'confidence': 4}]"
Deep Anti-Regularized Ensembles,"['Deep Ensemble', ' Uncertainty', ' Out-of-distribution', ' Anti-regularization']",9138,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}]"
CLASS-INCREMENTAL LEARNING USING GENERATIVE EXPERIENCE REPLAY BASED ON TIME-AWARE REGULARIZATION,"['lifelong learning', ' continual learning', ' class-incremental learning', ' regularization']",9137,"[{'mark': [3, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging,"['Cross-Lingual Transfer', ' Model Merging', ' Large Language Models']",9136,"[{'mark': [4, 4, 3], 'rate': 8, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 4}]"
Thin-Thick Adapter: Segmenting Thin Scans Using Thick Annotations,"['Semantic Segmentation', ' Computed Tomography', ' Domain Adaptation']",9133,"[{'mark': [3, 4, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}]"
Î³-Orthogonalized Tensor Deflation: Towards Robust \& Interpretable Tensor Decomposition in the Presence of Correlated Components,"['Low-rank signal reconstruction', ' tensor decomposition', ' random matrix theory', ' optimization.']",9131,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 5}, {'mark': [3, 1, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}]"
Dynamics-Informed Protein Design with Structure Conditioning,"['Diffusion Models', ' Generative Modeling', ' Protein Design', ' Normal Mode Analysis']",9128,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
DfPO: Degeneration-free Policy Optimization via Action Masking in Natural Language Action Spaces,"['Reinforcement learning', ' Natural language processing']",9127,"[{'mark': [1, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}]"
SR-OOD: Out-of-Distribution Detection via Sample Repairing,"['OOD Detection', ' Generative Model']",9125,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}]"
Syntactic Representations Enable Interpretable Hierarchical Word Vectors,"['Syntactic Representations', ' Interpretable Vectors', ' Hierarchical Vectors']",9124,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}]"
On Synthetic Data and Iterative Magnitude Pruning: a Linear Mode Connectivity Study,"['Neural Network Pruning', ' Linear Mode Connectivity', ' Dataset Distillation', ' Sparse Neural Networks']",9121,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 3}]"
Can General-Purpose Language Models Emulate a General-Purpose Computer In-Context?,"['large language models', ' general-purpose computing', ' AI alignment']",9120,"[{'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [4, 4, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}]"
VRAda: A Variance Reduced Adaptive Algorithm for Stochastic Parameter-Agnostic Minimax Optimizations,"['Stochastic minimax optimization', ' Parameter-agnostic', ' Variance-reduction']",9118,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}]"
Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models,"['LLM decoding', ' keyword-constrained generation', ' toxicity reduction', ' factual correctness']",9116,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 1, 4], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Partitioning Message Passing for Graph Fraud Detection,"['Graph Neural Networks', ' Fraud Detection']",9114,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
"Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation","['language model', ' hallucination', ' trustworthy artificial intelligence', ' reasoning']",9113,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}]"
Non-backtracking Graph Neural Networks,"['non-backtracking', ' redundancy', ' graph neural network', ' over-squashing']",9111,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}]"
FHA-Kitchens: A Novel Dataset for Fine-Grained Hand Action Recognition in Kitchen Scenes,"['hand action recognition', ' fine-grained', ' dataset', ' benchmark']",9109,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Towards Analyzing Self-attention via Linear Neural Network,"['transformers', ' linear neural networks', ' gradient flow analysis']",9108,"[{'mark': [2, 2, 1], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Efficient Denoising Diffusion via Probabilistic Masking,"['Diffusion Model', ' Sparse Training', ' Model Compression']",9106,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}]"
Phase Transitions in Contrastive Learning,"['representation learning', ' training dynamics', ' contrastive learning']",9102,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}]"
There is More to Graphs than Meets the Eye: Learning Universal Features with Self-supervision,"['Representation learning', ' Self supervised learning', ' Foundation models', ' Generalisability', ' Graph transformer']",9097,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Latent Lie Group Representations,"['deep learning', ' symmetry', ' lie groups']",9095,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 1, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 5, 'confidence': 4}]"
CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity,"['Distributed Optimization', ' Effective Dimension', ' Gradient Compression', ' Learning Theory']",9094,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}]"
The KNN Score for Evaluating Probabilistic Multivariate Time Series Forecasting,"['time series', ' forecasting', ' metric', ' evaluation', ' probabilistic', ' multivariate', ' knn', ' density estimation', ' scoring rule']",9092,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}]"
ProFeAT: Projected Feature Adversarial Training for Self-Supervised Learning of Robust Representations,"['Self-supervised Adversarial Training', ' Adversarial Training', ' Adversarial Robustness', ' Contrastive Learning']",9090,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 5}]"
Semi-Anchored Gradient Methods for Nonconvex-Nonconcave Minimax Problems,"['Optimization', ' Minimax', ' PDHG', ' nonconvex-nonconcave', ' Weak-MVI']",9088,"[{'mark': [3, 2, 1], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}]"
Tensor-Train Point Cloud Compression and Efficient Approximate Nearest Neighbor Search,"['Nearest neighbor search', ' Approximate Search', ' Information Storage and Retrieval']",9087,"[{'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 4}]"
Automating Continual Learning,"['continual learning', ' in-context learning', ' meta-learning', ' self-referential learning', ' linear Transformers']",9084,"[{'mark': [2, 4, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}]"
When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks,"['In-context learning', ' large language models', ' instruction tuning']",9082,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 4, 2], 'rate': 3, 'confidence': 4}]"
Mitigating backdoor attacks with generative modelling and dataset relabelling,"['backdoor defense', ' backdoor learning', ' trusthworty AI', ' AI security']",9081,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}]"
Layer-wise Pre-weight Decay,"['deep learning', ' regularization', ' generalization', ' weight decay']",9078,"[{'mark': [1, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Efficient Unsupervised Knowledge Distillation with Space Similarity,['unsupervised knowledge distillation'],9077,"[{'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
A Convergent Federated Clustering  Algorithm without Initial Condition,"['Federated Learning', ' Heterogeneity', ' Clustering']",9076,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Antibody DomainBed: Out-of-Distribution Generalization in Therapeutic Protein Design,"['domain generalization', ' invariance', ' benchmarks', ' drug discovery']",9074,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}]"
Adaptive Causal Balancing for Collaborative Filtering,"['Recommender System', ' Causal Inference', ' Bias', ' Debias', ' Balancing']",9073,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}]"
Integrated Model Explanations by Independent and Collaborative Feature Influence via Linear-Nonlinear Perspectives.,"['Explanation method', ' Linear simplification', ' Feature interactions', ' Independent influence', ' Collaborative influence', ' Linear-Nonlinear Explanation']",9072,"[{'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Where is the Invisible: Spatial-Temporal Reasoning with Object Permanence,"['Object Permanence', ' Visual Relational Reasoning', ' Trajectory Prediction']",9071,"[{'mark': [2, 1, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}]"
Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems,"['large language models', ' prompting', ' mathematical reasoning', ' natural language processing']",9070,"[{'mark': [2, 2, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}]"
Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching,"['Supervised contrastive learning', ' neural collapse', ' implicit bias', ' class imbalance']",9066,"[{'mark': [4, 4, 2], 'rate': 6, 'confidence': 4}, {'mark': [4, 4, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems,"['Efficient coding', ' object representation', ' dropout', ' robustness', ' human fMRI', ' occipitotemporal cortex', ' cognitive neuroscience', ' distributed coding']",9065,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}]"
Controllable Text-to-Image Generation with Automatic Sketches,"['text to image generation', ' controllable generation', ' large language models']",9064,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 5, 'confidence': 5}]"
Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher,"['knowledge distillation', ' language model', ' NLP']",9063,"[{'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}]"
Estimation error of gradient descent in deep regressions,"['deep regression', ' gradient descent', ' estimation error', ' approximation', ' generalization', ' optimization']",9062,"[{'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}]"
Fast Conditional Intervention in Algorithmic Recourse with Reinforcement Learning,"['Algorithmic recource', ' Causality', ' Reinforcement Learning', ' Explainable machine learning']",9061,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}]"
Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations,"['Implicit neural representation', ' generative model', ' domain agnostic']",9058,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 3}]"
DreamFuser: Value-guided Diffusion Policy for Offline Reinforcement Learning,['Trajectory-based Reinforcement Learning; Diffusion Model; Offline Reinforcement Learning;'],9052,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
2D-Supervised Monocular 3D Object Detection by Global-to-Local Reconstruction,['monocular 3D object detection'],9051,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}]"
TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems,"['NeuralODE', ' Graph Neural Networks', ' Dynamical Systems', ' Physical Simulations', ' Physics-informed Neural Networks']",9050,"[{'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 4}]"
Post-Training Recovery from Injected Bias with Self-Influence,"['Deep learning', ' dataset bias', ' debiasing']",9048,"[{'mark': [1, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}]"
Feature Selection in the Presence of Monotone Batch Effects,"['Batch Effect', ' Distribution Shift']",9047,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}]"
Misusing Tools in Large Language Models With Visual Adversarial Examples,"['LLM', ' Advesarial examples', ' Prompt Injection', ' Security']",9046,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 6, 'confidence': 5}]"
FedLoRA: When Personalized Federated Learning Meets Low-Rank Adaptation,"['Personalized Federated Learning', ' non-IID', ' Low-Rank Adaptation']",9045,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling,"['Active Learning', ' Meta Learning']",9044,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
A Neural Sandbox Framework for Discovering Spurious Concpets in LLM Decisions,"['Large Language Model', ' Spurious Corelation', ' NLP', ' AI Alignment']",9041,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 3, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 1, 'confidence': 4}]"
From Random to Relevant: Harnessing Salient Masks in Non-IID Federated Learning,"['Sparsity', ' Pruning', ' Federated Learning', ' Sparse Federated Learning', ' Communication efficiency', ' Efficient FL', ' Pruning at Initialization']",9040,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}]"
MedJourney: Counterfactual Medical Image Generation by Instruction-Learning from Multimodal Patient Journeys,"['instruction image editing', ' instruction-learning', ' image generation', ' diffusion', ' natural-language instruction', ' biomedicine', ' counterfactual generation', ' disease progression modeling', ' GPT-4', ' imaging reports', ' latent diffusion model', ' curriculum learning', ' MIMIC-CXR']",9035,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Bayesian Coreset Optimization for Personalized Federated Learning,"['federated learning', ' personalized federated learning', ' bayesian coreset', ' submodularity', ' variational inference', ' coresets', ' optimization']",9034,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}]"
Fooling the Textual Fooler via Randomizing Latent Representations,"['NLP', ' Adversarial Defense', ' Robustbess']",9033,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 4, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 4], 'rate': 5, 'confidence': 5}]"
In-context Autoencoder for Context Compression in a Large Language Model,"['large language model', ' context compression', ' in-context autoencoder', ' pretraining', ' fine-tuning', ' Llama', ' GPT', ' memorization']",9031,"[{'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning,"['instruction tuning', ' multimodal large language model', ' hallucination', ' datasets']",9027,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
Implicit Neural Representation Image Codec with Mixed Context for Fast Decoding,"['Image Compression', ' Implicit Neural Representation', ' Adaptive Entropy Modeling']",9026,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
DECOUPLING REASONING FROM OBSERVATIONS FOR EFFICIENT AUGMENTED LANGUAGE MODELS,"['Tool augmented language model', ' Efficiency', ' Prompt redundancy', ' Instruction fine-tuning']",9024,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Multimarginal Generative Modeling with Stochastic Interpolants,"['multi-marginal', ' unsupervised learning', ' generative modeling', ' measure transport', ' optimal transport']",9021,"[{'mark': [4, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Neural Bounds on Bayes Error: Advancing Classification and Generative Models,"['f-Divergence', ' Bayes Error', ' Generative Adversarial Networks (GANs)', ' Representation Learning Neural Networks', ' Multiclass Classification']",9020,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 2}]"
Proving Test Set Contamination for Black-Box Language Models,"['language modeling', ' memorization', ' dataset contamination']",9019,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification,"['Vision-Language Models', ' Large Language Model', ' Zero-Shot Learning', ' Few-Shot Learning']",9018,"[{'mark': [2, 3, 1], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 1], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}]"
Stochastic interpolants with data-dependent couplings,"['flows', ' diffusions', ' stochastic interpolants', ' generative models', ' sde', ' ode', ' image generation']",9016,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
FILI: Syntax Repair By Learning From Own Mistakes,"['Automatic Program Repair', ' Software Engineering', ' Neural Syntax Fix']",9014,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 2], 'rate': 6, 'confidence': 4}]"
GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length,"['Large Language Model', ' Pretraining']",9012,"[{'mark': [4, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Meta Compression: Learning to compress Deep Neural Networks,"['Model compression', ' meta learning', ' efficient inference', ' deep learning']",9011,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 5}]"
Non-Autoregressive Machine Translation as Constrained HMM,['text generation; label bias'],9009,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling,"['Prompting', ' In-Context Learning', ' Few-Shot Learning', ' GPT', ' Large Language Models', ' Multi-Step Reasoning', ' Natural Language Processing']",9008,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}]"
Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective,"['large language model', ' adaptive testing', ' model evaluation']",9005,"[{'mark': [1, 1, 1], 'rate': 3, 'confidence': 2}, {'mark': [2, 3, 3], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 5, 'confidence': 4}]"
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators,"['Multivariate time series forecasting', ' channel dependence', ' lead-lag relationships']",9003,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 4, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Structured Graph Reduction for Efficient GNN,"['structured graph coarsening', ' graph neural network', ' node classification', ' convex optimization']",9000,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}]"
RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval,"['Retrieval Augmented Language Models', ' Information Retrieval', ' summarization', ' QA']",8997,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
Fair and Efficient Contribution Valuation for Vertical Federated Learning,"['Vertical federated learning', ' Contribution valuation', ' Fairness']",8996,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Principal Component Analysis for Cross-Sectionally Correlated Pricing Errors,"['Unsupervised Learning', ' Optimization', ' Principal Component Analysis', ' Asset Pricing', ' Factor Pricing Model']",8992,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 5}]"
Self Guided Exploration for Automatic and Diverse AI Supervision,"['Language models', ' Reinforcement Learning', ' Unsupervised Reinforcement Learning']",8990,"[{'mark': [4, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Interpretable and Convergent Graph Neural Network Layers at Scale,"['Graph Neural Networks', ' Energy-based Models', ' Scalable Training', ' Bi-level Optimization', ' Interpretability']",8988,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 4, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}]"
Learning Invariances via Neural Network Pruning,"['Invariance Learning', ' Neural Network Pruning', ' Auto ML', ' Contrastive Learning', ' Lazy Training', ' Representation Learning', ' Self-Supervised Learning', ' Computer Vision', ' Tabular Learning']",8987,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}]"
In-Context Learning through the Bayesian Prism,"['In-context Learning', ' Transformers', ' Inductive Biases', ' Meta Learning', ' Language Modelling', ' Bayesian Inference']",8985,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}]"
Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning,"['Trojan attacks', ' Parameter-efficient fine-tuning', ' Pre-trained language models']",8984,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Harnessing Overlap in Blockwise Transformers for Near-Infinite Context,"['Language Model', ' Long Context Modeling', ' Reinforcement Learning', ' Unsupervised Reinforcement Learning']",8983,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}]"
L-MBOP-E: Latent-Model Based Offline Planning with Extrinsic Policy Guided Exploration,"['reinforcement learning', ' offline planning', ' offline reinforcement learning', ' model-based reinforcement learning']",8979,"[{'mark': [1, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}]"
Chain of Hindsight aligns Language Models with Feedback,"['Reinforcement Learning', ' Reinforcement Learning from Human Feedback', ' RLHF']",8976,"[{'mark': [3, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 6, 'confidence': 3}, {'mark': [4, 4, 3], 'rate': 6, 'confidence': 4}]"
Bounding the Robustness and Generalization for Individual Treatment Effect,"['Individual Treatment Effect', ' Causal inference']",8972,"[{'mark': [2, 2, 1], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks,"['Graph Neural Networks', ' GNN', ' Explainability', ' Decision Trees']",8971,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Safe Collaborative Filtering,"['recommender systems', ' collaborative filtering', ' scalability']",8970,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 2}]"
MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy,"['language modeling', ' natural language generation', ' decoding algorithms']",8969,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}]"
On Representation Complexity of Model-based and Model-free Reinforcement Learning,"['model-based and model-free RL', ' representation complexity', ' circuit complexity', ' approximation error']",8968,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 2}]"
TPA-Gen: A Multi-modal Data Generative Method for Text and Physics-based Animation,"['Text to Physics-based Animation', ' Multimodal Generation']",8967,"[{'mark': [1, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 1, 'confidence': 3}]"
Addressing Catastrophic Forgetting and Loss of Plasticity in Neural Networks,"['catastrophic forgetting', ' loss of plasticity', ' continual learning', ' streaming learning', ' online learning']",8965,"[{'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
Understanding Multimodal Instruction Format for In-context Learning,"['Visual instruction tuning', ' in-context learning', ' instruction format']",8964,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}]"
Creative Robot Tool Use with Large Language Models,"['Large Language Model', ' Robot Learning', ' Tool Use']",8963,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 4, 2], 'rate': 5, 'confidence': 4}]"
DFITE: Estimation of Individual Treatment Effect Using Diffusion Model,"['Individual Treatment Effect', ' Causal inference', ' diffusion model']",8962,"[{'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Hessian-Aware Bayesian Optimization for Decision Making Systems,"['Bayesian Optimization', ' Active Learning', ' Gaussian Process', ' Graphical Models', ' Bayesian', ' Probabilistic Methods', ' Hessian', ' High-dimensional optimization', ' Global optimization', ' Uncertainty', ' Optimization under Uncertainty']",8961,"[{'mark': [2, 1, 2], 'rate': 1, 'confidence': 4}, {'mark': [2, 1, 3], 'rate': 3, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 1, 3], 'rate': 5, 'confidence': 2}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 2}]"
A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation,"['Knowledge Distillation', ' Meta-Knowledge Distillation', ' Policy-driven Knowledge Distillation', ' Large Language Models']",8958,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 1}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 3}]"
Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity,"['Functional Connectivity', ' Graph Representation Learning', ' Anomaly Detection', ' Brain Representation Learning']",8957,"[{'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}]"
Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making,"['fine-tuning', ' transformer-based language models', ' feature analysis', ' interpretation', ' clinical classification']",8956,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
What happens when you fine-tuning your model? Mechanistic analysis of procedurally generated tasks.,"['Fine-Tuning', ' Interpretability', ' Mechanisms']",8951,"[{'mark': [4, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 1, 3], 'rate': 3, 'confidence': 4}]"
LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization,"['Neural Architecture Search', ' Large Language Models', ' Quality Diversity Optimization']",8949,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 1, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 3], 'rate': 3, 'confidence': 4}]"
Combine and Compare: Graph Rationale Learning with Conditional Non-Rationale Sampling,"['Non-Rationale Sampling', ' Rationale representation learning', ' Graph Generalization']",8946,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Harnessing Discrete Representations for Continual Reinforcement Learning,"['reinforcement learning', ' continual reinforcement learning', ' discrete representations', ' representation learning']",8945,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Learning Dynamical Systems with Helmholtz-Hodge Decomposition and Gaussian Processes,"['Gaussian process', ' dynamical system', ' Helmholtz-Hodge decomposition']",8944,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 3}]"
"MiniFold: Simple, Fast and Accurate Protein Structure Prediction","['protein', ' structure prediction', ' efficiency', ' hardware-optimization']",8942,"[{'mark': [3, 3, 3], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 5}]"
RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems,"['large language model', ' code completion', ' benchmark']",8936,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}]"
Outliers Memorized Last: Trends in Memorization of Diffusion Models Based on Training Distribution and Epoch,"['Diffusion Models', ' Generative AI', ' Memorization']",8934,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [2, 1, 2], 'rate': 1, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}]"
Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective,"['Parametric Knowledge Transfer', ' Large Language Model']",8932,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 1}]"
Generalization Error Analysis of Deep Physical Models With Latent Variables Trained on Trajectory Data,"['Hamiltonian Neural Networks', ' Generalization Error', ' AI for Science']",8931,"[{'mark': [4, 3, 4], 'rate': 8, 'confidence': 1}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 1}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}]"
Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning,"['large language models (LLM)', ' feature learning', ' text attributed graphs (TAG)', ' graph neural networks (GNN)']",8930,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain Networks,"['Graph Neural Network', ' Supply Chain Network', ' Shipment Prediction', ' Inventory Prediction', ' Event Prediction']",8927,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 1, 'confidence': 4}]"
Multi-Scale Generative Modeling in Wavelet Domain,['wavelet transform; score-based generative model; diffusion model; wavelet decomposition'],8926,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
A Theoretical Explanation of Deep RL Performance in Stochastic Environments,"['reinforcement learning', ' effective horizon', ' RL theory', ' theory of reinforcement learning', ' instance-dependent bounds', ' empirical validation of theory']",8925,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}]"
SMILE: Audio-Visual Speech Recognition with Siamese Masked Interaction Learning,"['Audio-Visual Speech Recognition', ' Siamese Masked Interaction Learning']",8924,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Investigating the Fairness of Large Language Models for Predictions on Tabular Data,"['Fairness', ' Social Biases', ' Large Language Models', ' In-Context Learning', ' Tabular Data', ' Trustworthy ML']",8922,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
SuRe: Improving Open-domain Question Answering of LLMs via Summarized Retrieval,"['question answering', ' large language model', ' retrieval']",8921,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API,['UI task automation; Instruction grounding; RL for computer vision'],8920,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
An Explainable AI-based Complementary Attention Mechanism for Detecting Identity Swaps,"['deep learning', ' fake content', ' fake faces', ' identity swap', ' scaled spatial attention', ' layer-integrated channel attention', ' LIME', ' deepfake', ' faceswap']",8919,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}]"
Retrieval meets Long Context Large Language Models,"['Large Language Models', ' Long Context Window', ' Retrieval']",8917,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}]"
From Child's Play to AI: Insights into Automated Causal Curriculum Learning,"['reinforcement learning', ' curriculum learning', ' cognitive science', ' cognitive development']",8916,"[{'mark': [3, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [2, 4, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}]"
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning,"['Diffusion Models', ' Large Language Models', ' Instruction-Finetuning', ' Reasoning']",8915,"[{'mark': [4, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range,"['Visual graph matching (GM)', ' certified robustness', ' randomized smoothing', ' joint smoothing distribution']",8914,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
DAME: A Distillation Based Approach For Model-agnostic Local Explainability,"['Post-hoc Explainability', ' Interpretability', ' Saliency']",8912,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [4, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Neural Spectral Methods,"['Machine learning for PDE', ' spectral methods', ' neural network differentiation', ' spectral loss']",8910,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}]"
Heterogeneous Decision Making towards Mixed Autonomy: When Uncertainty-aware Planning Meets Bounded Rationality,"['Mixed Autonomy', ' Reinforcement Learning', ' Bounded Rationality', ' Regret Analysis']",8909,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 4, 2], 'rate': 6, 'confidence': 2}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 2}]"
Plasticity-Driven Sparsity Training for Deep Reinforcement Learning,"['Reinforcement Learning', ' Sparse Training', ' Network Plasticity']",8907,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [1, 3, 1], 'rate': 3, 'confidence': 5}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
ScoreFlow: Bridging Score and Neural ODE for Reversible Generative Modeling,"['Reversible generative modeling', ' Neural ODEs', ' Diffusion models', ' Image translation', ' Deep learning']",8904,"[{'mark': [2, 2, 1], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 1, 1], 'rate': 3, 'confidence': 4}]"
Moral High Ground: A text-based games benchmark for moral evaluation,"['Text-based Games', ' LLM Evaluation', ' LLM Tuning']",8903,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 4}]"
Achieving Certified Robustness and Maintaining Clean Accuracy via Vanilla Model Guide,"['Adversarial examples', ' Certified Robustness']",8901,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 3], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}]"
Generating Images in Context with Multimodal Large Language Models,"['Diffusion Models', ' Vision-Language', ' Image Generation']",8897,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}]"
Learning to Explore for Stochastic Gradient MCMC,"['Bayesian Neural Networks', ' Meta-Learning', ' MCMC']",8896,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Selective Visual Representations Improve Convergence and Generalization for Embodied AI,"['Embodied-AI', ' Task-conditioned Representations', ' Visual Navigation', ' Reinforcement Learning']",8895,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
Fairness-Aware Attention for Contrastive Learning,"['Fairness', ' Contrastive Learning', ' Attention']",8894,"[{'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}]"
High variance score function estimates help diffusion models generalize,"['generative modeling', ' score-based modeling', ' score matching', ' generalization', ' diffusion', ' theory']",8893,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models,"['Interactive Fiction', ' Text-based Reinforcement Learning', ' Self-supervision']",8892,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition,"['Uncertainty', ' Face', ' Recognition', ' Performance', ' ROC', ' Fairness', ' Bootstrap']",8889,"[{'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Lightweight Language Model Calibration for Open-ended Question Answering with Varied Answer Lengths,"['calibration', ' hallucination', ' large language model']",8885,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}]"
Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models,"['large language model', ' clinical nlp', ' synthetic data generation']",8883,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 4}]"
FusionViT: Hierarchical 3D Object Detection via Lidar-Camera Vision Transformer Fusion,"['3D object detection', ' Camera-Lidar Fusion', ' Vision Transformer']",8882,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}]"
Improving Generalization of Alignment with Human Preferences through Group Invariant Learning,"['alignment', ' language model', ' invariant learning']",8880,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Learning and Forgetting Unsafe Examples in Large Language Models,['Large language models; Safety alignment; Neural networks forgetting'],8876,"[{'mark': [1, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}]"
Deep-Learning Approaches for Optimized Web Accessibility: Correcting Violations and Enhancing User Experience,"['web accessibility', ' artificial intelligence', ' large language models', ' benchmark', ' GPT']",8875,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources,"['large language model', ' knowledge grounding']",8874,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Disentangled Heterogeneous Collaborative Filtering,"['Collaborative Filtering', ' Recommender System', ' Contrastive Learning']",8872,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 2}]"
Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"['Reinforcement Learning', ' Model-based Reinforcement Learning', ' Offline Reinforcement Learning']",8871,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 2, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}]"
Energy-based Automated Model Evaluation,"['Automated Model Evalutaion', ' Energy', ' Meta-distribution', ' Distribution shift']",8869,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}]"
Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning,"['Large Language Models', ' In-context Learning', ' Natural Language Explanations']",8868,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 4], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Deceptive Fairness Attacks on Graphs via Meta Learning,"['graph learning', ' fairness', ' adversarial attacks']",8867,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [4, 2, 3], 'rate': 8, 'confidence': 3}]"
What Matters to You? Towards Visual Representation Alignment for Robot Learning,"['Robot learning', ' Preference learning', ' Visual reward learning', ' Representation alignment']",8866,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}]"
Village-Net clustering: A novel unsupervised manifold clustering method,"['Unsupervised clustering', ' Machine Learning', ' Random-Walks', ' Community detection']",8865,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}]"
Zero-shot Clustering of Embeddings with Pretrained and Self-Supervised Learning Encoders,"['ssl', ' clustering']",8864,"[{'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [4, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 1, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Genetic Algorithm for Curriculum Generation in Multi-Agent Reinforcement Learning,"['Reinforcement Learning', ' Curriculum Learning', ' Genetic Algorithm', ' Multiagent Reinforcement Learning']",8862,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}]"
Correct and speak: accent reduction with minimum supervision,"['Voice Conversion', ' Spoken Language Models', ' speech tokenizer', ' In-context Learning']",8861,"[{'mark': [3, 2, 2], 'rate': 1, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
PINNACLE: PINN Adaptive ColLocation and Experimental points selection,"['Physics-informed Neural Networks', ' PINNs', ' adaptive training points selection']",8858,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization,"['Federated Learning', ' Adaptive Gradient Methods']",8857,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 1, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
A Differentiable Physical Simulation Framework for Soft Robots on Multiple-Task Learning,"['Differentiable Physics', ' Multiple-task Learning', ' Soft Robot Learning']",8856,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers,"['instruction optimization', ' prompt optimization', ' large language models']",8855,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Instructing Large Language Models to Identify and Ignore Irrelevant Conditions,"['Math Word Problem Solving', ' Multi-step Reasoning', ' Prompting', ' Chain-of-Thought', ' Large Language Models']",8853,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
Adaptive Expansion for Hypergraph Learning,"['Hypergraph', ' Hypergraph Expansion.']",8851,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System,"['logistics', ' delivery address', ' pre-training', ' graph']",8849,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 5}]"
BooookScore: A systematic exploration of book-length summarization in the era of LLMs,"['summarization', ' evaluation', ' long context', ' prompting', ' LLM']",8848,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 4, 4], 'rate': 1, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
NP-GL: Extending Power of Nature from Binary Problems to Real-World Graph Learning,"['graph learning', ' nature-powered computing', ' dynamic physical system']",8845,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Transformer-Based Large Language Models Are Not General Learners: A Universal Circuit Perspective,"['Large Language Model', ' Transformer', ' Universal Circuit']",8842,"[{'mark': [4, 2, 1], 'rate': 1, 'confidence': 5}, {'mark': [2, 1, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Posterior Sampling via Langevin Monte Carlo for Offline Reinforcement Learning,"['reinforcement learning', ' offline RL', ' posterior sampling']",8841,"[{'mark': [3, 2, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 6, 'confidence': 4}]"
Efficient Point Cloud Matching for 3D Geometric Shape Assembly,"['Geometric shape assembly', ' High-dimensional feature transform', ' Correlation aggregation', ' Proxy Match Transform']",8840,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 2}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 4}]"
From Deterministic to Probabilistic World: Balancing Enhanced Doubly Robust Learning for Debiased Recommendation,"['Recommender system', ' Selection bias', ' Doubly robust', ' Probabilistic model']",8839,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
FLOOD SIMULATION WITH PHYSICS-INFORMED MESSAGE PASSING,"['Physics-informed GNN', ' flood simulation', ' PDEs']",8838,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}]"
PACIA: Parameter-Efficient Adapter for Few-Shot Molecular Property Prediction,"['molecular property prediction', ' few-shot learning', ' hypernetwork']",8837,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}]"
Provable Repair of Vision Transformers: Last Layer is All You Need,"['neural network repair', ' vision transformers', ' formal guarantees']",8836,"[{'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}]"
LLMZip: Lossless Text Compression using Large Language Models,"['Large Language Models', ' Transformers', ' Compression', ' Arithmetic Coding', ' Zip', ' Lossless Text Compression']",8834,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}]"
VMFTransformer: An Angle-Preserving and Auto-Scaling Machine for Multi-horizon Probabilistic Forecasting,"['time series forecasting', ' probabilistic forecasting']",8832,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}]"
AdaO2B: Adaptive Online to Batch Conversion for Out-of-Distribution Generalization,"['online to batch conversion', ' out-of-distribution (OOD) generalization', ' streaming applications', ' bandit']",8830,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}]"
Leveraging Previous Tasks in Optimizing Risk Measures with Gaussian Processes,"['risk measure', ' value-at-risk', ' conditional value-at-risk']",8829,"[{'mark': [2, 1, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}]"
Canonpipe: Data Debugging with Shapley Importance over Machine Learning Pipelines,"['data debugging', ' data valuation', ' shapley value', ' machine learning pipelines']",8826,"[{'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
MoAT: Multi-Modal Augmented Time Series Forecasting,"['time series', ' multi-modal', ' augmentation', ' forecasting']",8825,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}]"
Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances,"['scientific computing', ' data-driven algorithm design', ' online learning', ' multi-armed bandits', ' contextual bandits', ' numerical analysis', ' learning-augmented algorithms', ' algorithms with predictions']",8824,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}]"
Group Robustness via Adaptive Class-Specific Scaling,"['group robustness', ' debiasing']",8822,"[{'mark': [1, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
CT++: Complementary Co-Training for Semi-Supervised Semantic Segmentation,"['semi-supervised learning', ' semantic segmentation']",8819,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 5}]"
Towards Better Evaluation of GNN Expressiveness with BREC Dataset,"['GNN', ' Expressiveness', ' Datasets']",8818,"[{'mark': [3, 2, 2], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}]"
Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning,"['Offline reinforcement learning', ' instance-dependent', ' least-squares value iteration']",8813,"[{'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
EMP-SSL: Towards Self-Supervised Learning in One Training Epoch,"['Self-supervised Learning', ' Online Learning']",8812,"[{'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}]"
Nuisance-Robust Weighting Network for End-to-End Causal Effect Estimation,"['causal inference', ' pessimism', ' adversarial training']",8811,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 2}]"
Skill-Mix: a Flexible and Expandable Family of Evaluations for AI Models,"['Large language model', ' skill evaluation', ' LLM benchmark', ' emergence']",8809,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}]"
OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning,"['Deep Learing', ' Capsule Network', ' Orthogonality', ' Pruning']",8808,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 1, 2], 'rate': 1, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}]"
A Quadratic Synchronization Rule for Distributed Deep Learning,"['distributed training', ' Local SGD', ' local gradient methods', ' generalization', ' implicit bias', ' sharpness']",8805,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}]"
ArchLock: Locking DNN Transferability at the Architecture Level with a Zero-Cost Binary Predictor,['Defense; DNN Transferability; Neural Architecture Search'],8804,"[{'mark': [4, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}]"
Referring Expression Matters: Multi-referring Feature Aggregation for Referring Video Object Segmentation,"['Referring Video Object Segmentation', ' Referring expression segmentation', ' Multimodal representation earning']",8803,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Local-Forward: Towards Biological Plausibility in Deep Reinforcement Learning,"['biological plausibility', ' deep Q-learning', ' TD learning']",8802,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 1], 'rate': 1, 'confidence': 4}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}]"
Exploiting Code Symmetries for Learning Program Semantics,"['Code Symmetry', ' Program Representation', ' Code Modeling', ' Group-Equivariance', ' Robustness']",8801,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 3], 'rate': 3, 'confidence': 5}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}]"
Weight Uncertainty in Individual Treatment Effect,"['Individual Treatment Effect', ' Causal inference', ' Bayesian inference']",8800,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}]"
Latent Shattering: Turning Unconditional Pretrained Generators Into Conditional Models By Imposing Latent Structure,"['generative models', ' generative modeling', ' GANs', ' VAEs']",8799,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 4, 3], 'rate': 5, 'confidence': 3}]"
Efficient Large Language Models Fine-Tuning on Graphs,"['Graph Neural Networks', ' Large Language Models', ' Scalability', ' Label Efficiency']",8798,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
On information dropping and oversmoothing in graph neural networks,['Oversmoothing'],8797,"[{'mark': [3, 2, 3], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
HuRef: HUman-REadable Fingerprint  for Large Language Models,"['Large Language Models (LLMs)', ' Model Identification', ' Fingerprinting']",8795,"[{'mark': [4, 4, 4], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Adaptive Memory Module for Sequential Planning and Reasoning,"['Adaptive computation', ' Memory', ' Planning', ' Reasoning', ' Offline RL']",8794,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}]"
Projected Subnetworks Scale Adaptation,"['adaptation', ' subnetworks']",8793,"[{'mark': [1, 2, 2], 'rate': 1, 'confidence': 4}, {'mark': [1, 1, 2], 'rate': 1, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}]"
Multiple Modes for Continual Learning,['continual learning'],8792,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Robust Graph Neural Networks via Unbiased Aggregation,"['Graph Neural Networks', ' Adversarial Attack', ' Unbiased Graph Estimator']",8788,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 4], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 2, 2], 'rate': 3, 'confidence': 4}]"
Flashback: Understanding and Mitigating Forgetting in Federated Learning,"['Federated Learning', ' Forgetting', ' Knowledge Distillation', ' Deep Learning', ' Continual Learning']",8786,"[{'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Efficient Backdoor Mitigation in Federated Learning with Contrastive Loss,['Backdoor Defense; Federated Learning; Contrastive Loss'],8785,"[{'mark': [3, 1, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Turing Complete Transformers: Two Transformers Are More Powerful Than One,"['transformers', ' computational complexity', ' computation', ' generalization', ' agents', ' multi-model']",8781,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 3}]"
PolyFormer: Scalable Graph Transformer via Polynomial Attention,"['Graph Transformer', ' Graph Filter', ' Graph Neural Network']",8779,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
A Data-Centric Approach for Financial Large Language Models with Abductive Augmentation Reasoning,"['Data centic', ' LLM', ' Finance']",8778,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Cost Adaptive Recourse Recommendation by Adaptive Preference Elicitation,"['Algorithmic Recourse', ' Preference Elicitation']",8776,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}]"
"Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa","['self-training', ' large langauge models', ' finetuning', ' bootstrapping', ' multi-modal']",8775,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}]"
Improving Language Models with Advantage-based Offline Policy Gradients,"['Reinforcement Learning', ' Natural Language Generation', ' Offline Policy Gradients']",8769,"[{'mark': [2, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation,"['retrieval augmented language model', ' language modeling', ' question answering', ' summarization', ' distillation']",8767,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 4, 3], 'rate': 6, 'confidence': 4}]"
Federated Generalization via Information-Theoretic Distribution Diversification,"['Federated learning', ' Information theory', ' Generalization theory', ' Learning theory']",8766,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 3}]"
Defender of privacy and fairness: tiny but reversible generative model via mutually collaborative knowledge distillation,"['Privacy protection', ' and knowledge distillation.']",8765,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}]"
Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions,"['zero-shot classification', ' prompting', ' generative classification', ' label descriptions']",8763,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
A graph transformer for symbolic regression,"['attention mechanism', ' graph transformer', ' symbolic regression']",8761,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 4}]"
Learning Latent Structural Causal Models,"['Bayesian Causal Discovery', ' Latent variable models']",8760,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 3, 2], 'rate': 5, 'confidence': 3}]"
Flexible Diffusion for Graph Neural Networks,['Graph Neural Network; Diffusion; Smoothing Label'],8759,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}]"
In-Context Learning Dynamics with Random Binary Sequences,"['In-Context Learning', ' Large Language Models', ' Interpretability', ' Computational Cognitive Science']",8758,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 1, 3], 'rate': 3, 'confidence': 4}]"
Dynamic Representation of Optimal Transport via Ensemble Systems,['Optimal transport; ensemble systems; moment kernel representation'],8757,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 3, 1], 'rate': 3, 'confidence': 4}]"
Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking,"['grokking', ' implicit bias', ' margin', ' kernel', ' training dynamics', ' generalization']",8756,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference for Recommendation,"['Recommender system', ' Selection Bias', ' Neighborhood effect']",8755,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
GraphECL: Towards Efficient Contrastive Learning for Graphs,['Graph Neural Networks'],8753,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}]"
An Implicit Watermark Framework for Adversary Identification,"['Adversarial attack', ' Forensic investigation']",8752,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}]"
PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization,"['Large Language Models', ' Expert-level Prompt Optimization', ' Strategic Planning']",8751,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
Understanding Retrieval Augmentation for Long-Form Question Answering,"['Question Answering', ' Retrieval', ' Retrieval Augmented Generation', ' Long-Form Question Answering', ' Attribution', ' NLP', ' QA']",8750,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning,"['data pruning', ' dataset distillation', ' random sampling', ' corset selection', ' data-efficient learning']",8746,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 5}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs,"['Bias', ' Fairness', ' LLM', ' Reasoning', ' Persona']",8745,"[{'mark': [3, 4, 3], 'rate': 8, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Enhancing Instance-Level Image Classification with Set-Level Labels,"['set-level labels', ' fast excess risk rate', ' representation learning', ' few-shot learning']",8741,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}]"
Rotation has two sides: Evaluating Data Augmentation for Deep One-class Classification,"['self-supervised learning', ' deep one-class cilassification']",8740,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}]"
QuantEase: Optimization-based Quantization for Large Language Models,"['Post-training Quantization', ' Quantization', ' Large Language Models']",8739,"[{'mark': [4, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 4], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
On the Power of Multitask Representation Learning with Gradient Descent,"['representation learning', ' multi-task learning', ' gradient descent', ' generalization']",8738,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Pushing Boundaries: Mixup's Influence on Neural Collapse,"['mixup', ' neural collapse', ' unconstrained features model']",8736,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows,"['sRGB real noise modeling', ' Normalizing flow', ' Low-level vision']",8735,"[{'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}]"
Open-Ended Learning in General-Sum Games: The Role of Diversity in Correlated Equilibrium,"['Correlated Equilibrium', ' Policy Diversity', ' PSRO']",8733,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}]"
On Local Equilibrium in Non-Concave Games,"['non-concave games', ' learning in games', ' no-regret algorithms', ' local equilibrium']",8732,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 1, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 1], 'rate': 3, 'confidence': 3}]"
A Benchmark on Robust Semi-Supervised Learning in Open Environments,['Semi-Supervised Learning; Robustness; Open Environments'],8731,"[{'mark': [3, 3, 4], 'rate': 8, 'confidence': 5}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 5}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 5}]"
Uncertainty-aware Graph-based Hyperspectral Image Classification,"['Uncertainty Quantification', ' Graph', ' Hyperspectral Image Classification']",8730,"[{'mark': [4, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Zero-shot Human-Object Interaction Detection via Conditional Multi-Modal Prompts,"['Human Object Interaction Detection', ' Zero-shot']",8729,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}]"
Pick and Adapt: An Iterative Approach for Source-Free Domain Adaptation,"['representation learning', ' domain adaptation']",8728,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making,"['Dataset', ' Explanation', ' XAI', ' Language Model']",8727,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Generative Adversarial Equilibrium Solvers,"['Game Theory', ' Amortized Optimization', ' Generalized Nash equilibrium', ' Economics']",8725,"[{'mark': [4, 4, 4], 'rate': 1, 'confidence': 5}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 2, 3], 'rate': 6, 'confidence': 4}]"
"Rethinking Counterfactual Fairness: On Which Individuals to Enforce, and How?","['counterfactual fairness', ' fairness', ' causal effect', ' principal stratification']",8724,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}]"
Learning Boolean functions with neural networks,"['Deep Learning Theory', ' Learning Theory', ' Gradient Descent', ' Analysis of Boolean functions']",8722,"[{'mark': [2, 1, 3], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}]"
Emergent Corpus Pretraining Benefits Vision Language Modeling,"['emergent communication', ' vision language pretraining', ' corpus transfer']",8721,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}]"
Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection,"['outlier detection', ' ood', ' out-of-distribution', ' anomaly detection', ' variational autoencoder', ' VAE']",8717,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [2, 1, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 2}]"
Generative Adversarial Inverse Multiagent Learning,"['Inverse Game Theory', ' Inverse Multiagent Reinforcement Learning']",8714,"[{'mark': [2, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [4, 3, 4], 'rate': 1, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}]"
Learning to Play Atari in a World of Tokens,"['model-based reinforcement learning', ' transformer', ' vector quantised-variational autoencoder']",8712,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}]"
Do Pre-trained Transformers Really Learn In-context by Gradient Descent?,"['In-context learning', ' gradient descent', ' large language models']",8711,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data,"['Fairness-enhancing models', ' adversarial debiasing', ' mixed effects deep learning', ' out of distribution generalization']",8710,"[{'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
FedOD: Federated Outlier Detection via Neural Approximation,"['outlier detection', ' federated learning']",8709,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}]"
Robustness Evaluation of Proxy Models against Adversarial Optimization,"['proxy gaming', ' reward hacking', ' specification gaming', ' misspecification', ' robustness', ' adversarial robustness', ' adversarial attacks', ' alignment', ' ai safety']",8708,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 6, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 3}]"
Multi-Objective Multi-Solution Transport,['Multi-Objective Optimization'],8707,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 3, 3], 'rate': 1, 'confidence': 5}]"
FORKS: Fast Second-Order Online Kernel Learning using Incremental Sketching,"['Online Kernel Learning', ' Second-Order Method', ' Randomized Sketch']",8706,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 2}]"
Graph Transformers on EHRs: Better Representation Improves Downstream Performance,"['transformers', ' graph neural networks', ' electronic health records']",8705,"[{'mark': [4, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 8, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models,"['Offline In-Context Learning', ' Large Language Model Agent', ' Sequential Decision-Making']",8704,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Counterfactual Fairness from Partially DAGs: A General Min-Max Optimization Framework,"['fairness', ' counterfactual fairness', ' DAG', ' partially DAG']",8703,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 1}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}]"
On the Scalability and Memory Efficiency of Semidefinite Programs  for Lipschitz Constant Estimation of Neural Networks,"['Semidefinite programming', ' Lipschitz constant', ' Deep learning']",8702,"[{'mark': [3, 3, 4], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}]"
Efficient Recomputation of Marginal Likelihood upon Adding Training Data in Gaussian Processes and Simulator Fusion,"['Gaussian Process', ' bias variance tradeoff', ' marginal likelihood', ' model selection']",8700,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}]"
Preconditioning for Physics-Informed Neural Networks,"['physics-informed neural network', ' partial differential equation', ' condition number', ' application']",8699,"[{'mark': [4, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 3, 1], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
On the Role of Discrete Tokenization in Visual Representation Learning,"['Self-supervised learning', ' Masked image modeling', ' Discrete visual token']",8697,"[{'mark': [4, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}]"
Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models,"['LLMs', ' VLMs', ' Benchmark']",8696,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech,"['text-to-speech', ' speech synthesis', ' neural audio codec']",8692,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 4}]"
"AceGPT, Localizing Large Language Models in Arabic","['AceGPT', ' Arabic', ' Large Language Model', ' Localization']",8690,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Improving Large Language Model Fine-tuning for Solving Math Problems,"['Math Problem Solving', ' Large Language Models']",8689,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels,"['Depthwise Convolutions', ' Explainability', ' Neuroscience', ' Computer Vision', ' ConvNext']",8688,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models,"['XAI', ' Unsupervised node representation learning', ' Counterfactual Explanations']",8687,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
Reinforcement Learning for Large Group Systems using Hierarchical Kernel Representations,"['Reinforcement learning', ' Group Systems', ' Control Theory']",8686,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}]"
Optimizing Interpersonal Communication by Simulating Audiences with Large Language Models,"['Communication', ' Interpersonal Relationships', ' Large Language Model Applications', ' Agent Simulations', ' Generative Agents']",8685,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}]"
Enhancing Neural Network Transparency through Representation Analysis,"['transparency', ' interpretability', ' monitoring', ' alignment', ' ML safety']",8684,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [1, 1, 3], 'rate': 3, 'confidence': 3}]"
Are Bert Family Good Instruction Followers?  A Study on Their Potential And Limitations,"['Instruction tuning', ' Large language models', ' BERT family', ' Natural language generation']",8680,"[{'mark': [4, 4, 4], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection,"['out-of-distribution detection', ' graph classification']",8679,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Jorge: Approximate Preconditioning for GPU-Efficient Second-Order Optimization,"['second order optimizer', ' hardware efficiency', ' approximate preconditioning']",8677,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 4, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 4, 3], 'rate': 5, 'confidence': 5}]"
Imitation Bootstrapped Reinforcement Learning,"['reinforcement learning', ' robotics', ' continuous control']",8676,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
Exploring the Promise and Limits of Real-Time Recurrent Learning,"['recurrent neural networks', ' real-time recurrent learning', ' online recurrent learning', ' reinforcement learning', ' actor-critic', ' policy gradients']",8675,"[{'mark': [3, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [4, 4, 2], 'rate': 6, 'confidence': 3}, {'mark': [4, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Attribute-Enhanced Similarity Ranking for Sparse Link Prediction,"['Link Prediction', ' Graph Neural Networks', ' Graph Learning', ' Network Science']",8672,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Measuring Graph Similarity Using Transfer Cost of Forster Distributions,"['Graph similarity', ' Foster distributions']",8671,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [4, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}]"
TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting,['Forecasting; Time Series; Large Language Model'],8670,"[{'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}]"
Scaling physics-informed hard constraints with mixture-of-experts,"['Physics-Informed Machine Learning', ' PDEs', ' differentiable optimization', ' neural networks', ' mixture of experts', ' constrained optimization']",8669,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Uncovering Causal Variables in Transformers Using Circuit Probing,"['Interpretability', ' Analysis', ' NLP', ' Pruning']",8667,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining,"['Large Language Models', ' Pretraining', ' Retrieval', ' Instruction Tuning']",8665,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}]"
Discrimination-free Pricing with Privatized Sensitive Attributes,"['fairness', ' privatized sensitive attributes', ' privacy', ' insurance pricing', ' local differential privacy', ' noise estimation', ' transparency']",8664,"[{'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 2}]"
Recursive Score Estimation Accelerates Diffusion-Based Monte Carlo,"['posterior sampling', ' non-isopermetric conditions', ' Monte Carlo', ' SDE']",8663,"[{'mark': [3, 4, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 2, 4], 'rate': 6, 'confidence': 4}, {'mark': [4, 2, 4], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 2}]"
Ensemble Systems Representation for Function Learning over Manifolds,"['Function learning', ' dynamical systems', ' control theory']",8662,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 1, 'confidence': 3}]"
Generalization in diffusion models arises from geometry-adaptive harmonic representation,"['diffusion models', ' memorization', ' generalization', ' inductive bias', ' curse of dimensionality', ' denoising', ' geometry-adaptive harmonic basis']",8660,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 3}, {'mark': [4, 4, 4], 'rate': 1, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
Simplicity Bias of SGD via Sharpness Minimization,"['Sharpness minimization', ' Implicit bias', ' SGD', ' Simplicity Bias', ' trace of Hessian regularizer']",8659,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}]"
Structural Fairness-aware Active Learning for Graph Neural Networks,"['Active Learning', ' Graph Neural Networks', ' Structural Fairness']",8658,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [1, 2, 1], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Neural-Symbolic Recursive Machine for Systematic Generalization,"['Neuro-symbolic AI', ' Systematic Generalization', ' Compositional Generalization']",8657,"[{'mark': [3, 4, 2], 'rate': 6, 'confidence': 4}, {'mark': [4, 2, 3], 'rate': 8, 'confidence': 2}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation,"['text-to-image generation', ' text-to-image evaluation', ' Davidsonian semantics', ' large language models', ' scene graphs', ' visual question answering', ' question generation', ' benchmark']",8654,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}]"
Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations,"['abstract relations', ' vision transformers', ' visual concept learning', ' out-of-distribution generalization', ' same-different relation', ' equality relation', ' inductive biases']",8652,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
In-context Curriculum for Mathematical Reasoning in Small Language Models,"['small language models', ' mathematical reasoning', ' in-context learning', ' specialization', ' Chain-of-thought prompting', ' deep learning', ' transformers', ' large language models']",8651,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
The Consensus Game: Language Model Generation via Equilibrium Search,"['language models', ' decoding', ' planning', ' game theory']",8650,"[{'mark': [4, 3, 4], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Vector Quantized Representations for Efficient Hierarchical Delineation of Behavioral Repertoires,"['animal behavior', ' neuroscience', ' unsupervised unit discovery']",8649,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}]"
STRUCTDROP: A STRUCTURED RANDOM ALGORITHM TOWARDS EFFICIENT LARGE-SCALE GRAPH TRAINING,"['Efficient Training', ' Randomized Algorithm']",8648,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 4], 'rate': 6, 'confidence': 3}]"
Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication,"['Knowledge Distillation', ' Interactive Communication', ' Distill Foundation Model']",8647,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}]"
Chain of Thought Empowers Transformers to Solve Inherently Serial Problems,"['Chain of thought', ' language modeling', ' circuit complexity', ' deep learning theory']",8645,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 2}, {'mark': [3, 2, 4], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 2}, {'mark': [4, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 5}]"
Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy,"['Pure Differential Privacy', ' Monte Carlo sampling', ' Gaussian Differential Privacy', ' Exponential Mechanism']",8643,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}]"
X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning,"['multimodal language model', ' instruction aware representations', ' multitask', ' zero-shot', ' 3d', ' video', ' audio', ' image', ' language', ' frozenllm', ' alignment']",8642,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}]"
Efficient Multi-task Reinforcement Learning via Selective Behavior Sharing,"['Multi-task Reinforcement Learning', ' Behavior sharing']",8641,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Optimal Sketching for Residual Error Estimation for Matrix and Vector Norms,"['Sketching', ' Residual error', ' Low-rank approximation', ' sparse recovery']",8639,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Reverse Diffusion Monte Carlo,"['Posterior Sampling', ' Multi-modal sampling']",8638,"[{'mark': [2, 2, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 2, 1], 'rate': 1, 'confidence': 4}]"
LLM-based Stock Market Trend Prediction,"['Stock Market Trend Prediction', ' Moving Averages', ' Options Volume', ' Market Volatility', ' LLM', ' LSTM Sentiment Analysis', ' Demand & Supply Dependency tree', ' Multi Layer Neural Networks']",8637,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}]"
On the Dynamics of Learning Time-Aware Behavior with RNNs,"['recurrent neural networks', ' latent temporal features', ' developmental interpretability', ' phase transitions', ' dynamical systems theory']",8635,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}]"
Can LLMs Effectively Leverage Graph Structural Information: When and Why,"['Large Language Model', ' Graph', ' Multimodality', ' Data Leakage', ' Homophily']",8633,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}]"
Improving Branching in Neural Network Verification with Bound Implication Graph,"['neural network verification', ' adversarial robustness', ' branch and bound']",8632,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}]"
Counting Graph Substructures with Graph Neural Networks,"['graph neural networks', ' expressive power', ' representation learning', ' subgraph isomorphism', ' cliques', ' cycles', ' motifs', ' substructures', ' count', ' message-passing']",8631,"[{'mark': [3, 1, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 5, 'confidence': 3}]"
Optimal algorithms for group distributionally robust optimization and beyond,"['Distributionally robust optimization', ' Convex optimization', ' Fairness']",8630,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 3], 'rate': 5, 'confidence': 4}]"
Dynamic Mode Decomposition-inspired Autoencoders for Reduced-order Modeling and Control of PDEs : Theory and Design,"['PDEs', ' Autoencoders', ' Reduced-order modeling', ' Control', ' Dynamic mode decomposition']",8629,"[{'mark': [2, 2, 2], 'rate': 6, 'confidence': 3}, {'mark': [1, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 5, 'confidence': 3}]"
Exploiting Action Distances for Reward Learning from Human Preferences,"['Preference based Reinforcement Learning', ' Human Aware AI', ' Reward Learning']",8628,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}]"
Is the Glass Half-Empty or Half-Full? A Mixture-Of-Tasks Perspective on Missing Modality,"['missing modality', ' modality competition', ' multimodal learning', ' multimodal fusion']",8627,"[{'mark': [2, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 2}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 3}]"
Text2Data: Low-Resource Data Generation with Textual Control,"['low resource', ' text-to-data generation']",8626,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}]"
On the Effect of Defection in Federated Learning and How to Prevent It,"['Incentive Design', ' Optimization', ' Robustness', ' Federated Learning', ' Fairness', ' Adaptive Optimization']",8625,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}]"
DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee,"['mixed-integer linear programming', ' generative model']",8623,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}]"
Analyzing Complex Interdependencies in Financial Markets: A Neural Network-Based Approach for News Impact Assessment,"['Stock Market Trend Prediction', ' Market Volatility', ' LSTM Sentiment Analysis', ' Demand & Supply Dependency tree', ' Multi Layer Neural Networks', ' Learning Statistics', ' Regressions', ' Depth-First-Search', ' Advance Web Scraping', ' Balance Sheet']",8617,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 1, 'confidence': 5}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}]"
What's the Magic Word? A Control Theory of LLM Prompting,"['language models', ' control theory', ' LLMs', ' prompt optimization', ' alignment', ' mechanistic interpretability']",8616,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
End-to-end Story Plot Generator,"['automatic story generation', ' end-to-end generator', ' reader-specific reward model', ' rlhf']",8614,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 4, 3], 'rate': 5, 'confidence': 3}]"
Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks,"['learning theory', ' generalization analysis', ' gradient descent', ' stability']",8613,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Are Models Biased on Text without Gender-related Language?,"['Large language models', ' bias evaluation', ' gender bias', ' gender co-occurring words', ' gender-invariant', ' pretraining data statistics']",8612,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Decoupled Actor-Critic,"['Continuous Control', ' Reinforcement Learning', ' Actor-Critic']",8610,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}]"
Unleashing the Potential of LLMs for Quantum Computing: A Study in Quantum Architecture Design,"['Large Language Models', ' Quantum Computing', ' Variational Quantum Algorithms']",8609,"[{'mark': [2, 3, 1], 'rate': 1, 'confidence': 5}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
How do skip connections affect Graph Convolutional  networks  with graph sampling? A theoretical analysis on generalization,"['graph neural network (GNN)', ' skip-connection', ' graph samping', ' generalization analysis', ' deep learning theory']",8606,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [4, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 5, 'confidence': 5}]"
PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning,"['language-based planning', ' procedural/script knowledge', ' distillation', ' large language models', ' decoding-time algorithm']",8605,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction,"['atomic property prediction', ' pre-training', ' 3D atomic pre-training', ' graph neural networks', ' multi-task learning', ' molecules', ' materials']",8604,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}]"
"Patch Ranking Map: Explaining Relations among Top-Ranked Patches, Top-Ranked Features and Decisions of Convolutional Neural Networks for Image Classification","['convolutional neural networks', ' deep learning', ' feature selection', ' image classification', ' optimization']",8603,"[{'mark': [1, 2, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 1, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}]"
The Fine-Grained Chip Placement with Hybrid Action Spaces and Feature Fusion,"['Deep Reinforcement Learning', ' Chip Placement', ' hybrid action space', ' feature fusion']",8602,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 1, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [1, 1, 2], 'rate': 1, 'confidence': 5}]"
AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents,"['Meta-RL', ' Generalization', ' Long-Term Memory', ' Transformers']",8600,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 4], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 2], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 4], 'rate': 6, 'confidence': 4}]"
UniAudio: An  Audio Foundation Model Toward Universal Audio Generation,"['Audio Language Model', ' Universal Audio Generation', ' Foundation Model', ' Zero-shot']",8597,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 1], 'rate': 1, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [4, 4, 4], 'rate': 1, 'confidence': 3}]"
Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets,"['graph neural networks', ' Datasets', ' molecules', ' molecular graphs', ' Quantum', ' Multi-task', ' foundation model']",8594,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 2}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
ABKD: Graph Neural Network Compression with Attention-Based Knowledge Distillation,"['Graph Neural Networks', ' Compression', ' Knowledge Distillation']",8591,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Llamas Know What GPTs Don't Show: Surrogate Models for Selective Classification,"['calibration', ' uncertainty estimation', ' large language models']",8590,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference,"['Causal inference', ' Design of experiments', ' Interference', ' Random graph', ' Spillover effects', ' Treatment effects', ' Potential outcomes']",8589,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [4, 2, 2], 'rate': 5, 'confidence': 4}]"
FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores,"['convolutions', ' GPUs', ' hardware-efficient algorithms', ' long context', ' fast fourier transform', ' I/O awareness']",8584,"[{'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 5}]"
Towards Understanding The Winner-Take-Most Behavior Of Neural Network Representations,"['deep learning', ' neuron representations']",8583,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
SHINE: Shielding Backdoors in Deep Reinforcement Learning,"['deep reinforcement learning', ' trojan backdoor', ' explanation']",8582,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 2}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Semi-Supervised Learning of Tree-Based Models Using Uncertain Interpretation of Data,"['semi-supervised learning', ' decision tree', ' tree ensemble', ' random forest']",8579,"[{'mark': [2, 4, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}]"
Parameter-Efficient Tuning Helps Language Model Alignment,['Alignment; Large Lanugage Models; Controllable Generations'],8577,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
MIMIC: Masked Image Modeling with Image Correspondences,"['Data curation techniques', ' Masked Image Modeling', ' Dense vision tasks', ' Large scale pretraining', ' Self supervised learning', ' Datasets']",8576,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Transformer-VQ: Linear-Time Transformers via Vector Quantization,"['Transformer', ' Transformer Decoder', ' Decoder-Only Transformer', ' Natural Language Processing', ' NLP', ' Vector Quantization', ' VQ', ' K-Means', ' Clustering', ' Causal Attention', ' Autoregressive Attention', ' Efficient Attention', ' Linear-Time Attention', ' Autoregressive Modeling', ' Generative Modeling', ' Gated Attention', ' Compressive Attention', ' Kernelized Attention', ' Kernelizable Attention', ' Hierarchical Attention', ' Segment-Level Recurrent Attention', ' Long-Context Modeling', ' Long-Range Modeling', ' Long-Range Dependencies', ' Long-Term Dependencies', ' Cached Attention', ' Shift-Equivariant Attention']",8575,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Can adversarial samples benefit few-shot unsupervised implicit neural shape representation learning ?,"['3D reconstruction', ' Implicit Neural Representations']",8574,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Understanding Contrastive Learning Through the Lens of Margins,"['Contrastive learning', ' Margins', ' Self-supervised learning']",8573,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}]"
Efficient Model-Agnostic Multi-Group Equivariant Networks,"['Group equivariant networks', ' efficient equivariant networks', ' large equivariant networks']",8572,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}]"
Simple Data Sharing for Multi-Tasked Goal-Oriented Problems,"['goal-conditioned RL', ' offline RL']",8571,"[{'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions,"['In-context learning', ' Transformers', ' Large language models', ' Boolean functions']",8569,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}]"
Initializing the Layer-wise Learning Rate,"['Learning Rate', ' Exploding Gradient', ' Vanishing Gradient']",8568,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Relational Convolutional Networks: A framework for learning representations of hierarchical relations,"['representation learning', ' relational architectures', ' relational representation learning', ' hierarchical feature representations', ' compositionality', ' higher-order relations', ' convolutions']",8566,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}]"
How does overparametrization affect features?,"['deep learning', ' overparametrization']",8564,"[{'mark': [4, 4, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}]"
The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry,"['linear attention', ' transformers']",8562,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}]"
Understanding and Robustifying Sub-domain Alignment for Domain Adaptation,"['Sub-domain method', ' Domain/Distribution alignment', ' Robust knowledge transfer', ' Theory driven methodology']",8560,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers,"['relational representation learning', ' attention', ' transformers', ' sequence models', ' abstract representations']",8558,"[{'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}]"
Doubly Robust Instance-Reweighted Adversarial Training,"['adversarial training', ' distributionally robust optimization', ' bilevel optimization', ' instance reweighting']",8556,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}]"
Estimating uncertainty from feed-forward network based sensing using quasilinear approximation,"['Uncertainty propagation', ' quasilinear approximation', ' stochastic linearization', ' neural networks', ' Kalman filter.']",8555,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}]"
SOLVING SCHRODINGER BRIDGE PROBLEM VIA STOCHASTIC ACTION MINIMIZATION,"['Schrodinger bridge', ' optimal transport', ' single-cell', ' trajectories']",8551,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}]"
Training Diffusion Models with Reinforcement Learning,"['reinforcement learning', ' RLHF', ' diffusion models']",8548,"[{'mark': [3, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
DIRECTIONALITY IN GRAPH TRANSFORMERS,"['graph transformers', ' graph neural networks']",8547,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 4}]"
Latent Conservative Objective Models for Offline Data-Driven Crystal Structure Prediction,"['crystal structure prediction', ' offline model-based optimization']",8545,"[{'mark': [3, 3, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}]"
Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning,"['reinforcement learning', ' federated learning', ' temporal difference learning']",8544,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Explaining the Out-of-Distribution Detection Paradox through Likelihood Peaks,"['out-of-distribution detection', ' normalizing flows', ' manifold hypothesis', ' intrinsic dimension']",8543,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}]"
Federated Q-Learning: Linear Regret Speedup with Low Communication Cost,"['Federated Learning', ' Reinforcement Learning', ' Q-Learning', ' Regret', ' Communication Cost']",8542,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
The Trickle-down Impact of Reward Inconsistency on RLHF,"['Large language model', ' reward model', ' RLHF', ' consistency']",8541,"[{'mark': [1, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}]"
Multi-Resolution Learning with DeepONets and Long Short-Term Memory Neural Networks,"['multi-resolution learning', ' operator learning', ' recurrent neural networks', ' DeepONet', ' LSTM', ' dynamical systems']",8539,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 4, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}]"
Non-stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling,"['Nonstationary Contextual Bandit', ' Neural Bandit Learning', ' Continual Learning', ' Exploration vs Exploitation']",8538,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 2}]"
Depth From Camera Model,"['Depth Estimation', ' Camera Model', ' 3D Reconstruction']",8536,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}]"
Deep probabilistic 3D angular regression for directional dark matter detectors,"['3D', ' Directionality', ' Probabilistic', ' Particle Physics']",8534,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 3}]"
Unsupervised Sign Language Translation and Generation,"['unsupervised', ' sign language translation', ' natural language processing']",8533,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
Cross-modality debiasing: using language to mitigate sub-population shifts in imaging,"['cross-modality', ' sub-population shift']",8531,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Decompose Time and Frequency Dependencies: Multivariate Time Series Physiological Signal Emotion Recognition,"['Physiological Signal', ' Emotion Recognition', ' Time Series', ' Representation Learning', ' Affective Computing']",8530,"[{'mark': [2, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 4}]"
Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation,"['Explainability', ' Behavior Modeling', ' Large Language Models']",8529,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}]"
Continual Graph Learning for Thermal Analysis of Composite Materials under Interface Variations,"['Graph Neural Network', ' Continual Graph Learning', ' Thermal Analysis']",8528,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 1], 'rate': 5, 'confidence': 4}]"
Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models,"['Uncertainty Quantification', ' Selective Generation', ' Natural Language Generation']",8527,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}]"
Stochastic two points method for deep model gradient free optimization,"['zeroth-order optimization', ' gradient free adaptation']",8526,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
SafeDiffuser: Safe Planning with Diffusion Probabilistic Models,"['Diffusion', ' Safe Planning', ' Specification Guarantees']",8525,"[{'mark': [2, 1, 2], 'rate': 1, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 3}]"
Revisitng graph neural networks for traffic forecasting,"['Graph-structured dynamics', ' Linear model', ' Large network', ' Traffic forecasting']",8523,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 1], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Neural Collapse meets Differential Privacy:  Curious behaviors of NoisySGD with Near-Perfect Representation Learning,['Differential privacy; neural collapse; DP-SGD; representation Learning'],8521,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 3, 1], 'rate': 3, 'confidence': 5}]"
Efficient Modulation for Vision Networks,"['EfficientMod', ' Efficient Networks']",8518,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
Pre-training LiDAR-based 3D Object Detectors through Colorization,"['3D object detection', ' LiDAR point cloud', ' pre-training', ' autonomous driving', ' self-supervised learning']",8517,"[{'mark': [4, 4, 4], 'rate': 8, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 5}]"
An Emulator for Fine-tuning Large Language Models using Small Language Models,"['pre-training', ' fine-tuning', ' decouple', ' scale', ' reward', ' alignment']",8516,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 2}, {'mark': [4, 2, 4], 'rate': 8, 'confidence': 5}]"
Toward Student-oriented Teacher Network Training for Knowledge Distillation,"['Knowledge distillation', ' Teacher-student training', ' Empirical risk minimization']",8515,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}]"
Language Models Represent Space and Time,"['Interpretability', ' world models', ' probing']",8514,"[{'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning,"['Causal Reasoning', ' Causal Discovery', ' Structural Causal Models', ' Large Language Models']",8513,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}]"
Fast-ELECTRA for Efficient Pre-training,"['Language model Pre-training', ' ELECTRA', ' Efficiency']",8511,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 6, 'confidence': 3}]"
Maximum Entropy Model Correction in Reinforcement Learning,"['reinforcement learning', ' model-based reinforcement learning', ' maximum entropy', ' planning']",8509,"[{'mark': [3, 2, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 4}]"
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings,"['security', ' machine learning', ' adversarial perturbations', ' large language models']",8508,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Identifying Latent State Transition Processes for Individualized Reinforcement Learning,"['individualized reinforcement learning', ' latent state transition', ' identifiability']",8507,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}]"
Learning Multi-Agent Communication using Regularized Attention Messages,"['Multi-Agent Reinforcement Learning', ' Communication', ' Attention', ' Message Compression']",8505,"[{'mark': [1, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
The mechanistic basis of data dependence and abrupt learning in an in-context classification task,"['in-context learning', ' mechanistic interpretability', ' language models', ' induction heads']",8504,"[{'mark': [4, 3, 3], 'rate': 1, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 1, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 3}]"
SpaCE: The Spatial Confounding Environment,"['causal inference', ' datasets', ' benchmarks', ' spatial confounding', ' public health']",8502,"[{'mark': [3, 4, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}]"
Benchmarking Cognitive Biases in Large Language Models as Evaluators,"['Large Language Models', ' Automatic Evaluation', ' Cognitive Biases']",8501,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
PILOT: An O(1/T)-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation,"['min-max optimization', ' adaptive batch size', ' policy evaluation.']",8499,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
Graph Neural Networks Gone Hogwild,['graph neural network'],8497,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Impact of Molecular Representations on Deep Learning Model Comparisons in Drug Response Predictions,"['Cancer Drug Response Prediction', ' Model Comparison']",8496,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}]"
Positional Description Matters for Transformers Arithmetic,"['Transformer', ' Arithmetic', ' Language Model', ' Deep Learning']",8495,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}]"
FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,"['fresh LLMs', ' search engine-augmented LLMs', "" LLMs' factuality""]",8494,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}]"
Contrastive Post-training Large Language Models on Data Curriculum,"['large language model', ' curriculum learning', ' contrastive learning', ' alignment']",8492,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning,"['multi-objective optimization', ' sample complexity', ' variance reduction', ' momentum']",8491,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
Language Model Detectors Are Easily Optimized Against,"['detector', ' language model', ' learning from preferences']",8488,"[{'mark': [2, 2, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}]"
The Representation Jensen-Shannon Divergence,"['Statistical Divergence', ' Kernel methods', ' Two sample testing']",8487,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models,"['robot learning', ' diffusion model']",8486,"[{'mark': [2, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
Learning to Reach Goals via Diffusion,"['Goal-conditioned reinforcement learning', ' Offline reinforcement learning', ' Diffusion modeling']",8484,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}]"
"LUMOS: Towards Language Agents that are Unified, Modular, and Open Source","['language agent', ' interactive NLP', ' tool-augmented LLM']",8483,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Physics Informed Neurally Constructed ODE Networks (PINeCONes),"['Scientific Machine Learning', ' Neural ODEs', ' PINNs', ' PDEs']",8482,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
CrysFormer: Protein Structure Prediction via 3d Patterson Maps and Partial Structure Attention,"['AI for science', ' protein crystallography', ' Transformer model']",8479,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 3, 'confidence': 3}]"
Latent Space Simulator for Unveiling Molecular Free Energy Landscapes and Predicting Transition Dynamics,"['molecular dynamics', ' simulation', ' Boltzmann distribution', ' sampling']",8477,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 3, 'confidence': 5}]"
Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US,['causal inference; neural networks; air pollution; stochastic interventions; doubly robust inference'],8476,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
An empirical investigation of generalization dynamics in deep ReLU networks via nonlinear mode decomposition,"['learning', ' generalization', ' statistical mechanics', ' teacher-student', ' svd']",8473,"[{'mark': [2, 1, 2], 'rate': 3, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Efficient Transfer Learning from Arbitrary Pre-Trained Models,"['Transfer learning', ' Foundation models']",8472,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
On the Role of Edge Dependency in Graph Generative Models,"['graph', ' network', ' generative', ' model', ' random', ' dependence', ' overlap', ' triangle', ' cycle', ' bound']",8471,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
DiffDock-Pocket: Diffusion for Pocket-Level Docking with Sidechain Flexibility,"['diffusion', ' diffusion models', ' docking', ' generative model']",8469,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Learning Diverse Quadruped Locomotion Gaits via Reward Machines,"['Reward Machine', ' Quadruped Locomotion', ' Reinforcement Learning', ' Robotics']",8468,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}]"
"How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks","['Transformers', ' Capabilities', ' Mechanistic interpretability', ' Synthetic task']",8467,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 4], 'rate': 8, 'confidence': 4}]"
Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation,"['Differential Privacy', ' tCDP', ' Auto DP-SGD', ' clipping threshold estimation', ' noise multiplier decay']",8466,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [2, 2, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
"A New, Physics-Based Continuous-Time Reinforcement Learning Algorithm with Performance Guarantees","['Reinforcement Learning (RL)', ' Continuous Time (CT)', ' Optimal Control', ' Physics-Based']",8465,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 5, 'confidence': 2}, {'mark': [1, 2, 1], 'rate': 1, 'confidence': 4}]"
Graph neural processes and their application to molecular functions,"['Neural processes', ' molecules', ' drug discovery', ' meta-learning', ' docking']",8464,"[{'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}]"
Simple Hierarchical Planning with Diffusion,"['Hierarchical Offline RL', ' Hierarchical planning', ' Hierarchical Reinforcement Learning', ' Diffusion-Based Planning']",8463,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [4, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Stochastic Gradient Descent for Gaussian Processes Done Right,"['Gaussian process', ' stochastic gradient descent']",8461,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}]"
Confronting Reward Model Overoptimization with Constrained RLHF,"['rlhf', ' overoptimization', ' constrained RL']",8460,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [4, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings,"['Time-series', ' Transformer', ' Spatiotemporal']",8459,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}]"
Improved order analysis and design of exponential integrator for diffusion models sampling,['diffusion model;order analysis;fast sampling'],8456,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 1], 'rate': 3, 'confidence': 4}]"
Setting the Record Straight on Transformer Oversmoothing,"['transformers', ' oversmoothing', ' rank collapse']",8455,"[{'mark': [4, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
KBFormer: A Transformer-based Diffusion Model of Structured Entities with Heterogeneous Properties,"['Knowledge Bases', ' Structured Data', ' Discrete State Diffusion']",8454,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 4], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}]"
Bridging Sequence and Structure: Latent Diffusion for Conditional Protein Generation,"['Protein Design', ' Geometric Machine Learning', ' Latent Diffusion', ' Protein Docking']",8453,"[{'mark': [4, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 5}]"
Why is SAM Robust to Label Noise?,"['generalization', ' sharpness', ' robustness', ' SAM']",8452,"[{'mark': [2, 3, 3], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}]"
Gradient descent for matrix factorization: Understanding large initialization,"['Gradient descent', ' matrix factorization', ' large initialization', ' implicit bias', ' incremental learning']",8450,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}]"
How FaR Are Large Language Models From Agents with Theory-of-Mind?,"['Large Language Models', ' Theory-of-Mind', ' Social Reasoning', ' Language Agent', ' Prompting']",8449,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 1], 'rate': 5, 'confidence': 4}]"
Predicting the Performance of Foundation Models via Agreement-on-the-line,"['robustness', ' OOD performance estimation', ' foundation model safety']",8448,"[{'mark': [2, 2, 1], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Revisiting the Last-Iterative Convergence of Stochastic Gradient Methods,"['Convex Optimization', ' Stochastic Optimization', ' Last Iterates']",8446,"[{'mark': [3, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
Learning transferrable and interpretable representation for brain network,"['Self-Supervised Learning', ' Masked Autoencoding', ' Characterizing representations', ' Neuroscience']",8443,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}]"
CNN Kernels Can Be the Best Shapelets,"['Shapelet', ' Covolutional Neural Network', ' Time-series']",8442,"[{'mark': [2, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 5}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 4}]"
COTIC: Embracing Non-uniformity in Event Sequence Data via Multilayer Continuous Convolution,"['temporal point process', ' time series', ' continuous convolutions', ' neural networks']",8440,"[{'mark': [1, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Stochastic Vision Transformers with Wasserstein Distance-Aware Attention,"['Robust Self-supervised Representation Learning', ' Stochastic Transformer', ' Guassian Embedding']",8438,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 1}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 2}]"
Fine-Tuning Language Models with Advantage-Induced Policy Alignment,['reinforcement learning with human feedback'],8437,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 8, 'confidence': 5}, {'mark': [2, 4, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Score-Based Multimodal Autoencoders,"['multimodal autoencoders', ' multimodal variational autoencoders', ' multimodal generative models', ' latent-space score-based models']",8436,"[{'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Fine-Tuning Language Models for Factuality,"['factuality', ' hallucination', ' language model', ' dpo']",8435,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 3, 'confidence': 4}]"
Improving classifier decision boundaries using nearest neighbors,"['decision boundary', ' computer vision', ' CNN', ' kNN']",8434,"[{'mark': [2, 1, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures,"['Self Supervised Learning', ' Joint Embedding Architectures']",8433,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 1}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Malcom-PSGD: Inexact Proximal Stochastic Gradient Descent for Communication Efficient Decentralized Machine Learning,"['Decentralized Machine Learning', ' Proximal SGD', ' Vector Source Encoding', ' Gossip', ' Compressed Communication', ' Model Sparsification']",8432,"[{'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}]"
Absolute Policy Optimization,"['Reinforcement Learning', ' Trust Region Policy Optimization', ' Worst-case Performance Improvement', ' Atari Games']",8430,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"['Biological inspired high performance energy efficient vision system', ' data efficient training', ' energy saving sensoring', ' learned saccade', ' reinforcement learning', ' foveated visual sampling', ' continuous scene reconstruction.']",8429,"[{'mark': [2, 2, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 2}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}]"
Approaching an unknown communication system by latent space exploration and causal inference,"['unsupervised learning', ' structure discovery', ' generative adversarial networks', ' causal inference', ' audio']",8428,"[{'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 4, 2], 'rate': 3, 'confidence': 2}, {'mark': [1, 2, 2], 'rate': 5, 'confidence': 2}]"
Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate,"['Quasi-Newton method', ' limited memory', ' non-asymptotic superlinear convergence']",8427,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}]"
Overthinking the Truth: Understanding how Language Models Process False Demonstrations,"['Mechanistic Interpretability', ' AI Safety', ' Interpretability', ' Science of ML', ' few-shot learning', ' Large Language Models']",8426,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}]"
Transforming Smallholder Farmers Support with an AI-Powered FAQbot: A Comparison of Techniques,"['Agriculture', ' FAQBot', ' LLMs', ' Natural Language Processing']",8425,"[{'mark': [1, 1, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}]"
Efficient Subgraph Rule Induction via Tree Folding in Differentiable Logic Programming,"['inductive logic programming', ' subgraph rules', ' gradient-based']",8424,"[{'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 4}]"
Estimating Unknown Population Sizes Using Hypergeometric Maximum Likelihood,"['multivariate hypergeometric distribution', ' maximum likelihood estimation', ' variational autoencoder', ' genomics']",8423,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 4, 2], 'rate': 3, 'confidence': 4}]"
"Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity","['risk-sensitive reinforcement learning', ' robust Markov Decision Processes']",8422,"[{'mark': [3, 2, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}]"
Federated Binary Matrix Factorization using Proximal Optimization,"['federated learning', ' binary matrix factorization', ' boolean matrix factorization', ' proximal operator', ' differential privacy']",8420,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}]"
Feature Learning in Infinite Depth Neural Networks,"['Tensor Programs', ' mup', ' deep learning', ' optimization', ' optimal hyperparameter transfer']",8419,"[{'mark': [3, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 4], 'rate': 6, 'confidence': 4}]"
Two Facets of SDE Under an Information-Theoretic Lens: Generalization of SGD via Training Trajectories and via Terminal States,"['generalization', ' information theory', ' SGD', ' SDE']",8418,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}]"
VideoDirectorGPT: Consistent Multi-Scene Video Generation via LLM-Guided Planning,"['Text-to-Video Generation', ' Large Language Models', ' Layout-Guided Video Generation', ' Temporal Consistency', ' Multi-Scene Video Generation', ' Layout Control']",8416,"[{'mark': [2, 2, 3], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Out-of-domain Fact Checking,"['fact checking', ' misinformation', ' natural language processing', ' distribution shift', ' text classification']",8415,"[{'mark': [1, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}]"
Language Models Linearly Represent Sentiment,"['NLP', ' Mechanistic Interpretability', ' Large Language Models']",8411,"[{'mark': [2, 2, 4], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 6, 'confidence': 3}]"
Improved Techniques for Training Consistency Models,"['Consistency Models', ' Consistency Training', ' Diffusion Models', ' Score-Based Generative Models', ' Score-Based Diffusion Models', ' Distillation']",8410,"[{'mark': [2, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}]"
A Theoretical Study of the Jacobian Matrix in Deep Neural Networks,['Theory;Deep Neural Networks; Jacobian Matrix'],8409,"[{'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [1, 3, 1], 'rate': 3, 'confidence': 4}]"
Why Do We Need Weight Decay in Modern Deep Learning?,"['Weight decay', ' overparameterization', ' implicit regularization', ' large language models', ' optimization dynamics.']",8408,"[{'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 4, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
Demystifying Poisoning Backdoor Attacks from a Statistical Perspective,"['backdoor attack', ' machine learning safety', ' asymptotic', ' statistical risk']",8404,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Concept Alignment as a Prerequisite for Value Alignment,"['Human-AI alignment', ' concept alignment', ' cognitive science']",8403,"[{'mark': [3, 4, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [2, 2, 1], 'rate': 1, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 3}]"
Learning to make adherence-aware advice,"['Human-AI interaction', ' Reinforcement Learning']",8402,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 2, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 1, 2], 'rate': 5, 'confidence': 3}]"
Provable Compositional Generalization for Object-Centric Learning,"['compositional generalization', ' identifiability', ' object-centric learning', ' generalization', ' OOD generalization', ' unsupervised learning', ' slot attention', ' disentanglement', ' autoencoders', ' representation learning']",8400,"[{'mark': [2, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 8, 'confidence': 3}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 4}]"
"Battle of the Wordsmiths: Comparing ChatGPT, GPT-4, Claude, and Bard","['ChatGPT', ' Bard', ' Claude', ' GPT-4', ' large language models', ' chatbots', ' conversational agents']",8398,"[{'mark': [2, 3, 3], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
iHyperTime: Interpretable Time Series Generation with Implicit Neural Representations,"['Time series generation', ' implicit neural representations']",8397,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
Bridging the Fairness Divide: Achieving Group and Individual Fairness in Graph Neural Networks,"['Graph Neural Networks', ' Fairness in Graph Learning', ' Individual Fairness', ' Group Fairness']",8396,"[{'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 3], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 1, 'confidence': 4}]"
Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs,"['Temporal Dynamic Graphs', ' Spectral Transform', ' GNN']",8395,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 1, 3], 'rate': 6, 'confidence': 4}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
RoCA: A Robust Method to Discover Causal or Anticausal Relation by Noise Injection,"['Causal or Anticausal Relation Discovery', ' Semi-Supervised Learning']",8394,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}]"
Cycle Consistency Driven Object Discovery,"['cycle consistency', ' object discovery', ' downstream RL']",8392,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
RF-POLICY: Rectified Flows are Adaptive Decision Makers,"['robot learning', ' imitation learning', ' flow-based policies']",8391,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 2, 3], 'rate': 5, 'confidence': 4}]"
Rectifying Group Irregularities in Explanations for Distribution Shift,"['explainability', ' distribution shift', ' group robust']",8390,"[{'mark': [2, 2, 3], 'rate': 6, 'confidence': 3}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 2}]"
Contrastive Decoding Improves Reasoning in Large Language Models,"['natural language processing', ' language models', ' contrastive decoding', ' decoding', ' reasoning']",8389,"[{'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 5}]"
Sufficient conditions for offline reactivation in recurrent neural networks,"['computational neuroscience', ' offline reactivation', ' replay', ' recurrent neural networks', ' path integration', ' noise']",8388,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 5}]"
Centroid-Based Learning for Malware Detection and Novel Family Identification,['malware; graphs; GNN;'],8386,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 4}]"
GUARD: A Safe Reinforcement Learning Benchmark,"['Safe Reinforcement Learning', ' Reinforcement Learning Benchmark', ' Safe Reinforcement Learning Algorithm', ' Customizable', ' Robotics']",8384,"[{'mark': [4, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}]"
Forward Learning of Graph Neural Networks,"['graph neural networks', ' forward learning', ' forward-forward algorithm']",8383,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [4, 2, 3], 'rate': 6, 'confidence': 3}]"
Size Generalization of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective,"['Graph neural networks', ' Out of distribution', ' Size-induced distribution shifts']",8382,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 3}]"
Self-Supervised Learning with the Matching Gap,"['optimal transport', ' self-supervised learning']",8379,"[{'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [2, 1, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 4}]"
COMPARATOR: Reference-free machine translation evaluation by inter-system comparison,['Machine Translation Evaluation'],8378,"[{'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [4, 4, 4], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}]"
Multi-timestep models for Model-based Reinforcement Learning,"['Model-based Reinforcement Learning', ' Compounding errors', ' Multi-timestep models']",8377,"[{'mark': [2, 3, 1], 'rate': 1, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [1, 3, 2], 'rate': 3, 'confidence': 4}]"
Multisensory Geospatial Models via Cross-Sensor Pretraining,"['geospatial pretraining', ' multisensor modalities', ' cross-sensor pretraining', ' remote sensing applications', ' masked image modeling']",8376,"[{'mark': [2, 3, 3], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}]"
Embedding Improves Neural Regularizers for Inverse Problems,"['Inverse Problems', ' High Dimensional Embedding', ' Dictionary Learning']",8375,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 3], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 2}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}]"
SwapTransformer: Highway Overtaking Tactical Planner Model via Imitation Learning on OSHA Dataset,"['Autonomous driving', ' Imitation learning', ' highway', ' overtaking', ' machine learning', ' transformer']",8374,"[{'mark': [1, 3, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 5, 'confidence': 3}]"
GEO: Generative Engine Optimization,"['generative models', ' search engines', ' datasets and benchmarks']",8373,"[{'mark': [1, 2, 2], 'rate': 3, 'confidence': 5}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Clarify When Necessary: Resolving Ambiguity with Language Models,"['Language Models', ' Ambiguity', ' Uncertainty']",8372,"[{'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 3}]"
From gradient attacks to data poisoning,"['data poisoning', ' safety', ' attacks', ' gradient attack', ' manipulation.']",8371,"[{'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 5}, {'mark': [1, 1, 1], 'rate': 1, 'confidence': 5}, {'mark': [2, 1, 2], 'rate': 5, 'confidence': 4}]"
Curriculum reinforcement learning for quantum architecture search under hardware errors,"['Quantum Computing', ' Reinforcement Learning', ' Quantum Chemistry', ' Quantum Architecture Search', ' Optimization']",8370,"[{'mark': [2, 1, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}]"
Does CLIPâ€™s generalization performance mainly stem from high train-test similarity?,"['robustness', ' foundation models', ' CLIP', ' LAION', ' ImageNet', ' generalization', ' OOD robustness', ' distribution shift', ' vision language models', ' self-supervised learning', ' contrastive learning', ' ObjectNet', ' ImageNet-R', ' ImageNet-Sketch', ' ImageNet-A', ' ImageNet-V2']",8369,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 5}]"
State-wise Constrained Policy Optimization,"['Safe Reinforcement Learning', ' State-wise Safety Guarantee', ' Trust Region Optimization']",8367,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 8, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}]"
Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift,"['Distribution-Shift', ' Domain-Adaptation', ' Robust-Machine-Learning']",8365,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 3], 'rate': 6, 'confidence': 4}]"
A Unified Approach for Online Continuous DR-Submodular Maximization,"['Stochastic optimization', ' submodular maximization', ' Frank-Wolfe algorithm']",8364,"[{'mark': [3, 4, 4], 'rate': 1, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 2, 3], 'rate': 6, 'confidence': 3}]"
Fast Sampling via De-randomization for Discrete Diffusion Models,"['Sampling', ' Discrete Diffusion', ' Text Generation']",8363,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 5}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 2, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions,"['reverse engineering deceptions', ' GAN inversion', ' optimization', ' adversarial attacks', ' generative models', ' inverse problems']",8358,"[{'mark': [2, 3, 2], 'rate': 6, 'confidence': 3}, {'mark': [2, 1, 2], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 2}]"
Amortized Bayesian Inference with Hybrid Expert-in-the-Loop and Learnable Summary Statistics,"['Bayesian inference', ' summary statistics', ' generative models', ' amortized inference', ' expert-in-the-loop']",8356,"[{'mark': [2, 3, 3], 'rate': 6, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [2, 3, 1], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}]"
SEArch: A Self-Evolving Framework for Network Architecture Optimization,"['network architecture optimization', ' network pruning', ' knowledge distillation']",8353,"[{'mark': [3, 2, 2], 'rate': 5, 'confidence': 4}, {'mark': [3, 3, 1], 'rate': 3, 'confidence': 3}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}]"
"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method","['LLM finetuning', ' Scaling Laws', ' Full-model finetuning', ' Parameter efficient tuning', ' Machine Translation', ' Multilingual Summarization']",8351,"[{'mark': [2, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 3], 'rate': 8, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [4, 4, 3], 'rate': 8, 'confidence': 3}]"
Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance,"['Natural Language Processing', ' Large Language Models', ' Prompt Engineering']",8350,"[{'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [2, 2, 2], 'rate': 3, 'confidence': 3}]"
Learning to design protein-protein interactions with enhanced generalization,"['protein-protein interactions', ' protein design', ' generalization', ' self-supervised learning', ' equivariant 3D representations']",8349,"[{'mark': [3, 3, 3], 'rate': 5, 'confidence': 3}, {'mark': [3, 4, 4], 'rate': 8, 'confidence': 5}, {'mark': [3, 3, 2], 'rate': 6, 'confidence': 4}, {'mark': [3, 3, 3], 'rate': 6, 'confidence': 2}, {'mark': [4, 3, 3], 'rate': 6, 'confidence': 5}]"
Robustness to Multi-Modal Environment Uncertainty in MARL using Curriculum Learning,"['Multi-Modal Uncertainty', ' Robustness', ' Multi-Agent Reinforcement Learning']",8348,"[{'mark': [2, 2, 2], 'rate': 1, 'confidence': 4}, {'mark': [1, 1, 2], 'rate': 3, 'confidence': 4}, {'mark': [1, 1, 1], 'rate': 3, 'confidence': 5}, {'mark': [2, 3, 2], 'rate': 3, 'confidence': 4}]"
AlphaFold Distillation for Protein Design,"['Inverse Protein Folding Design', ' Protein Design', ' Model Distillation', ' AlphaFold', ' Protein Folding']",8347,"[{'mark': [3, 3, 2], 'rate': 3, 'confidence': 4}, {'mark': [2, 3, 2], 'rate': 5, 'confidence': 4}, {'mark': [1, 2, 2], 'rate': 3, 'confidence': 5}]"
Video2Demo: Grounding Videos in State-Action Demonstrations,"['multimodal applications', ' vision language models', ' large language models', ' task planning', ' open-vocabulary recognition']",8346,"[{'mark': [2, 2, 1], 'rate': 3, 'confidence': 3}, {'mark': [3, 4, 2], 'rate': 3, 'confidence': 4}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 3}, {'mark': [3, 3, 2], 'rate': 5, 'confidence': 4}]"
